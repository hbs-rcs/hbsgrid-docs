{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HBS Research Computing Services is building powerful and user-friendly tools and environments to bring the computing power of the HBS Grid to a wider community of HBS users. Our environments include a wide range of software including Julia , Matlab , Python , R , Stata , and hundreds of other popular programs These environments are in active development and are currently available as a Technology Preview for testing. Quicklinks \u2b8a Get started by enabling HBS grid Technology Preview features Start here! \ud83d\ude80 Run Applications from the Desktop Run applications on powerful HBS Grid compute nodes using desktop menus \ud83d\uddd8 Copy and Sync Files Mount or copy data from local drives or cloud storage to the HBS Grid \ud83d\udc65 Collaborate and Share Safely and Securely Learn how to work safely and securely in a multi-user environment \ud83d\udce6 Use Software Versions Learn about available software and how to run different software versions \u2503> Use the Command-line Run jobs on powerful HBS Grid compute nodes from the terminal \ud83e\uddba Support and Troubleshooting What to do if things don't work as expected You can click the video thumbnail below to watch a short demonstration of our software environment. Your browser does not support the video tag. User testing and project status We are currently conducting user testing and assessing the possibility of making this software more widely available. The environment is generally stable but the software documented here remains a technology preview and any and all use is at your own risk . If you find that something doesn't work as expected, of if you have a feature request, we would love to know about it so we can fix or improve it. Bug reports and feature requests are important contributions to this project and are always welcome and encouraged! You can either start a discussion at https://github.com/hbs-rcs/hbsgrid-docs/discussions or create an issue report at https://github.com/hbs-rcs/hbsgrid-docs/issues . We are building a friendly and welcoming community of HBS Grid users and we invite you to join us using the links above.","title":"Home"},{"location":"#hbs-research-computing-services-is-building-powerful-and-user-friendly-tools-and-environments-to-bring-the-computing-power-of-the-hbs-grid-to-a-wider-community-of-hbs-users-our-environments-include-a-wide-range-of-software-including-julia-matlab-python-r-stata-and-hundreds-of-other-popular-programs","text":"These environments are in active development and are currently available as a Technology Preview for testing. Quicklinks \u2b8a Get started by enabling HBS grid Technology Preview features Start here! \ud83d\ude80 Run Applications from the Desktop Run applications on powerful HBS Grid compute nodes using desktop menus \ud83d\uddd8 Copy and Sync Files Mount or copy data from local drives or cloud storage to the HBS Grid \ud83d\udc65 Collaborate and Share Safely and Securely Learn how to work safely and securely in a multi-user environment \ud83d\udce6 Use Software Versions Learn about available software and how to run different software versions \u2503> Use the Command-line Run jobs on powerful HBS Grid compute nodes from the terminal \ud83e\uddba Support and Troubleshooting What to do if things don't work as expected You can click the video thumbnail below to watch a short demonstration of our software environment. Your browser does not support the video tag. User testing and project status We are currently conducting user testing and assessing the possibility of making this software more widely available. The environment is generally stable but the software documented here remains a technology preview and any and all use is at your own risk . If you find that something doesn't work as expected, of if you have a feature request, we would love to know about it so we can fix or improve it. Bug reports and feature requests are important contributions to this project and are always welcome and encouraged! You can either start a discussion at https://github.com/hbs-rcs/hbsgrid-docs/discussions or create an issue report at https://github.com/hbs-rcs/hbsgrid-docs/issues . We are building a friendly and welcoming community of HBS Grid users and we invite you to join us using the links above.","title":"HBS Research Computing Services is building powerful and user-friendly tools and environments to bring the computing power of the HBS Grid to a wider community of HBS users. Our environments include a wide range of software including Julia, Matlab, Python, R, Stata, and hundreds of other popular programs"},{"location":"support/trouble/","text":"The software documented here is under active development and may break or cause other problems . Issue reports and community discussion Bug reports and feature requests are important contributions to this project and are always welcome and encouraged! You can either start a discussion at https://github.com/hbs-rcs/hbsgrid-docs/discussions or create an issue report at https://github.com/hbs-rcs/hbsgrid-docs/issues . We are building a friendly and welcoming community of HBS Grid users and we invite you to join us using the links above. Basic troubleshooting steps If you are experiencing problems with the desktop or application launchers, a good first step is to re-start your desktop using the HBS Grid Configuration utility in the Applications => Accessories menu. If problems persist you can terminate your NoMachine session and start a new one. This will re-initialize your desktop environment and resolve many issues that can occur when your settings get out of sync. Known bugs and work-arounds My application just disappeared! Users are often surprised to find that their application just disappears, often after running for several hours or days. Usually this happens because your application was killed by the system after exceeding a time or memory limit. For example, jobs running in the short interactive queue are limited to a maximum runtime of 24 hours. After 24 hours your job will be killed without warning or notice. Similarly, the system will kill your job if it tries to use more memory than you requested when you started the job. You can avoid having your job killed by staying within the system and job limits. These limits are described in more detail in Handling system limits and Interactive and batch queue limits . Disabling the technology preview If you wish you can disable technology preview features entirely by starting the HBS Grid configuration utility and turning off the technology preview features, as shown below. Selecting the Stable/Standard option under HBS Grid launcher style will mostly restore your system to the original state. To ensure that your environment is completely reset terminate your NoMachine session and start a new one.","title":"Trouble-shooting"},{"location":"support/trouble/#issue-reports-and-community-discussion","text":"Bug reports and feature requests are important contributions to this project and are always welcome and encouraged! You can either start a discussion at https://github.com/hbs-rcs/hbsgrid-docs/discussions or create an issue report at https://github.com/hbs-rcs/hbsgrid-docs/issues . We are building a friendly and welcoming community of HBS Grid users and we invite you to join us using the links above.","title":"Issue reports and community discussion"},{"location":"support/trouble/#basic-troubleshooting-steps","text":"If you are experiencing problems with the desktop or application launchers, a good first step is to re-start your desktop using the HBS Grid Configuration utility in the Applications => Accessories menu. If problems persist you can terminate your NoMachine session and start a new one. This will re-initialize your desktop environment and resolve many issues that can occur when your settings get out of sync.","title":"Basic troubleshooting steps"},{"location":"support/trouble/#known-bugs-and-work-arounds","text":"","title":"Known bugs and work-arounds"},{"location":"tutorials/PythonWebScrape/","text":"Python Web-Scraping This is an intermediate / advanced Python tutorial: Assumes knowledge of Python, including: lists dictionaries logical indexing iteration with for-loops Assumes basic knowledge of web page structure If you need an introduction to Python or a refresher, we recommend our Python Introduction . Goals This workshop is organized into two main parts: 1. Retrive information in JSON format 2. Parse HTML files Note that this workshop will not teach you everything you need to know in order to retrieve data from any web service you might wish to scrape. Web scraping background What is web scraping? Web scraping is the activity of automating retrieval of information from a web service designed for human interaction. Is web scraping legal? Is it ethical? It depends. If you have legal questions seek legal counsel. You can mitigate some ethical issues by building delays and restrictions into your web scraping program so as to avoid impacting the availability of the web service for other users or the cost of hosting the service for the service provider. Web scraping approaches No two websites are identical \u2014 websites are built for different purposes by different people and so have different underlying structures. Because they are heterogeneous, there is no single way to scrape a website. The scraping approach therefore has to be tailored to each individual site. Here are some commonly used approaches: Use requests to extract information from structured JSON files Use requests to extract information from HTML Automate a browser to retrieve information from HTML Bear in mind that even once you\u2019ve decided upon the best approach for a particular site, it will be necessary to modify that approach to suit your particular use-case. How does the web work? Components Computers connected to the web are called clients and servers . A simplified diagram of how they interact might look like this: Clients are the typical web user\u2019s internet-connected devices (for example, your computer connected to your Wi-Fi) and web-accessing software available on those devices (usually a web browser like Firefox or Chrome). Servers are computers that store webpages, sites, or apps. When a client device wants to access a webpage, a copy of the webpage is downloaded from the server onto the client machine to be displayed in the user\u2019s web browser. HTTP is a language for clients and servers to speak to each other. So what happens? When you type a web address into your browser: The browser finds the address of the server that the website lives on. The browser sends an HTTP request message to the server, asking it to send a copy of the website to the client. If the server approves the client\u2019s request, the server sends the client a 200 OK message, and then starts displaying the website in the browser. Retrieve data in JSON format if you can **GOAL: To retrieve information in JSON format and organize it into a spreadsheet.** 1. Inspect the website to check if the content is stored in JSON format 2. Make a request to the website server to retrieve the JSON file 3. Convert from JSON format into a Python dictionary 4. Extract the data from the dictionary and store in a .csv file We wish to extract information from https://www.harvardartmuseums.org/collections . Like most modern web pages, a lot goes on behind the scenes to produce the page we see in our browser. Our goal is to pull back the curtain to see what the website does when we interact with it. Once we see how the website works we can start retrieving data from it. If we are lucky we\u2019ll find a resource that returns the data we\u2019re looking for in a structured format like JSON . This is useful because it is very easy to convert data from JSON into a spreadsheet type format \u2014 like a csv or Excel file. Examine the website\u2019s structure The basic strategy is pretty much the same for most scraping projects. We will use our web browser (Chrome or Firefox recommended) to examine the page you wish to retrieve data from, and copy/paste information from your web browser into your scraping program. We start by opening the collections web page in a web browser and inspecting it. If we scroll down to the bottom of the Collections page, we\u2019ll see a button that says \u201cLoad More\u201d. Let\u2019s see what happens when we click on that button. To do so, click on \u201cNetwork\u201d in the developer tools window, then click the \u201cLoad More Collections\u201d button. You should see a list of requests that were made as a result of clicking that button, as shown below. If we look at that second request, the one to a script named browse , we\u2019ll see that it returns all the information we need, in a convenient format called JSON . All we need to retrieve collection data is to make GET requests to https://www.harvardartmuseums.org/browse with the correct parameters. Launch JupyterLab Start the Anaconda Navigator program Click the Launch button under Jupyter Lab A browser window will open with your computer\u2019s files listed on the left hand side of the page. Navigate to the folder called PythonWebScrape that you downloaded to your desktop and double-click on the folder Within the PythonWebScrape folder, double-click on the file with the word \u201cBLANK\u201d in the name ( PythonWebScrape_BLANK.ipynb ). A pop-up window will ask you to Select Kernal \u2014 you should select the Python 3 kernal. The Jupyter Notebook should now open on the right hand side of the page A Jupyter Notebook contains one or more cells containing notes or code. To insert a new cell click the + button in the upper left. To execute a cell, select it and press Control+Enter or click the Run button at the top. Making requests To retrieve information from the website (i.e., make a request), we need to know the location of the information we want to collect. The Uniform Resource Locator (URL) \u2014 commonly know as a \u201cweb address\u201d, specifies the location of a resource (such as a web page) on the internet. A URL is usually composed of 5 parts: The 4th part, the \u201cquery string\u201d, contains one or more parameters . The 5th part, the \u201cfragment\u201d, is an internal page reference and may not be present. For example, the URL we want to retrieve data from has the following structure: protocol domain path parameters https www.harvardartmuseums.org browse load_amount=10&offset=0 It is often convenient to create variables containing the domain(s) and path(s) you\u2019ll be working with, as this allows you to swap out paths and parameters as needed. Note that the path is separated from the domain with / and the parameters are separated from the path with ? . If there are multiple parameters they are separated from each other with a & . For example, we can define the domain and path of the collections URL as follows: museum_domain = 'https://www.harvardartmuseums.org' collection_path = 'browse' collection_url = ( museum_domain + \"/\" + collection_path ) print ( collection_url ) ## 'https://www.harvardartmuseums.org/browse' Note that we omit the parameters here because it is usually easier to pass them as a dict when using the requests library in Python. This will become clearer shortly. Now that we\u2019ve constructed the URL we wish to interact with, we\u2019re ready to make our first request in Python. import requests collections1 = requests . get ( collection_url , params = { 'load_amount' : 10 , 'offset' : 0 } ) Note that the parameters load_amount and offset are essentially another way of setting page numbers \u2014 they refer to the amount of information retrieved at one time and the starting position, respectively. Parsing JSON data We already know from inspecting network traffic in our web browser that this URL returns JSON, but we can use Python to verify this assumption. collections1 . headers [ 'Content-Type' ] ## 'application/json' Since JSON is a structured data format, parsing it into Python data structures is easy. In fact, there\u2019s a method for that! collections1 = collections1 . json () # print(collections1) That\u2019s it. Really, we are done here. Everyone go home! OK not really, there is still more we can learn. But you have to admit that was pretty easy. If you can identify a service that returns the data you want in structured from, web scraping becomes a pretty trivial enterprise. We\u2019ll discuss several other scenarios and topics, but for some web scraping tasks this is really all you need to know. Organizing & saving the data The records we retrieved from https://www.harvardartmuseums.org/browse are arranged as a list of dictionaries. We can easily select the fields of arrange these data into a pandas DataFrame to facilitate subsequent analysis. import pandas as pd records1 = pd . DataFrame . from_records ( collections1 [ 'records' ]) print ( records1 ) ## copyright contextualtextcount ... dimensions seeAlso ## 0 None 0 ... 18.5 x 25.5 cm (7 5/16 x 1... [{'id': 'https://iiif.harv... ## 1 None 0 ... 17.8 x 14.6 cm (7 x 5 3/4 ... [{'id': 'https://iiif.harv... ## 2 None 0 ... H. 17.7 x W. 11.0 x D. 9.0... [{'id': 'https://iiif.harv... ## 3 None 0 ... image: 21 x 27.6 cm (8 1/4... [{'id': 'https://iiif.harv... ## 4 None 0 ... image: 33 x 25 cm (13 x 9 ... [{'id': 'https://iiif.harv... ## 5 \u00a9 William Kentridge 0 ... plate: 24.5 x 37.7 cm (9 5... [{'id': 'https://iiif.harv... ## 6 None 0 ... 257.98 g\\r\\n13.2 \u00d7 9.7 cm ... [{'id': 'https://iiif.harv... ## 7 \u00a9 Artists Rights Society (... 0 ... sheet: 34.5 x 37 cm (13 9/... [{'id': 'https://iiif.harv... ## 8 None 0 ... 16.8 x 26.7 cm (6 5/8 x 10... [{'id': 'https://iiif.harv... ## 9 None 0 ... paintings proper: H. 145.5... [{'id': 'https://iiif.harv... ## ## [10 rows x 62 columns] and write the data to a file. records1 . to_csv ( \"records1.csv\" ) Iterating to retrieve all the data Of course we don\u2019t want just the first page of collections. How can we retrieve all of them? Now that we know the web service works, and how to make requests in Python, we can iterate in the usual way. records = [] for offset in range ( 0 , 50 , 10 ): param_values = { 'load_amount' : 10 , 'offset' : offset } current_request = requests . get ( collection_url , params = param_values ) records . extend ( current_request . json ()[ 'records' ]) ## convert list of dicts to a `DataFrame` records_final = pd . DataFrame . from_records ( records ) # write the data to a file. records_final . to_csv ( \"records_final.csv\" ) print ( records_final ) ## copyright contextualtextcount ... seeAlso details ## 0 None 0 ... [{'id': 'https://iiif.harv... NaN ## 1 None 0 ... [{'id': 'https://iiif.harv... NaN ## 2 None 0 ... [{'id': 'https://iiif.harv... NaN ## 3 None 0 ... [{'id': 'https://iiif.harv... NaN ## 4 None 0 ... [{'id': 'https://iiif.harv... NaN ## 5 \u00a9 William Kentridge 0 ... [{'id': 'https://iiif.harv... NaN ## 6 None 0 ... [{'id': 'https://iiif.harv... NaN ## 7 \u00a9 Artists Rights Society (... 0 ... [{'id': 'https://iiif.harv... NaN ## 8 None 0 ... [{'id': 'https://iiif.harv... NaN ## 9 None 0 ... [{'id': 'https://iiif.harv... NaN ## 10 None 0 ... [{'id': 'https://iiif.harv... NaN ## 11 None 0 ... [{'id': 'https://iiif.harv... {'coins': {'reverseinscrip... ## 12 None 0 ... [{'id': 'https://iiif.harv... NaN ## 13 \\r\\n\u00a9 Vija Celmins, Courte... 0 ... [{'id': 'https://iiif.harv... NaN ## 14 None 0 ... [{'id': 'https://iiif.harv... NaN ## 15 None 1 ... [{'id': 'https://iiif.harv... NaN ## 16 None 0 ... [{'id': 'https://iiif.harv... NaN ## 17 None 0 ... [{'id': 'https://iiif.harv... NaN ## 18 None 0 ... [{'id': 'https://iiif.harv... NaN ## 19 \u00a9 Sarah Jane Roszak / Arti... 0 ... [{'id': 'https://iiif.harv... NaN ## 20 None 0 ... [{'id': 'https://iiif.harv... NaN ## 21 None 0 ... [{'id': 'https://iiif.harv... NaN ## 22 None 0 ... [{'id': 'https://iiif.harv... NaN ## 23 \u00a9 Jenny Holzer / Artists R... 0 ... [{'id': 'https://iiif.harv... NaN ## 24 None 0 ... [{'id': 'https://iiif.harv... NaN ## 25 None 0 ... [{'id': 'https://iiif.harv... NaN ## 26 None 0 ... [{'id': 'https://iiif.harv... NaN ## 27 None 0 ... [{'id': 'https://iiif.harv... NaN ## 28 None 0 ... [{'id': 'https://iiif.harv... NaN ## 29 None 0 ... [{'id': 'https://iiif.harv... NaN ## 30 None 0 ... [{'id': 'https://iiif.harv... NaN ## 31 None 0 ... [{'id': 'https://iiif.harv... NaN ## 32 None 0 ... [{'id': 'https://iiif.harv... NaN ## 33 None 0 ... [{'id': 'https://iiif.harv... NaN ## 34 None 0 ... [{'id': 'https://iiif.harv... NaN ## 35 None 0 ... [{'id': 'https://iiif.harv... NaN ## 36 \u00a9 Gary Schneider 0 ... [{'id': 'https://iiif.harv... NaN ## 37 None 0 ... [{'id': 'https://iiif.harv... NaN ## 38 \u00a9 1974 Mimi Smith 0 ... [{'id': 'https://iiif.harv... NaN ## 39 None 0 ... [{'id': 'https://iiif.harv... NaN ## 40 None 0 ... [{'id': 'https://iiif.harv... NaN ## 41 None 0 ... [{'id': 'https://iiif.harv... NaN ## 42 None 0 ... [{'id': 'https://iiif.harv... NaN ## 43 None 0 ... [{'id': 'https://iiif.harv... NaN ## 44 None 0 ... [{'id': 'https://iiif.harv... NaN ## 45 None 0 ... [{'id': 'https://iiif.harv... NaN ## 46 None 0 ... [{'id': 'https://iiif.harv... NaN ## 47 \u00a9 Andy Warhol Foundation ... 0 ... [{'id': 'https://iiif.harv... NaN ## 48 None 0 ... [{'id': 'https://iiif.harv... NaN ## 49 None 0 ... [{'id': 'https://iiif.harv... NaN ## ## [50 rows x 63 columns] Exercise 0 Retrieve exhibits data In this exercise you will retrieve information about the art exhibitions at Harvard Art Museums from https://www.harvardartmuseums.org/exhibitions Using a web browser (Firefox or Chrome recommended) inspect the page at https://www.harvardartmuseums.org/exhibitions . Examine the network traffic as you interact with the page. Try to find where the data displayed on that page comes from. ## Make a get request in Python to retrieve the data from the URL identified in step1. ## Write a loop or list comprehension in Python to retrieve data for the first 5 pages of exhibitions data. ## Bonus (optional): Convert the data you retrieved into a pandas DataFrame and save it to a .csv file. ## Click for Exercise 0 Solution Question #1: museum_domain = \"https://www.harvardartmuseums.org\" exhibit_path = \"search/load_more\" exhibit_url = museum_domain + \"/\" + exhibit_path print ( exhibit_url ) ## 'https://www.harvardartmuseums.org/search/load_more' Question #2: import requests from pprint import pprint as print exhibit1 = requests . get ( exhibit_url , params = { 'type' : 'past-exhibition' , 'page' : 1 }) print ( exhibit1 . headers [ \"Content-Type\" ]) ## 'application/json' exhibit1 = exhibit1 . json () # print(exhibit1) Questions #3+4 (loop solution): firstFivePages = [] for page in range ( 1 , 6 ): records_per_page = requests . get ( exhibit_url , \\ params = { 'type' : 'past-exhibition' , 'page' : page }) . json ()[ 'records' ] firstFivePages . extend ( records_per_page ) firstFivePages_records = pd . DataFrame . from_records ( firstFivePages ) print ( firstFivePages_records ) ## shortdescription images ... videos publications ## 0 None [{'date': '2018-08-03', 'c... ... NaN NaN ## 1 None [{'date': '2018-01-12', 'c... ... [{'description': 'Bauhaus ... [{'publicationplace': 'Cam... ## 2 None [{'date': '2019-01-22', 'c... ... NaN NaN ## 3 None [{'date': None, 'copyright... ... NaN NaN ## 4 None [{'date': '2018-11-09', 'c... ... NaN NaN ## 5 None [{'date': '2018-06-04', 'c... ... NaN [{'publicationplace': 'Cam... ## 6 None [{'date': '2001-03-01', 'c... ... NaN NaN ## 7 None [{'date': '2005-04-18', 'c... ... NaN NaN ## 8 None [{'date': '2018-06-29', 'c... ... [{'description': 'Marina I... NaN ## 9 None [{'date': '2018-03-15', 'c... ... NaN NaN ## 10 None [{'date': '2016-10-17', 'c... ... NaN [{'publicationplace': 'Cam... ## 11 None [{'date': '2017-02-16', 'c... ... NaN [{'publicationplace': 'Cam... ## 12 None [{'date': '2018-01-23', 'c... ... NaN NaN ## 13 None [{'date': '2001-04-01', 'c... ... NaN NaN ## 14 None [{'date': '2016-06-10', 'c... ... [{'description': 'Fernando... NaN ## 15 None [{'date': '2008-10-27', 'c... ... NaN NaN ## 16 None [{'date': '2017-10-05', 'c... ... NaN NaN ## 17 None [{'date': '2002-05-01', 'c... ... NaN NaN ## 18 None [{'date': '2007-08-01', 'c... ... NaN NaN ## 19 None [{'date': '2003-03-21', 'c... ... NaN NaN ## 20 None [{'date': '2017-05-08', 'c... ... NaN [{'publicationplace': 'Cam... ## 21 None [{'date': '2002-08-01', 'c... ... [{'description': 'Symposiu... NaN ## 22 None [{'date': '2016-07-05', 'c... ... NaN NaN ## 23 None [{'date': '2017-03-29', 'c... ... NaN NaN ## 24 None [{'date': '2015-03-22', 'c... ... [{'description': 'The Phil... [{'publicationplace': 'Cam... ## 25 None [{'date': '2017-03-07', 'c... ... NaN NaN ## 26 Harvard professor Ewa Laje... [{'date': '2001-03-01', 'c... ... NaN [{'publicationplace': 'Cam... ## 27 This exhibition features w... [{'date': '2016-07-12', 'c... ... NaN [{'publicationplace': 'Cam... ## 28 None [{'date': '2017-02-08', 'c... ... NaN NaN ## 29 None [{'date': '2001-06-01', 'c... ... NaN NaN ## 30 None [{'date': '2005-11-03', 'c... ... NaN NaN ## 31 None [{'date': '2015-04-06', 'c... ... [{'description': 'Wolfgang... NaN ## 32 In progress [{'date': '2016-07-28', 'c... ... NaN NaN ## 33 None [{'date': None, 'copyright... ... NaN NaN ## 34 None [{'date': '2016-03-21', 'c... ... NaN NaN ## 35 None [{'date': '2007-04-18', 'c... ... NaN [{'publicationplace': 'Cam... ## 36 None [{'date': '2015-04-01', 'c... ... NaN [{'publicationplace': 'Cam... ## 37 None [{'date': '2016-01-12', 'c... ... NaN NaN ## 38 None [{'date': '2002-07-01', 'c... ... NaN NaN ## 39 None [{'date': '2007-02-27', 'c... ... NaN NaN ## 40 None [{'date': '2006-01-18', 'c... ... NaN NaN ## 41 None [{'date': '2015-10-29', 'c... ... NaN NaN ## 42 None [{'date': '2014-08-12', 'c... ... [{'description': 'Teaser: ... [{'publicationplace': 'Cam... ## 43 None [{'date': '2005-11-03', 'c... ... NaN NaN ## 44 None [{'date': '1989-08-01', 'c... ... NaN NaN ## 45 This multi-component insta... [{'date': '2014-03-31', 'c... ... NaN NaN ## 46 None [{'date': '2009-11-30', 'c... ... NaN NaN ## 47 Harvard Art Museums\u2019 new p... [{'date': '2012-06-29', 'c... ... [{'description': 'Terry Wi... NaN ## 48 None [{'date': '2014-11-10', 'c... ... NaN NaN ## 49 In the Japanese context, a... [{'date': '2012-07-17', 'c... ... NaN NaN ## ## [50 rows x 19 columns] Questions #3+4 (list comprehension solution): first5Pages = [ requests . get ( exhibit_url , \\ params = { 'type' : 'past-exhibition' , 'page' : page }) . json ()[ 'records' ] for page in range ( 1 , 6 )] from itertools import chain first5Pages = list ( chain . from_iterable ( first5Pages )) import pandas as pd first5Pages_records = pd . DataFrame . from_records ( first5Pages ) print ( first5Pages_records ) ## shortdescription images ... videos publications ## 0 None [{'date': '2018-08-03', 'c... ... NaN NaN ## 1 None [{'date': '2018-01-12', 'c... ... [{'description': 'Bauhaus ... [{'publicationplace': 'Cam... ## 2 None [{'date': '2019-01-22', 'c... ... NaN NaN ## 3 None [{'date': None, 'copyright... ... NaN NaN ## 4 None [{'date': '2018-11-09', 'c... ... NaN NaN ## 5 None [{'date': '2018-06-04', 'c... ... NaN [{'publicationplace': 'Cam... ## 6 None [{'date': '2001-03-01', 'c... ... NaN NaN ## 7 None [{'date': '2005-04-18', 'c... ... NaN NaN ## 8 None [{'date': '2018-06-29', 'c... ... [{'description': 'Marina I... NaN ## 9 None [{'date': '2018-03-15', 'c... ... NaN NaN ## 10 None [{'date': '2016-10-17', 'c... ... NaN [{'publicationplace': 'Cam... ## 11 None [{'date': '2017-02-16', 'c... ... NaN [{'publicationplace': 'Cam... ## 12 None [{'date': '2018-01-23', 'c... ... NaN NaN ## 13 None [{'date': '2001-04-01', 'c... ... NaN NaN ## 14 None [{'date': '2016-06-10', 'c... ... [{'description': 'Fernando... NaN ## 15 None [{'date': '2008-10-27', 'c... ... NaN NaN ## 16 None [{'date': '2017-10-05', 'c... ... NaN NaN ## 17 None [{'date': '2002-05-01', 'c... ... NaN NaN ## 18 None [{'date': '2007-08-01', 'c... ... NaN NaN ## 19 None [{'date': '2003-03-21', 'c... ... NaN NaN ## 20 None [{'date': '2017-05-08', 'c... ... NaN [{'publicationplace': 'Cam... ## 21 None [{'date': '2002-08-01', 'c... ... [{'description': 'Symposiu... NaN ## 22 None [{'date': '2016-07-05', 'c... ... NaN NaN ## 23 None [{'date': '2017-03-29', 'c... ... NaN NaN ## 24 None [{'date': '2015-03-22', 'c... ... [{'description': 'The Phil... [{'publicationplace': 'Cam... ## 25 None [{'date': '2017-03-07', 'c... ... NaN NaN ## 26 Harvard professor Ewa Laje... [{'date': '2001-03-01', 'c... ... NaN [{'publicationplace': 'Cam... ## 27 This exhibition features w... [{'date': '2016-07-12', 'c... ... NaN [{'publicationplace': 'Cam... ## 28 None [{'date': '2017-02-08', 'c... ... NaN NaN ## 29 None [{'date': '2001-06-01', 'c... ... NaN NaN ## 30 None [{'date': '2005-11-03', 'c... ... NaN NaN ## 31 None [{'date': '2015-04-06', 'c... ... [{'description': 'Wolfgang... NaN ## 32 In progress [{'date': '2016-07-28', 'c... ... NaN NaN ## 33 None [{'date': None, 'copyright... ... NaN NaN ## 34 None [{'date': '2016-03-21', 'c... ... NaN NaN ## 35 None [{'date': '2007-04-18', 'c... ... NaN [{'publicationplace': 'Cam... ## 36 None [{'date': '2015-04-01', 'c... ... NaN [{'publicationplace': 'Cam... ## 37 None [{'date': '2016-01-12', 'c... ... NaN NaN ## 38 None [{'date': '2002-07-01', 'c... ... NaN NaN ## 39 None [{'date': '2007-02-27', 'c... ... NaN NaN ## 40 None [{'date': '2006-01-18', 'c... ... NaN NaN ## 41 None [{'date': '2015-10-29', 'c... ... NaN NaN ## 42 None [{'date': '2014-08-12', 'c... ... [{'description': 'Teaser: ... [{'publicationplace': 'Cam... ## 43 None [{'date': '2005-11-03', 'c... ... NaN NaN ## 44 None [{'date': '1989-08-01', 'c... ... NaN NaN ## 45 This multi-component insta... [{'date': '2014-03-31', 'c... ... NaN NaN ## 46 None [{'date': '2009-11-30', 'c... ... NaN NaN ## 47 Harvard Art Museums\u2019 new p... [{'date': '2012-06-29', 'c... ... [{'description': 'Terry Wi... NaN ## 48 None [{'date': '2014-11-10', 'c... ... NaN NaN ## 49 In the Japanese context, a... [{'date': '2012-07-17', 'c... ... NaN NaN ## ## [50 rows x 19 columns] Parsing HTML if you have to **GOAL: To retrieve information in HTML format and organize it into a spreadsheet.** 1. Make a request to the website server to retrieve the HTML 2. Inspect the HTML to determine the XPATHs that point to the data we want 3. Extract the information from the location the XPATHs point to and store in a dictionary 4. Convert from a dictionary to a .csv file As we\u2019ve seen, you can often inspect network traffic or other sources to locate the source of the data you are interested in and the API used to retrieve it. You should always start by looking for these shortcuts and using them where possible. If you are really lucky, you\u2019ll find a shortcut that returns the data as JSON. If you are not quite so lucky, you will have to parse HTML to retrieve the information you need. Document Object Model (DOM) To parse HTML, we need to have a nice tree structure that contains the whole HTML file through which we can locate the information. This tree-like structure is the Document Object Model (DOM) . DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects . DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following is an example of DOM hierarchy in an HTML document: Retrieving HTML When I inspect the network traffic while interacting with https://www.harvardartmuseums.org/calendar I don\u2019t see any requests that return JSON data. The best we can do appears to be to return HTML. To retrieve data on the events listed in the calender, the first step is the same as before: we make a get request. calendar_path = 'calendar' calendar_url = ( museum_domain # recall that we defined museum_domain earlier + \"/\" + calendar_path ) print ( calendar_url ) ## 'https://www.harvardartmuseums.org/calendar' events = requests . get ( calendar_url ) As before, we can check the headers to see what type of content we received in response to our request. events . headers [ 'Content-Type' ] ## 'text/html; charset=UTF-8' Parsing HTML using the lxml library Like JSON, HTML is structured; unlike JSON, it is designed to be rendered into a human-readable page rather than simply to store and exchange data in a computer-readable format. Consequently, parsing HTML and extracting information from it is somewhat more difficult than parsing JSON. While JSON parsing is built into the Python requests library, parsing HTML requires a separate library. I recommend using the HTML parser from the lxml library; others prefer an alternative called beautifulsoup4 . from lxml import html # convert a html text representation (`events.text`) into # a tree-structure (DOM) html representation (`events_html`) events_html = html . fromstring ( events . text ) Using XPath to extract content from HTML XPath is a tool for identifying particular elements within a HTML document. The developer tools built into modern web browsers make it easy to generate XPath s that can be used to identify the elements of a web page that we wish to extract. We can open the HTML document we retrieved and inspect it using our web browser. html . open_in_browser ( events_html , encoding = 'UTF-8' ) Once we identify the element containing the information of interest we can use our web browser to copy the XPath that uniquely identifies that element. Next we can use Python to extract the element of interest: events_list_html = events_html . xpath ( '//*[@id=\"events_list\"]/article' ) Let\u2019s just extract the second element in our events list. second_event_html = events_list_html [ 1 ] Once again, we can use a web browser to inspect the HTML we\u2019re currently working with - from the second event - and to figure out what we want to extract from it. html . open_in_browser ( second_event_html , encoding = 'UTF-8' ) As before, we can use our browser to find the xpath of the elements we want. (Note that the html.open_in_browser function adds enclosing html and body tags in order to create a complete web page for viewing. This requires that we adjust the xpath accordingly.) By repeating this process for each element we want, we can build a list of the xpaths to those elements. elements_we_want = { 'figcaption' : 'div/figure/div/figcaption' , 'date' : 'div/div/header/time' , 'title' : 'div/div/header/h2/a' , 'time' : 'div/div/div/p[1]/time' , 'description' : 'div/div/div/p[3]' } Finally, we can iterate over the elements we want and extract them. second_event_values = {} for key in elements_we_want . keys (): element = second_event_html . xpath ( elements_we_want [ key ])[ 0 ] second_event_values [ key ] = element . text_content () . strip () print ( second_event_values ) ## {'date': 'Wednesday, February 23, 2022', ## 'description': 'Virtual gallery tours are designed especially for our Friends ' ## 'and Fellows and are led by our curators, fellows, and other ' ## 'specialists.', ## 'figcaption': 'Photo: Kate Smith.', ## 'time': '9:30am - 10:30am', ## 'title': 'Virtual Gallery Tour for Friends and Fellows of the Museums'} Iterating to retrieve content from a list of HTML elements So far we\u2019ve retrieved information only for the second event. To retrieve data for all the events listed on the page we need to iterate over the events. If we are very lucky, each event will have exactly the same information structured in exactly the same way and we can simply extend the code we wrote above to iterate over the events list. Unfortunately, not all these elements are available for every event, so we need to take care to handle the case where one or more of these elements is not available. We can do that by defining a function that tries to retrieve a value and returns an empty string if it fails. If you\u2019re not familiar with Python functions, here\u2019s the basic syntax: # anatomy of a function def name_of_function ( arg1 , arg2 , ... argn ): # define the function name and arguments < body of function > # specify calculations return < result > # output result of calculations Here\u2019s an example of a simple function: def square_fun ( x ): y = x ** 2 # exponentiation return y square_fun ( 4 ) ## 16 Here\u2019s a function to perform our actual task: def get_event_info ( event , path ): try : info = event . xpath ( path )[ 0 ] . text_content () . strip () except : info = '' return info Armed with this function, we can iterate over the list of events and extract the available information for each one. all_event_values = {} for key in elements_we_want . keys (): key_values = [] for event in events_list_html : key_values . append ( get_event_info ( event , elements_we_want [ key ])) all_event_values [ key ] = key_values For convenience we can arrange these values in a pandas DataFrame and save them as .csv files, just as we did with our exhibitions data earlier. all_event_values = pd . DataFrame . from_dict ( all_event_values ) all_event_values . to_csv ( \"all_event_values.csv\" ) print ( all_event_values ) Exercise 1 parsing HTML In this exercise you will retrieve information about the physical layout of the Harvard Art Museums. The web page at https://www.harvardartmuseums.org/visit/floor-plan contains this information in HTML from. Using a web browser (Firefox or Chrome recommended) inspect the page at https://www.harvardartmuseums.org/visit/floor-plan . Copy the XPath to the element containing the list of level information. (HINT: the element if interest is a ul , i.e., unordered list .) Make a get request in Python to retrieve the web page at https://www.harvardartmuseums.org/visit/floor-plan . Extract the content from your request object and parse it using html.fromstring from the lxml library. ## Use your web browser to find the XPath s to the facilities housed on level one. Use Python to extract the text from those Xpath s. ## Bonus (optional): Write a for loop or list comprehension in Python to retrieve data for all the levels. ## Click for Exercise 1 Solution Question #2: from lxml import html floor_plan = requests . get ( 'https://www.harvardartmuseums.org/visit/floor-plan' ) floor_plan_html = html . fromstring ( floor_plan . text ) Question #3: level_one = floor_plan_html . xpath ( '/html/body/main/section/ul/li[5]/div[2]/ul' )[ 0 ] print ( type ( level_one )) ## print ( len ( level_one )) ## 6 level_one_facilities = floor_plan_html . xpath ( '/html/body/main/section/ul/li[5]/div[2]/ul/li' ) print ( len ( level_one_facilities )) ## 6 print ([ facility . text_content () for facility in level_one_facilities ]) ## ['Admissions', 'Collection Galleries', 'Courtyard', 'Shop', 'Caf\u00e9', 'Coatroom'] Question #4: all_levels = floor_plan_html . xpath ( '/html/body/main/section/ul/li' ) print ( len ( all_levels )) ## 6 all_levels_facilities = [] for level in all_levels : level_facilities = [] level_facilities_collection = level . xpath ( 'div[2]/ul/li' ) for level_facility in level_facilities_collection : level_facilities . append ( level_facility . text_content ()) all_levels_facilities . append ( level_facilities ) print ( all_levels_facilities ) ## [['Conservation Center / Lightbox Gallery'], ## ['Art Study Center'], ## ['Collection Galleries', ## 'Special Exhibitions Gallery', ## 'University Galleries'], ## ['Collections Galleries'], ## ['Admissions', ## 'Collection Galleries', ## 'Courtyard', ## 'Shop', ## 'Caf\u00e9', ## 'Coatroom'], ## ['Lower Lobby', ## 'Lecture Halls', ## 'Seminar Room', ## 'Materials Lab', ## 'Coatroom', ## 'Offices']] Scrapy : for large / complex projects Scraping websites using the requests library to make GET and POST requests, and the lxml library to process HTML is a good way to learn basic web scraping techniques. It is a good choice for small to medium size projects. For very large or complicated scraping tasks the scrapy library offers a number of conveniences, including asynchronous retrieval, session management, convenient methods for extracting and storing values, and more. More information about scrapy can be found at https://doc.scrapy.org . Browser drivers: a last resort It is sometimes necessary (or sometimes just easier) to use a web browser as an intermediary rather than communicate directly with a web service. This method of using a \u201cbrowser driver\u201d has the advantage of being able to use the javascript engine and session management features of a web browser; the main disadvantage is that it is slower and tends to be more fragile than using requests or scrapy to make requests directly from Python. For small scraping projects involving complicated sites with CAPTHAs or lots of complicated javascript using a browser driver can be a good option. More information is available at https://www.seleniumhq.org/docs/03_webdriver.jsp . Wrap-up Feedback These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu Resources IQSS Workshops: https://www.iq.harvard.edu/data-science-services/workshop-materials Data Science Services: https://www.iq.harvard.edu/data-science-services Research Computing Environment: https://iqss.github.io/dss-rce/ HBS Research Computing Services workshops: https://training.rcs.hbs.org/workshops Other HBS RCS resources: https://training.rcs.hbs.org/workshop-materials RCS consulting email: mailto:research @hbs .edu","title":"Web Scraping in Python"},{"location":"tutorials/PythonWebScrape/#python-web-scraping","text":"This is an intermediate / advanced Python tutorial: Assumes knowledge of Python, including: lists dictionaries logical indexing iteration with for-loops Assumes basic knowledge of web page structure If you need an introduction to Python or a refresher, we recommend our Python Introduction .","title":"Python Web-Scraping"},{"location":"tutorials/PythonWebScrape/#web-scraping-background","text":"","title":"Web scraping background"},{"location":"tutorials/PythonWebScrape/#retrieve-data-in-json-format-if-you-can","text":"**GOAL: To retrieve information in JSON format and organize it into a spreadsheet.** 1. Inspect the website to check if the content is stored in JSON format 2. Make a request to the website server to retrieve the JSON file 3. Convert from JSON format into a Python dictionary 4. Extract the data from the dictionary and store in a .csv file We wish to extract information from https://www.harvardartmuseums.org/collections . Like most modern web pages, a lot goes on behind the scenes to produce the page we see in our browser. Our goal is to pull back the curtain to see what the website does when we interact with it. Once we see how the website works we can start retrieving data from it. If we are lucky we\u2019ll find a resource that returns the data we\u2019re looking for in a structured format like JSON . This is useful because it is very easy to convert data from JSON into a spreadsheet type format \u2014 like a csv or Excel file.","title":"Retrieve data in JSON format if you can"},{"location":"tutorials/PythonWebScrape/#parsing-html-if-you-have-to","text":"**GOAL: To retrieve information in HTML format and organize it into a spreadsheet.** 1. Make a request to the website server to retrieve the HTML 2. Inspect the HTML to determine the XPATHs that point to the data we want 3. Extract the information from the location the XPATHs point to and store in a dictionary 4. Convert from a dictionary to a .csv file As we\u2019ve seen, you can often inspect network traffic or other sources to locate the source of the data you are interested in and the API used to retrieve it. You should always start by looking for these shortcuts and using them where possible. If you are really lucky, you\u2019ll find a shortcut that returns the data as JSON. If you are not quite so lucky, you will have to parse HTML to retrieve the information you need.","title":"Parsing HTML if you have to"},{"location":"tutorials/PythonWebScrape/#scrapy-for-large-complex-projects","text":"Scraping websites using the requests library to make GET and POST requests, and the lxml library to process HTML is a good way to learn basic web scraping techniques. It is a good choice for small to medium size projects. For very large or complicated scraping tasks the scrapy library offers a number of conveniences, including asynchronous retrieval, session management, convenient methods for extracting and storing values, and more. More information about scrapy can be found at https://doc.scrapy.org .","title":"Scrapy: for large / complex projects"},{"location":"tutorials/PythonWebScrape/#browser-drivers-a-last-resort","text":"It is sometimes necessary (or sometimes just easier) to use a web browser as an intermediary rather than communicate directly with a web service. This method of using a \u201cbrowser driver\u201d has the advantage of being able to use the javascript engine and session management features of a web browser; the main disadvantage is that it is slower and tends to be more fragile than using requests or scrapy to make requests directly from Python. For small scraping projects involving complicated sites with CAPTHAs or lots of complicated javascript using a browser driver can be a good option. More information is available at https://www.seleniumhq.org/docs/03_webdriver.jsp .","title":"Browser drivers: a last resort"},{"location":"tutorials/PythonWebScrape/#wrap-up","text":"","title":"Wrap-up"},{"location":"tutorials/large_data_R/","text":"Large Data in R: Tools and Techniques Updated December 06, 2021 Set Up and Example Data The examples and exercises require R and several R packages, including tidyverse , data.table , arrow , and duckdb . This software is all installed and ready to use on the HBS Grid. If running elsewhere make sure these required software programs are installed before proceeding. You can download and extract the data used in the examples and exercises from https://www.dropbox.com/s/vbodicsu591o7lf/original_csv.zip?dl=1 (this is a 1.3Gb zip file). These data record for-hire vehicle (aka \u201cride sharing\u201d) trips in NYC in 2020. Each row contains the record of a trip and the variable descriptions can be found in https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf Nature and Scope of the Problem: What is Large Data? Most popular data analysis software is designed to operate on data stored in random access memory (aka just \u201cmemory\u201d or \u201cRAM\u201d). This makes modifying and copying data very fast and convenient, until you start working with data that is too large for your computer\u2019s memory system. At that point you have two options: get a bigger computer or modify your workflow to process the data more carefully and efficiently. This workshop focuses on option two, using the arrow and duckdb packages in R to work with data without necessarily loading it all into memory at once. A common definition of \u201cbig data\u201d is \u201cdata that is too big to process using traditional software\u201d. We can use the term \u201clarge data\u201d as a broader category of \u201cdata that is big enough that you have to pay attention to processing it efficiently\u201d. In a typical (traditional) program, we start with data on disk, in some format. We read it in to memory, do some stuff to it on the CPU, store the results of that stuff back in memory, then write those results back to disk so they can be available for the future, as depicted below. The reason most data analysis software is designed to process data this way is because \u201cdoing some stuff\u201d is much much faster in RAM than it is if you have to read values from disk every time you need them. The downside is that RAM is much more expensive than disk storage, and typically available in smaller quantities. Memory can only hold so much data and we must either stay under that limit or buy more memory. Problem example Grounding our discussion in a concrete problem example will help make things clear. I want to know how many Lyft rides were taken in New York City during 2020 . The data is publicly available as documented at https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page and I have made a subset available on dropbox as described in the Setup section above for convenience. Documentation can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf In order to demonstrate large data problems and solutions I\u2019m going to artificially limit my system to 4Gb of memory. This will allow us to quickly see what happens when we reach the memory limit, and to look at solutions to that problem without waiting for our program to read in hundreds of Gb of data. (There is no need to follow along with this step, the purpose is just to make sure we all know what happens when you run out of memory.) Start by looking at the file names and sizes: fhvhv_csv_files <- list.files ( \"original_csv\" , recursive = TRUE , full.names = TRUE ) data.frame ( file = fhvhv_csv_files , size_Mb = file.size ( fhvhv_csv_files ) / 1024 ^ 2 ) ## file size_Mb ## 1 original_csv/2020/01/fhvhv_tripdata_2020-01.csv 1243.4975 ## 2 original_csv/2020/02/fhvhv_tripdata_2020-02.csv 1313.2442 ## 3 original_csv/2020/03/fhvhv_tripdata_2020-03.csv 808.5597 ## 4 original_csv/2020/04/fhvhv_tripdata_2020-04.csv 259.5806 ## 5 original_csv/2020/05/fhvhv_tripdata_2020-05.csv 366.5430 ## 6 original_csv/2020/06/fhvhv_tripdata_2020-06.csv 454.5977 ## 7 original_csv/2020/07/fhvhv_tripdata_2020-07.csv 599.2560 ## 8 original_csv/2020/08/fhvhv_tripdata_2020-08.csv 667.6880 ## 9 original_csv/2020/09/fhvhv_tripdata_2020-09.csv 728.5463 ## 10 original_csv/2020/10/fhvhv_tripdata_2020-10.csv 798.4743 ## 11 original_csv/2020/11/fhvhv_tripdata_2020-11.csv 698.0638 ## 12 original_csv/2020/12/fhvhv_tripdata_2020-12.csv 700.6804 We can already guess based on these file sizes that with only 4 Gb of RAM available we\u2019re going to have a problem. library ( tidyverse ) fhvhv_data <- map ( fhvhv_csv_files , read_csv ) %>% bind_rows ( show_col_types = FALSE ) ## Error in eval(expr, envir, enclos): cannot allocate vector of size 7.6 Mb Perhaps you\u2019ve seen similar messages before. Basically it means that we don\u2019t have enough memory available to hold the data we want to work with. I previously ran the code chunk above with more memory and found that it required just over 16 Gb. How can we work with data when we only have 1/4 of the memory requirement available? An example of a large object causing a bottleneck General strategies and principles Part of the problem with our first attempt is that CSV files do not make it easy to quickly read subsets or select columns. In this section we\u2019ll spend some time identifying strategies for working with large data and identify some tools that make it easy to implement those strategies. Use a fast binary data storage format that enables reading data subsets CSVs, TSVs, and similar delimited files are all text-based formats that are typically used to store tabular data. Other more general text-based data storage formats are in wide use as well, including XML and JSON. These text-based formats have the advantage of being both human and machine readable, but text is a relatively inefficient way to store data, and loading it into memory requires a time-consuming parsing process to separate out the fields and records. As an alternative to text-based data storage formats, binary formats have the advantage of being more space efficient on disk and faster to read. They often employ advanced compression techniques, store metadata, and allow fast selective access to data subsets. These substantial advantages come at the cost of human readability; you cannot easily inspect the contents of binary data files directly. If you are concerned with reducing memory use or data processing time this is probably a trade-off you are happy to make. The parquet binary storage format is among the best currently available. Support in R is provided by the arrow package. In a moment we\u2019ll see how we can use the arrow package to dramatically reduce the time it takes to get data from disk to memory and back. Partition the data on disk to facilitate chunked access and computation Memory requirements can be reduced by partitioning the data and computation into chunks, running each one sequentially, and combining the results at the end. It is common practice to partition the data on disk storage to make this computational strategy more natural and efficient. For example, the taxi data is already partitioned by year and month. Only read in the data you need If we think carefully about it we\u2019ll see that our previous attempt to process the taxi data by reading in all the data at once was wasteful. Not all the rows are Lyft rides, and the only column I really need is the one that tells me if the ride was operated by Lyft or not. I can perform the computation I need by only reading in that one column, and only the rows for which the hvfhs_license_num column is equal to HV0005 (Lyft). Use streaming data tools and algorithms It\u2019s all fine and good to say \u201conly read the data you need\u201d, but how do you actually do that? Unless you have full control over the data collection and storage process, chances are good that your data provider included a bunch of stuff you don\u2019t need. The key is to find a data selection and filtering tool that works in a streaming fashion so that you can access subsets without ever loading data you don\u2019t need into memory. Both the arrow and duckdb R packages support this type of workflow and can dramatically reduce the time and hardware requirements for many computations. Moreover, processing data in a streaming fashion without needing to load it into memory is a general technique that can be applied to other tasks as well. For example the duckdb package allows you to carry out data aggregation in a streaming fashion, meaning that you can compute summary statistics for data that is too large to fit in memory. Avoid unnecessarily storing or duplicating data in memory It is also important to pay some attention to storing and processing data efficiently once we have it loaded in memory. R likes to make copies of the data, and while it does try to avoid unnecessary duplication this process can be unpredictable. At a minimum you can remove or avoid storing intermediate results you don\u2019t need and take care not to make copies of your data structures unless you have to. The data.table package additionally makes it easier to efficiently modify R data objects in-place, reducing the risk of accidentally or unknowingly duplicating large data structures. Technique summary We\u2019ve accumulated a list of helpful techniques! To review: Use a fast binary data storage format that enables reading data subsets Partition the data on disk to facilitate chunked access and computation Only read in the data you need Use streaming data tools and algorithms Avoid unnecessarily storing or duplicating data in memory Solution example Now that we have some theoretical foundations to build on we can start putting these techniques into practice. Using the techniques identified above will allow us to overcome the memory limitation we ran up against before, and finally answer the question \u201c How many Lyft rides were taken in New York City during 2020? \u201d? Convert .csv to parquet The first step is to take the slow and inefficient text-based data provided by the city of New York and convert it to parquet using the arrow package. This is a one-time up-front cost that may be expensive in terms of time and/or computational resources. If you plan to work with the data a lot it will be well worth it because it allows subsequent reads to be faster and more memory efficient. library ( arrow ) if ( ! dir.exists ( \"converted_parquet\" )) { dir.create ( \"converted_parquet\" ) ## this doesn't yet read the data in, it only creates a connection csv_ds <- open_dataset ( \"original_csv\" , format = \"csv\" , partitioning = c ( \"year\" , \"month\" )) ## this reads each csv file in the csv_ds dataset and converts it to a .parquet file write_dataset ( csv_ds , \"converted_parquet\" , format = \"parquet\" , partitioning = c ( \"year\" , \"month\" )) } This conversion is relatively easy (even with limited memory) because the data provider is already using one of our strategies, i.e., they partitioned the data by year/month. This allows us to convert each file one at a time, without ever needing to read in all the data at once. We also took care to preserve the year/month partition into sub-directories. We improved on the implementation by using what is known as \u201chive-style\u201d partitioning, i.e., including both the variable names and values in the directory names. This is convenient because it makes it easy for arrow (and other tools that recognize the hive partitioning standard) to automatically recognize the partitions. We can look at the converted files and compare the naming scheme and storage requirements to the original CSV data. fhvhv_csv_files <- list.files ( \"original_csv\" , recursive = TRUE , full.names = TRUE ) fhvhv_files <- list.files ( \"converted_parquet\" , full.names = TRUE , recursive = TRUE ) data.frame ( csv_file = fhvhv_csv_files , parquet_file = fhvhv_files , csv_size_Mb = file.size ( fhvhv_csv_files ) / 1024 ^ 2 , parquet_size_Mb = file.size ( fhvhv_files ) / 1024 ^ 2 ) CSV Parquet csv_file parquet_file size_Mb size_Mb fhvhv_tripdata_2020-01.csv year=2020/month=1/part-0.parquet 1243.4975 190.26387 fhvhv_tripdata_2020-02.csv year=2020/month=10/part-0.parquet 1313.2442 125.17837 fhvhv_tripdata_2020-03.csv year=2020/month=11/part-0.parquet 808.5597 110.92144 fhvhv_tripdata_2020-04.csv year=2020/month=12/part-0.parquet 259.5806 111.67697 fhvhv_tripdata_2020-05.csv year=2020/month=2/part-0.parquet 366.5430 198.87074 fhvhv_tripdata_2020-06.csv year=2020/month=3/part-0.parquet 454.5977 127.53637 fhvhv_tripdata_2020-07.csv year=2020/month=4/part-0.parquet 599.2560 48.32047 fhvhv_tripdata_2020-08.csv year=2020/month=5/part-0.parquet 667.6880 64.17768 fhvhv_tripdata_2020-09.csv year=2020/month=6/part-0.parquet 728.5463 76.45972 fhvhv_tripdata_2020-10.csv year=2020/month=7/part-0.parquet 798.4743 97.99151 fhvhv_tripdata_2020-11.csv year=2020/month=8/part-0.parquet 698.0638 107.80694 fhvhv_tripdata_2020-12.csv year=2020/month=9/part-0.parquet 700.6804 115.25221 As expected, the binary parquet storage format is much more compact than the text-based CSV format. This is one reason that reading parquet data is so much faster: ## tidyverse csv reader system.time ( invisible ( readr :: read_csv ( fhvhv_csv_files [[ 1 ]], show_col_types = FALSE ))) ## user system elapsed ## 79.982 6.362 31.824 ## arrow package parquet reader system.time ( invisible ( read_parquet ( fhvhv_files [[ 1 ]]))) ## user system elapsed ## 5.761 2.226 22.533 Read and count Lyft records with arrow The arrow package makes it easy to read and process only the data we need for a particular calculation. It allows us to use the partitioned data directories we created earlier as a single dataset and to query it using the dplyr verbs many R users are already familiar with. Start by creating a dataset representation from the partitioned data directory: fhvhv_ds <- open_dataset ( \"converted_parquet\" , schema = schema ( hvfhs_license_num = string (), dispatching_base_num = string (), pickup_datetime = string (), dropoff_datetime = string (), PULocationID = int64 (), DOLocationID = int64 (), SR_Flag = int64 (), year = int32 (), month = int32 ())) Because we have hive-style directory names open_dataset automatically recognizes the partitions. Note that usually we do not need to manually specify the schema , we do so here to work around an issue with duckdb support. Importantly, open_dataset doesn\u2019t actually read the data into memory. It just opens a connection to the dataset and makes it easy for us to query it. Finally, we can compute the number of NYC Lyft trips in 2020, even on a machine with limited memory: library ( dplyr , warn.conflicts = FALSE ) fhvhv_ds %>% filter ( hvfhs_license_num == \"HV0005\" ) %>% select ( hvfhs_license_num ) %>% collect () %>% summarize ( total_Lyft_trips = n ()) ## # A tibble: 1 \u00d7 1 ## total_Lyft_trips ## <int> ## 1 37250101 Note that arrow datasets do not support summarize natively, that is why we call collect first to actually read in the data. The arrow package makes it fast and easy to query on-disk data and read in only the fields and records needed for a particular computation. This is a tremendous improvement over the typical R workflow, and may well be all you need to start using your large datasets more quickly and conveniently, even on modest hardware. Efficiently query taxi data with duckdb If you need even more speed and convenience you can use the duckdb package. It allows you to query the same parquet datasets partitioned on disk as we did above. You can use either SQL statements via the DBI package or tidyverse style verbs using dbplyr . Let\u2019s see how it works. First we create a duckdb table from our arrow dataset. library ( duckdb ) library ( dplyr ) con <- DBI :: dbConnect ( duckdb :: duckdb ()) fhvhv_tbl <- to_duckdb ( fhvhv_ds , con , \"fhvhv\" ) The duckdb table can be queried using tidyverse style verbs or SQL. ## number of Lyft trips, tidyverse style fhvhv_tbl %>% filter ( hvfhs_license_num == \"HV0005\" ) %>% select ( hvfhs_license_num ) %>% count () ## # Source: lazy query [?? x 1] ## # Database: duckdb_connection ## n ## <dbl> ## 1 37250101 ## number of Lyft trips, SQL style y <- dbSendQuery ( con , \"SELECT COUNT(*) FROM fhvhv WHERE hvfhs_license_num=='HV0005';\" ) dbFetch ( y ) ## count_star() ## 1 37250101 The main advantages of duckdb are that it has full SQL support, supports aggregating data in a streaming fashion, allows you to set memory limits, and is optimized for speed. The way I think about the relationship between arrow and duckdb is that arrow is primarily about reading and writing data as fast and efficiently as possible, with some built-in analysis capabilities, while duckdb is a database engine with more complete data manipulation and aggregation capabilities. It can be instructive to compare arrow and duckdb capabilities and performance using a slightly more complicated example. Here we compute a grouped average using arrow : system.time ({ fhvhv_ds %>% filter ( hvfhs_license_num == \"HV0005\" ) %>% select ( hvfhs_license_num , month ) %>% group_by ( hvfhs_license_num ) %>% collect () %>% summarize ( avg = mean ( month , na.rm = TRUE )) %>% print ()}) ## # A tibble: 1 \u00d7 2 ## hvfhs_license_num avg ## <chr> <dbl> ## 1 HV0005 6.10 ## user system elapsed ## 19.456 4.064 14.254 note that we use collect to read the data into memory before the summarize step because arrow does not support aggregating in a streaming fashion. Here is the same query using duckdb : system.time ({ fhvhv_tbl %>% filter ( hvfhs_license_num == \"HV0005\" ) %>% select ( hvfhs_license_num , month ) %>% group_by ( hvfhs_license_num ) %>% summarize ( avg = mean ( month , na.rm = TRUE )) %>% print ()}) ## # Source: lazy query [?? x 2] ## # Database: duckdb_connection ## hvfhs_license_num avg ## <chr> <dbl> ## 1 HV0005 6.10 ## user system elapsed ## 18.766 1.251 8.984 note that it is slightly faster, and we don\u2019t need to read as much data into memory because duckdb supports aggregating in a streaming fashion. This capability is very powerful because it allows us to perform computations on data that is too big to fit into memory. Your turn! Now that you understand some of the basic techniques for working with large data and have seen an example, you can start to apply what you\u2019ve learned. Using the same taxi data, try answering the following questions: What percentage of trips are made by Lyft? In which month did Lyft log the most trips? Documentation for these data can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf Additional resources Arrow R package documentation Arrow Python package documentation DuckDB documentation","title":"Large data in R"},{"location":"tutorials/large_data_R/#large-data-in-r-tools-and-techniques","text":"Updated December 06, 2021","title":"Large Data in R: Tools and Techniques"},{"location":"tutorials/large_data_R/#set-up-and-example-data","text":"The examples and exercises require R and several R packages, including tidyverse , data.table , arrow , and duckdb . This software is all installed and ready to use on the HBS Grid. If running elsewhere make sure these required software programs are installed before proceeding. You can download and extract the data used in the examples and exercises from https://www.dropbox.com/s/vbodicsu591o7lf/original_csv.zip?dl=1 (this is a 1.3Gb zip file). These data record for-hire vehicle (aka \u201cride sharing\u201d) trips in NYC in 2020. Each row contains the record of a trip and the variable descriptions can be found in https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf","title":"Set Up and Example Data"},{"location":"tutorials/large_data_R/#nature-and-scope-of-the-problem-what-is-large-data","text":"Most popular data analysis software is designed to operate on data stored in random access memory (aka just \u201cmemory\u201d or \u201cRAM\u201d). This makes modifying and copying data very fast and convenient, until you start working with data that is too large for your computer\u2019s memory system. At that point you have two options: get a bigger computer or modify your workflow to process the data more carefully and efficiently. This workshop focuses on option two, using the arrow and duckdb packages in R to work with data without necessarily loading it all into memory at once. A common definition of \u201cbig data\u201d is \u201cdata that is too big to process using traditional software\u201d. We can use the term \u201clarge data\u201d as a broader category of \u201cdata that is big enough that you have to pay attention to processing it efficiently\u201d. In a typical (traditional) program, we start with data on disk, in some format. We read it in to memory, do some stuff to it on the CPU, store the results of that stuff back in memory, then write those results back to disk so they can be available for the future, as depicted below. The reason most data analysis software is designed to process data this way is because \u201cdoing some stuff\u201d is much much faster in RAM than it is if you have to read values from disk every time you need them. The downside is that RAM is much more expensive than disk storage, and typically available in smaller quantities. Memory can only hold so much data and we must either stay under that limit or buy more memory.","title":"Nature and Scope of the Problem: What is Large Data?"},{"location":"tutorials/large_data_R/#problem-example","text":"Grounding our discussion in a concrete problem example will help make things clear. I want to know how many Lyft rides were taken in New York City during 2020 . The data is publicly available as documented at https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page and I have made a subset available on dropbox as described in the Setup section above for convenience. Documentation can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf In order to demonstrate large data problems and solutions I\u2019m going to artificially limit my system to 4Gb of memory. This will allow us to quickly see what happens when we reach the memory limit, and to look at solutions to that problem without waiting for our program to read in hundreds of Gb of data. (There is no need to follow along with this step, the purpose is just to make sure we all know what happens when you run out of memory.) Start by looking at the file names and sizes: fhvhv_csv_files <- list.files ( \"original_csv\" , recursive = TRUE , full.names = TRUE ) data.frame ( file = fhvhv_csv_files , size_Mb = file.size ( fhvhv_csv_files ) / 1024 ^ 2 ) ## file size_Mb ## 1 original_csv/2020/01/fhvhv_tripdata_2020-01.csv 1243.4975 ## 2 original_csv/2020/02/fhvhv_tripdata_2020-02.csv 1313.2442 ## 3 original_csv/2020/03/fhvhv_tripdata_2020-03.csv 808.5597 ## 4 original_csv/2020/04/fhvhv_tripdata_2020-04.csv 259.5806 ## 5 original_csv/2020/05/fhvhv_tripdata_2020-05.csv 366.5430 ## 6 original_csv/2020/06/fhvhv_tripdata_2020-06.csv 454.5977 ## 7 original_csv/2020/07/fhvhv_tripdata_2020-07.csv 599.2560 ## 8 original_csv/2020/08/fhvhv_tripdata_2020-08.csv 667.6880 ## 9 original_csv/2020/09/fhvhv_tripdata_2020-09.csv 728.5463 ## 10 original_csv/2020/10/fhvhv_tripdata_2020-10.csv 798.4743 ## 11 original_csv/2020/11/fhvhv_tripdata_2020-11.csv 698.0638 ## 12 original_csv/2020/12/fhvhv_tripdata_2020-12.csv 700.6804 We can already guess based on these file sizes that with only 4 Gb of RAM available we\u2019re going to have a problem. library ( tidyverse ) fhvhv_data <- map ( fhvhv_csv_files , read_csv ) %>% bind_rows ( show_col_types = FALSE ) ## Error in eval(expr, envir, enclos): cannot allocate vector of size 7.6 Mb Perhaps you\u2019ve seen similar messages before. Basically it means that we don\u2019t have enough memory available to hold the data we want to work with. I previously ran the code chunk above with more memory and found that it required just over 16 Gb. How can we work with data when we only have 1/4 of the memory requirement available? An example of a large object causing a bottleneck","title":"Problem example"},{"location":"tutorials/large_data_R/#general-strategies-and-principles","text":"Part of the problem with our first attempt is that CSV files do not make it easy to quickly read subsets or select columns. In this section we\u2019ll spend some time identifying strategies for working with large data and identify some tools that make it easy to implement those strategies.","title":"General strategies and principles"},{"location":"tutorials/large_data_R/#solution-example","text":"Now that we have some theoretical foundations to build on we can start putting these techniques into practice. Using the techniques identified above will allow us to overcome the memory limitation we ran up against before, and finally answer the question \u201c How many Lyft rides were taken in New York City during 2020? \u201d?","title":"Solution example"},{"location":"tutorials/large_data_R/#your-turn","text":"Now that you understand some of the basic techniques for working with large data and have seen an example, you can start to apply what you\u2019ve learned. Using the same taxi data, try answering the following questions: What percentage of trips are made by Lyft? In which month did Lyft log the most trips? Documentation for these data can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf","title":"Your turn!"},{"location":"tutorials/large_data_R/#additional-resources","text":"Arrow R package documentation Arrow Python package documentation DuckDB documentation","title":"Additional resources"},{"location":"tutorials/usecases/","text":"This section contains longer tutorials and examples showing how you can carry out common tasks using the HBS Grid. Select a topic from the menu.","title":" &nbsp; "},{"location":"userguide/commandline/","text":"The HBS Grid uses IBM Spectrum LSF to run applications on powerful remote computers. LSF is a large and complex set of tools; our goal here is to give you just enough information so that you can use it to run jobs on our system, without overwhelming you with details and options. Note This software environment includes robust graphical tools that reduce the need to use the command line for many interactive tasks. This section is for those who prefer the command line, either for aesthetic reasons or because they need to submit batch jobs or carry out complex operations that cannot be easily performed using graphical menu-driven tools. LSF provides bsub , a command-line program for running applications on powerful remote computers. For example, you can use bsub -q short_int -Is R to start an interactive R job on a compute node. Breaking this example down will make the basics of bsub clear: bsub ( b atch sub mission) is the top-level command used to run applications on powerful remote machines. -q short_int means you want to run on the short int eractive q ueue (details below). -Is means we are running an I nteractive s hell. The rest of the command ( R in this case) is the command that will be run on the remote machine. Compute cluster basics When you first log in to the HBS Grid using NoMachine or ssh you are running on what we call a \"login node\". The login nodes do not have substantial CPU or RAM available. All computationally intensive processes should be run on what we call \"compute nodes\". A diagram of the HBS Grid architecture helps make this clear: As this diagram shows, the primary purpose of the login nodes is to serve as a hub for launching jobs on powerful compute nodes . You can do that from the command line using bsub or from the desktop menu using application launchers . You may sometimes wish to run applications on the login node , and this is perfectly fine as long as you are not using it for computationally intensive work. For example, you may wish to run ipython to work out a small code example, or use locate to find a file you were working on. These low-resource activities can and should be done on the login node . The important thing to remember is that bsub is used to run commands on powerful compute nodes . Resource requirements The bsub command allows you to specify RAM and CPU requirements for your job via the -M and -n arguments. For example, you can run a python job with 50 GB of RAM and 4 CPUs with bsub -q short_int -M 50G -n 4 -Is python Knowing just these arguments to bsub will take you a long way. There is much more to know about bsub , but these basics will get you started. Interactive and batch queue limits Machines on the HBS Grid are grouped in queues and bsub can start jobs in either batch (background) or interactive modes. Batch jobs make it easier to run many jobs at once and are more efficient because jobs don't keep running after the program is executed. Interactive jobs on the other hand tend to be more convenient, especially for exploratory work or when developing or debugging a script or program. Batch queues including short and long are for running commands without interaction. For example bsub -q short Rscript my_r_code.R runs my_r_code.R in batch mode, and bsub -q short stata -b my_stata_code.do runs my_stata_code.do in batch mode. Info The key differences when submitting batch vs interactive jobs are the -q and -Is arguments. For example we used -q short for batch and -q short_int for interactive. Interactive jobs must also include the -Is option. Interactive queues like short_int and long_int are used to run applications that you will interact with. For example, bsub -q short_int -Is rstudio runs an interactive RStudio application, and bsub -q short_int -Is xstata runs an interactive Stata application. Queues have other characteristics in addition to the batch vs. interactive distinction. These include the maximum run time and maximum number of CPUs that can be reserved per job. These queue-level limits are summarized in the table below. Queue Type Length Max Cores/Job long_int interactive 3 days 4 short_int interactive 1 day 12 sas_int interactive no limit 4 long batch 7 days 12 short batch 3 days 16 gpu interactive or batch no limit 4 sas batch no limit 4 unlimited interactive or batch no limit 4 Software environments The available software environments contain a huge selection of graphical and terminal-based tools and are described in Software Applications and Environments . For the most part you shouldn't need to do anything to install or configure these tools -- if there is some software you would like that we don't have please get in touch and we'll see if we can install and set it up for you. In order to facilitate reproducible research and analysis we preserve old software environments so that you can switch back to them later if needed. These older environments can be loaded using Lmod . Running ml avail will show you the available environments, named by date and version number. For example, suppose that you have a python project and that your pandas code no longer works with the latest pandas release in the current software environment. In that case you can revert to a previous software environment and run your analysis using an older version of pandas. You can use the ml command from the terminal to list , load , and unload Lmod environment, as shown below. ml avail -------------- /usr/local/app/rcs_bin/techpreview-dev/modulefiles -------------- rcs/rcs_2020.01 (E) rcs/rcs_2021.01 (E) rcs/rcs_2021.03 (E,L,D) Where: D: Default Module E: Technology Preview L: Module is loaded Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". You can get detailed information about specific software modules using the ml spyder command: module spyder rcs/rcs_2021.03 --------------------------------------------------------------------------- rcs: rcs/rcs_2021.03 --------------------------------------------------------------------------- Description: Conda environment for research computing Help: Sets up environment for Data Science and Statistical computing. A huge list of software is avalable, including 'python', 'spyder', 'R', 'rstudio', 'emacs', 'vscode', rclone, ripgrep, nnn and much more. Key software versions: libgcc-ng 9 cudatoolkit 10.1 tensorflow-gpu 2.2 python 3.8 jupyterlab 3.0 numpy 1.20 pandas 1.2 r-base 4.0 r-tidyverse 1.3 sas 9.4 stata 16 octave 6.2 mathematica 12 matlab R2020a emacs 27.1 QGIS 3.16 For a detailed software list open a terminal and run conda env export -n rcs_2021.03 Finally you can use ml to load and unload specific environments. ml rcs_2021.03 will load the rcs_2021.03 environment, and ml -rcs_2021.03 will unload it. Detailed Lmod documentation is available here and you can learn more about the environments available on the HBS Grid in the Environments documentation .","title":"\u2503> &nbsp;Use the Command-line"},{"location":"userguide/commandline/#resource-requirements","text":"The bsub command allows you to specify RAM and CPU requirements for your job via the -M and -n arguments. For example, you can run a python job with 50 GB of RAM and 4 CPUs with bsub -q short_int -M 50G -n 4 -Is python Knowing just these arguments to bsub will take you a long way. There is much more to know about bsub , but these basics will get you started.","title":"Resource requirements"},{"location":"userguide/commandline/#interactive-and-batch-queue-limits","text":"Machines on the HBS Grid are grouped in queues and bsub can start jobs in either batch (background) or interactive modes. Batch jobs make it easier to run many jobs at once and are more efficient because jobs don't keep running after the program is executed. Interactive jobs on the other hand tend to be more convenient, especially for exploratory work or when developing or debugging a script or program. Batch queues including short and long are for running commands without interaction. For example bsub -q short Rscript my_r_code.R runs my_r_code.R in batch mode, and bsub -q short stata -b my_stata_code.do runs my_stata_code.do in batch mode. Info The key differences when submitting batch vs interactive jobs are the -q and -Is arguments. For example we used -q short for batch and -q short_int for interactive. Interactive jobs must also include the -Is option. Interactive queues like short_int and long_int are used to run applications that you will interact with. For example, bsub -q short_int -Is rstudio runs an interactive RStudio application, and bsub -q short_int -Is xstata runs an interactive Stata application. Queues have other characteristics in addition to the batch vs. interactive distinction. These include the maximum run time and maximum number of CPUs that can be reserved per job. These queue-level limits are summarized in the table below. Queue Type Length Max Cores/Job long_int interactive 3 days 4 short_int interactive 1 day 12 sas_int interactive no limit 4 long batch 7 days 12 short batch 3 days 16 gpu interactive or batch no limit 4 sas batch no limit 4 unlimited interactive or batch no limit 4","title":"Interactive and batch queue limits"},{"location":"userguide/commandline/#software-environments","text":"The available software environments contain a huge selection of graphical and terminal-based tools and are described in Software Applications and Environments . For the most part you shouldn't need to do anything to install or configure these tools -- if there is some software you would like that we don't have please get in touch and we'll see if we can install and set it up for you. In order to facilitate reproducible research and analysis we preserve old software environments so that you can switch back to them later if needed. These older environments can be loaded using Lmod . Running ml avail will show you the available environments, named by date and version number. For example, suppose that you have a python project and that your pandas code no longer works with the latest pandas release in the current software environment. In that case you can revert to a previous software environment and run your analysis using an older version of pandas. You can use the ml command from the terminal to list , load , and unload Lmod environment, as shown below. ml avail -------------- /usr/local/app/rcs_bin/techpreview-dev/modulefiles -------------- rcs/rcs_2020.01 (E) rcs/rcs_2021.01 (E) rcs/rcs_2021.03 (E,L,D) Where: D: Default Module E: Technology Preview L: Module is loaded Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". You can get detailed information about specific software modules using the ml spyder command: module spyder rcs/rcs_2021.03 --------------------------------------------------------------------------- rcs: rcs/rcs_2021.03 --------------------------------------------------------------------------- Description: Conda environment for research computing Help: Sets up environment for Data Science and Statistical computing. A huge list of software is avalable, including 'python', 'spyder', 'R', 'rstudio', 'emacs', 'vscode', rclone, ripgrep, nnn and much more. Key software versions: libgcc-ng 9 cudatoolkit 10.1 tensorflow-gpu 2.2 python 3.8 jupyterlab 3.0 numpy 1.20 pandas 1.2 r-base 4.0 r-tidyverse 1.3 sas 9.4 stata 16 octave 6.2 mathematica 12 matlab R2020a emacs 27.1 QGIS 3.16 For a detailed software list open a terminal and run conda env export -n rcs_2021.03 Finally you can use ml to load and unload specific environments. ml rcs_2021.03 will load the rcs_2021.03 environment, and ml -rcs_2021.03 will unload it. Detailed Lmod documentation is available here and you can learn more about the environments available on the HBS Grid in the Environments documentation .","title":"Software environments"},{"location":"userguide/environments/","text":"A huge range of software applications, utilities, and libraries are installed and configured for you. Whether you need Rstudio or Spyder , Julia running in VSCode , popular R or Python packages, of fully configured Jupyter Notebooks , we have you covered. Available software The list of installed software is so large we make no effort to enumerate everything here, but you can always get an up-to-date list by opening a terminal on the Grid and running conda list Start with the expectation that all the software you need is already installed and ready to use . If that expectation is ever broken please put in a request using our discussion forum or issue tracker . Using older software environments Each time we update our software environments we preserve previous versions so that you can roll back for reproducibility or if your code stops working after an update. This section details the specific software environment versions available. As an illustration of the benefits of preserving historical software environments, imagine that you have a python project and that your Pandas code no longer works with the latest Pandas release in the current software environment. In that case you can start Spyder and revert to a previous software environment in order to run your analysis using an older version of Pandas . The screen-shot below shows how to use the Software environment version selector to run and older version of python . Software environments are named following a rcs_year.version scheme. For example, the first environment released in 2021 is named rcs_2021.01 . The list below shows you key information about each environment, including a command that you can run from the terminal to get a detailed software version list. Software environment versions Current and historical software environments are described below. rcs_2022.01 After more than 6 months of hard work, the HBS grid technology preview software environment version 2022.01 was released in January 2022! This environment remains a technology preview and any and all use is at your own risk . This technology preview software environment is a user-friendly set of software and utilities designed to make data science and statistics easier for HBS Grid users. If you have not yet done so, you can try it by following the quick-start guide . If you are already using the technology preview environment you will be prompted to upgrade next time you log in to the HBS Grid. As always you can continue using previous versions if needed, as described in the environments documentation . In this release we have added a large number of new statistics and data science applications and packages, including: JASP , a free menu-driven statistics application similar to SPSS Cytoscape , an open source software platform for visualizing complex networks, DuckDB , an in-process SQL OLAP database management system texminer , functions for text mining and topic modeling in R Dedupe , a library that uses machine learning to perform de-duplication and entity resolution in Python awscli , a unified tool to manage your AWS services snakemake , a workflow management system to create reproducible and scalable data analyses and many many more! If you find a software program that you need is not yet available please let us know and we will try to install it for you. The 2022.01 release also brings a huge number of application and package updates, including: Python updated to 3.9.9 R updated to 4.1.1 Octave updaed to 6.4 Julia updated to 1.7.1 RStudio updated to 2021.09.1 Spyder updated to 5.2.1 LibreOffice updated to 7.1.8 VSCode updated to 1.63.2 Emacs updated to 27.2 Arrow (C++, R and Python) updated to 6.0 Tensorflow updated to 2.7 PyTorch updated to 1.10.0 CUDA toolkit updated to 11.5.0 Jupyterlab updated to 3.10 MKL updated to 2021.4.0 and hundreds of others. In this release we have also dropped support for several infrequently used programs: OCRfeeder -- use gImageReader for OCR instead Gephi -- replaced by Cytoscape for network visualization PSPP -- replaced by JASP, a modern statistics GUI that uses R under the hood Meld -- use Diffuse for graphical text comparisony Documentation is available on line or via the HBS Grid help application on the Grid. If you have any difficulties or feature requests please reach out on the discussion forum . VirtualBox image available for download from Dropbox and can be imported and run locally for convenience, reproducibility, or testing purposes. For complete environment details, open a terminal and run conda env export -n rcs_2022.01 rcs_2021.06 The rcs_2021.06 environment was released in May 2021. It includes updated Octave , Python , QGIS , R , Stata , and other software. Key software versions included in this environment are listed below. CUDAtoolkit 11.2 Spyder 5.0 Texlive 2021 Emacs 27.2 Julia 1.6.1 Jupyterlab 3.0 Mathematica 12 Matlab R2021a Numpy 1.20 Octave 6.2 Pandas 1.2 Python 3.9 Pytorch 1.8 QGIS 3.18 R 4.0 R-tidyverse 1.3 SAS 9.4 Stata 17 Tensorflow 2.4 For complete environment details, open a terminal and run conda env export -n rcs_2021.06 rcs_2021.03 The rcs_2021.03 environment was released in March 2021. It includes updated Octave , Python , QGIS , R , Stata , and other software. Key software versions included in this environment are listed below. CUDAtoolkit 10.1 Emacs 27.1 Julia 1.5.3 Jupyterlab 3.0 Mathematica 12 Matlab R2020a Numpy 1.20 Octave 6.2 Pandas 1.2 Python 3.8 Pytorch 1.7 QGIS 3.16 R 4.0 R-tidyverse 1.3 SAS 9.4 Stata 16 Tensorflow 2.2 For complete environment details, open a terminal and run conda env export -n rcs_2021.03 rcs_2020.01 The rcs_2020.01 environment was released in March 2020. It includes updated Octave , Python , QGIS , R , Stata , and other software. Key software versions included in this environment are listed below. CUDAtoolkit 10.1 Emacs 27.1 Julia 1.5.3 Jupyterlab 2 Mathematica 12 Matlab R2019a Numpy 1.19 Octave 6.2 Pandas 1.2 Python 3.7 R 3.6 R-tidyverse 1.2 SAS 9.4 Stata 15 Tensorflow 2.2 For complete environment details, open a terminal and run conda env export -n rcs_2020.01","title":"\ud83d\udce6 &nbsp;&nbsp;Use Software Versions"},{"location":"userguide/environments/#available-software","text":"The list of installed software is so large we make no effort to enumerate everything here, but you can always get an up-to-date list by opening a terminal on the Grid and running conda list Start with the expectation that all the software you need is already installed and ready to use . If that expectation is ever broken please put in a request using our discussion forum or issue tracker .","title":"Available software"},{"location":"userguide/environments/#using-older-software-environments","text":"Each time we update our software environments we preserve previous versions so that you can roll back for reproducibility or if your code stops working after an update. This section details the specific software environment versions available. As an illustration of the benefits of preserving historical software environments, imagine that you have a python project and that your Pandas code no longer works with the latest Pandas release in the current software environment. In that case you can start Spyder and revert to a previous software environment in order to run your analysis using an older version of Pandas . The screen-shot below shows how to use the Software environment version selector to run and older version of python . Software environments are named following a rcs_year.version scheme. For example, the first environment released in 2021 is named rcs_2021.01 . The list below shows you key information about each environment, including a command that you can run from the terminal to get a detailed software version list.","title":"Using older software environments"},{"location":"userguide/environments/#software-environment-versions","text":"Current and historical software environments are described below.","title":"Software environment versions"},{"location":"userguide/menulaunch/","text":"You can run applications on powerful HBS Grid compute nodes by connecting to the HBS grid via NoMachine, enabling technology preview features , and clicking one of the application icons under the Applications or Activities menus. This allows you to easily run compute and/or memory intensive applications with just a few mouse clicks! Basic launcher options Each application will open a dialog where you can configure your environment and resource requirements. Info Please keep in mind that the system reserves the resources you select , e.g., CPUs used by your job become unavailable for other users. Don't be shy about using resources, but please don't go overboard and reserve way more then you need just because you can. The application launchers are meant to be intuitive and easy to use. Most of the fields should be self-explanatory, e.g., there is a numeric field for memory (RAM) in Gigabytes, and another for the number of CPUs needed. As a convenience you can select a starting directory. Specific resource requirements depend on the nature of the job, but as a rough guide we recommend requesting RAM 4-10 times the size of your data . For example, if you have a 6 Gb .csv file you may wish to request 24GB of memory or so. Click the video thumbnail below to watch an application launcher demonstration: Your browser does not support the video tag. Advanced launcher options Info It is a good idea to note the current software environment version when you start a new project so you will know which environment to switch back to if needed. If you need to use an older software environment you can do so using the software environment version dialog. Usually there is no need to use an environment other than the default ; the purpose of this mechanism is to make it easy to reproduce an old analysis using a specific software environment. The software and versions available in each environment are documented in Software Applications and Environments . Some application launchers have a Pre-submission command field. This allows you to run an arbitrary bash command immediately before submitting the job. For example, you can use it to set environment variables or activate conda environments . Handling system limits In some cases the desktop launchers will down-grade your request to the maximum available if your request exceeds the system specified limits. There are both user-level, queue-level and job-level limits on the resources that are available to you: You are limited to 3 concurrent interactive jobs. You are limited to a total of 24 CPUs allocated to interactive jobs at any given time. Interactive jobs are limited to 12 CPUs for up to 24 hours, or 4 CPUs for up to 72 hours. As a practical example of these limits, if you try request 12 CPUs and runtime greater than 24 hours the system will not be able to meet your request. In that case it will offer to give you 4 CPUS (the maximum available for jobs running more than 24 hours). More information about queue-level limits can be found in the command line documentation . Handling resource contention The HBS grid usually has substantial computational resources available, but sometimes an unusually large number of users are trying to use a lot of resources at the same time. In this case resources may become scarce and you may not be able to access the resources you need. You can get a quick overview of the state of the cluster using the HBS Grid available resources utility, available in the applications menu. This will give you a sense of the current activity on the cluster, and a rough idea of the resources currently available to you. Click the video thumbnail below to watch a demonstration of the available resources utility: Your browser does not support the video tag. Starting batch jobs The graphical menu-based launchers documented in this section are a quick and convenient way to run interactive applications on powerful compute nodes. In the case where you wish to run many such jobs you may find it more convenient to run batch jobs. There is a HBS Grid Batch Submission Utility in the application menu that provides basic batch submission capabilities. For more advanced use refer to the command line documentation .","title":"\ud83d\ude80 &nbsp;&nbsp;Run Desktop Applications"},{"location":"userguide/menulaunch/#basic-launcher-options","text":"Each application will open a dialog where you can configure your environment and resource requirements. Info Please keep in mind that the system reserves the resources you select , e.g., CPUs used by your job become unavailable for other users. Don't be shy about using resources, but please don't go overboard and reserve way more then you need just because you can. The application launchers are meant to be intuitive and easy to use. Most of the fields should be self-explanatory, e.g., there is a numeric field for memory (RAM) in Gigabytes, and another for the number of CPUs needed. As a convenience you can select a starting directory. Specific resource requirements depend on the nature of the job, but as a rough guide we recommend requesting RAM 4-10 times the size of your data . For example, if you have a 6 Gb .csv file you may wish to request 24GB of memory or so. Click the video thumbnail below to watch an application launcher demonstration: Your browser does not support the video tag.","title":"Basic launcher options"},{"location":"userguide/menulaunch/#advanced-launcher-options","text":"Info It is a good idea to note the current software environment version when you start a new project so you will know which environment to switch back to if needed. If you need to use an older software environment you can do so using the software environment version dialog. Usually there is no need to use an environment other than the default ; the purpose of this mechanism is to make it easy to reproduce an old analysis using a specific software environment. The software and versions available in each environment are documented in Software Applications and Environments . Some application launchers have a Pre-submission command field. This allows you to run an arbitrary bash command immediately before submitting the job. For example, you can use it to set environment variables or activate conda environments .","title":"Advanced launcher options"},{"location":"userguide/menulaunch/#handling-system-limits","text":"In some cases the desktop launchers will down-grade your request to the maximum available if your request exceeds the system specified limits. There are both user-level, queue-level and job-level limits on the resources that are available to you: You are limited to 3 concurrent interactive jobs. You are limited to a total of 24 CPUs allocated to interactive jobs at any given time. Interactive jobs are limited to 12 CPUs for up to 24 hours, or 4 CPUs for up to 72 hours. As a practical example of these limits, if you try request 12 CPUs and runtime greater than 24 hours the system will not be able to meet your request. In that case it will offer to give you 4 CPUS (the maximum available for jobs running more than 24 hours). More information about queue-level limits can be found in the command line documentation .","title":"Handling system limits"},{"location":"userguide/menulaunch/#handling-resource-contention","text":"The HBS grid usually has substantial computational resources available, but sometimes an unusually large number of users are trying to use a lot of resources at the same time. In this case resources may become scarce and you may not be able to access the resources you need. You can get a quick overview of the state of the cluster using the HBS Grid available resources utility, available in the applications menu. This will give you a sense of the current activity on the cluster, and a rough idea of the resources currently available to you. Click the video thumbnail below to watch a demonstration of the available resources utility: Your browser does not support the video tag.","title":"Handling resource contention"},{"location":"userguide/menulaunch/#starting-batch-jobs","text":"The graphical menu-based launchers documented in this section are a quick and convenient way to run interactive applications on powerful compute nodes. In the case where you wish to run many such jobs you may find it more convenient to run batch jobs. There is a HBS Grid Batch Submission Utility in the application menu that provides basic batch submission capabilities. For more advanced use refer to the command line documentation .","title":"Starting batch jobs"},{"location":"userguide/quickstart/","text":"The technology preview software and environments described in this documentation are not enabled on the HBS Grid by default. This guide show you how to connect to the HBS Grid and enable them. Prerequisites Note Skip this section if you are already using the HBS Grid and know how to connect using NoMachine. To get started you must have an HBS Grid account and be connected to the HBS network, either directly if you are on-campus or connect via VPN otherwise. You must also have the NoMachine Enterprise Client remote desktop application installed on your computer. Connect and enable software environments Once you have an account and are connected to the HBS network, follow this simple procedure to enable our in-development research computing environment: Example Log in to the HBS Grid by using NoMachine to connect to hosthbsgrid-nx.hbs.edu Open a Terminal in NoMachine: Applications => Favorites => Terminal Run /usr/local/app/rcs_bin/techpreview-dev/enable.sh & exit to open the grid configuration utility (you can copy/paste from the documentation to your Terminal). Select the Technology Preview environment in the welcome dialog and click OK. The video below demonstrates these steps visually. Your browser does not support the video tag. A selection of our most popular applications are available in the favorites list pinned to the task-bar. Additional application launchers can be found in the Applications menu or by searching in Activities. You can add applications to your favorites list by right-clicking and selecting Add to Favorites. Next steps It is our hope that this environment will be intuitive and user-friendly, and you are encouraged to start exploring the available software and tools. If you are doing real work you will probably want to refer to Mount Drives and Copy Data to learn how to get your data onto the HBS Grid. Additional documentation is available if you need it, including the Launch Applications from the Desktop and Start Jobs from the Terminal sections. If you run into any problems please let us know by posting at https://github.com/hbs-rcs/hbsgrid-docs/discussions and letting us know so we can fix them! You may also find the Support and Troubleshooting section helpful.","title":"\u2b8a &nbsp;&nbsp;Get Started"},{"location":"userguide/quickstart/#prerequisites","text":"Note Skip this section if you are already using the HBS Grid and know how to connect using NoMachine. To get started you must have an HBS Grid account and be connected to the HBS network, either directly if you are on-campus or connect via VPN otherwise. You must also have the NoMachine Enterprise Client remote desktop application installed on your computer.","title":"Prerequisites"},{"location":"userguide/quickstart/#connect-and-enable-software-environments","text":"Once you have an account and are connected to the HBS network, follow this simple procedure to enable our in-development research computing environment: Example Log in to the HBS Grid by using NoMachine to connect to hosthbsgrid-nx.hbs.edu Open a Terminal in NoMachine: Applications => Favorites => Terminal Run /usr/local/app/rcs_bin/techpreview-dev/enable.sh & exit to open the grid configuration utility (you can copy/paste from the documentation to your Terminal). Select the Technology Preview environment in the welcome dialog and click OK. The video below demonstrates these steps visually. Your browser does not support the video tag. A selection of our most popular applications are available in the favorites list pinned to the task-bar. Additional application launchers can be found in the Applications menu or by searching in Activities. You can add applications to your favorites list by right-clicking and selecting Add to Favorites.","title":"Connect and enable software environments"},{"location":"userguide/quickstart/#next-steps","text":"It is our hope that this environment will be intuitive and user-friendly, and you are encouraged to start exploring the available software and tools. If you are doing real work you will probably want to refer to Mount Drives and Copy Data to learn how to get your data onto the HBS Grid. Additional documentation is available if you need it, including the Launch Applications from the Desktop and Start Jobs from the Terminal sections. If you run into any problems please let us know by posting at https://github.com/hbs-rcs/hbsgrid-docs/discussions and letting us know so we can fix them! You may also find the Support and Troubleshooting section helpful.","title":"Next steps"},{"location":"userguide/syncfiles/","text":"The HBS Grid is primarily used for data analysis, machine learning, data wrangling, and data visualization. Usually this means that you need to copy or sync your data to the HBS Grid in order to do your work . Access local files from NoMachine login nodes This makes files accessible from the login nodes but not from the compute nodes . For that you need to copy your files to the HBS Grid . NoMachine makes it easy to mount your local file system on the HBS Grid login node. This is useful for reading documentation, scripts, and other small files without needing to physically copy anything to the HBS Grid. Mount your local file system to the HBS Grid: Log in to the HBS Grid by using NoMachine to connect to host hbsgrid-nx.hbs.edu . Press Ctrl-Alt-0 to open the NoMachine session menu. Click Devices Click Connect a disk . Locate the disk you want to mount under Local disks and click on it. click the Connect button to mount your local disk on the HBS Grid. Click the image below for a local drive mounting demonstration: Your browser does not support the video tag. Sync data from/to local storage NoMachine file mounting is useful, but it has a major limitation; drives mounted via NoMachine are not accessible by applications running on HBS Grid compute nodes . This means that you will usually want to copy or sync your data to the HBS Grid. The easiest way to do this for files on your local machine is to mount your local drive as described above, and then use grsync to sync files from the NoMachine mount to the HBS Grid. Because you must physically copy data to the HBS Grid in order to access it from the compute nodes , you have to decide where to put it. There are three options: home directory , project space , or scratch storage . HBS Grid storage overview A home directory was created at /export/home/<group>/<username> when you requested your account. Your home folder has limited storage capacity and is accessible only by you. Project spaces are directories that are shared and accessible by all HBS Grid users working on that project. You can request a new project space using the new project space request form and you can request modifications to an existing project space using the change request form . Scratch storage is available at /export/scratch . It is appropriate only for temporary, short-term storage. Files are not backed up and will be deleted after 60 days. Scratch storage is a shared resource accessible to all users on the HBS Grid; make sure you set permissions on your files accordingly . Once you have decided where to store your files on the HBS Grid you can follow the steps below to transfer them. Sync data from your local machine to the HBS Grid Log in to the HBS Grid and connect your local drive using NoMachine as described above. Identify the directory on the HBS Grid that you will copy your data too, creating it if needed. From the HBS Grid desktop, open the grsync application. Choose a source directory under the NoMachine mount and specify the target directory from step 2. Click the run button in the upper-right corner of the grsync application. Note that transferring many small files is much slower than transferring a small number of large files. You may find it faster to compress folders with many small files into .zip or .tar archives, transfer those, and decompress/extract them on the other end. Click the image below for a demonstration showing how to sync your data from a local drive to the HBS Grid: Your browser does not support the video tag. Sync data from/to cloud storage (OneDrive,Dropbox etc.) If your data is in cloud storage you may wish to sync it directly from there. While the HBS Grid does not offer native Dropbox , OneDrive , or other cloud storage clients, you can use rclone to perform on-demand data synchronization with all major cloud storage providers. Sync your data from a cloud provider to the HBS Grid: Log in to the HBS Grid and connect your local drive using NoMachine as described above. Identify the directory on the HBS Grid that you will copy your data too, creating it if needed. From the HBS Grid desktop, open the rclone browser application. Click the Config... button and follow the prompts (only needed the first time). Click the cloud storage icon in the Remotes tab and select the directory you wish to sync. Specify the target directory from step 2 in the destination field. Click the image below for a quick demonstration showing how to copy files from Dropbox to the HBS Grid. Your browser does not support the video tag. If you run into any problems please let us know by posting at https://github.com/hbs-rcs/hbsgrid-docs/discussions and letting us know so we can fix them! You may also find the Support and Troubleshooting section helpful.","title":"\ud83d\uddd8 &nbsp;&nbsp;Copy and Sync Files"},{"location":"userguide/syncfiles/#access-local-files-from-nomachine-login-nodes","text":"This makes files accessible from the login nodes but not from the compute nodes . For that you need to copy your files to the HBS Grid . NoMachine makes it easy to mount your local file system on the HBS Grid login node. This is useful for reading documentation, scripts, and other small files without needing to physically copy anything to the HBS Grid. Mount your local file system to the HBS Grid: Log in to the HBS Grid by using NoMachine to connect to host hbsgrid-nx.hbs.edu . Press Ctrl-Alt-0 to open the NoMachine session menu. Click Devices Click Connect a disk . Locate the disk you want to mount under Local disks and click on it. click the Connect button to mount your local disk on the HBS Grid. Click the image below for a local drive mounting demonstration: Your browser does not support the video tag.","title":"Access local files from NoMachine login nodes"},{"location":"userguide/syncfiles/#sync-data-fromto-local-storage","text":"NoMachine file mounting is useful, but it has a major limitation; drives mounted via NoMachine are not accessible by applications running on HBS Grid compute nodes . This means that you will usually want to copy or sync your data to the HBS Grid. The easiest way to do this for files on your local machine is to mount your local drive as described above, and then use grsync to sync files from the NoMachine mount to the HBS Grid. Because you must physically copy data to the HBS Grid in order to access it from the compute nodes , you have to decide where to put it. There are three options: home directory , project space , or scratch storage . HBS Grid storage overview A home directory was created at /export/home/<group>/<username> when you requested your account. Your home folder has limited storage capacity and is accessible only by you. Project spaces are directories that are shared and accessible by all HBS Grid users working on that project. You can request a new project space using the new project space request form and you can request modifications to an existing project space using the change request form . Scratch storage is available at /export/scratch . It is appropriate only for temporary, short-term storage. Files are not backed up and will be deleted after 60 days. Scratch storage is a shared resource accessible to all users on the HBS Grid; make sure you set permissions on your files accordingly . Once you have decided where to store your files on the HBS Grid you can follow the steps below to transfer them. Sync data from your local machine to the HBS Grid Log in to the HBS Grid and connect your local drive using NoMachine as described above. Identify the directory on the HBS Grid that you will copy your data too, creating it if needed. From the HBS Grid desktop, open the grsync application. Choose a source directory under the NoMachine mount and specify the target directory from step 2. Click the run button in the upper-right corner of the grsync application. Note that transferring many small files is much slower than transferring a small number of large files. You may find it faster to compress folders with many small files into .zip or .tar archives, transfer those, and decompress/extract them on the other end. Click the image below for a demonstration showing how to sync your data from a local drive to the HBS Grid: Your browser does not support the video tag.","title":"Sync data from/to local storage"},{"location":"userguide/syncfiles/#sync-data-fromto-cloud-storage-onedrivedropbox-etc","text":"If your data is in cloud storage you may wish to sync it directly from there. While the HBS Grid does not offer native Dropbox , OneDrive , or other cloud storage clients, you can use rclone to perform on-demand data synchronization with all major cloud storage providers. Sync your data from a cloud provider to the HBS Grid: Log in to the HBS Grid and connect your local drive using NoMachine as described above. Identify the directory on the HBS Grid that you will copy your data too, creating it if needed. From the HBS Grid desktop, open the rclone browser application. Click the Config... button and follow the prompts (only needed the first time). Click the cloud storage icon in the Remotes tab and select the directory you wish to sync. Specify the target directory from step 2 in the destination field. Click the image below for a quick demonstration showing how to copy files from Dropbox to the HBS Grid. Your browser does not support the video tag. If you run into any problems please let us know by posting at https://github.com/hbs-rcs/hbsgrid-docs/discussions and letting us know so we can fix them! You may also find the Support and Troubleshooting section helpful.","title":"Sync data from/to cloud storage (OneDrive,Dropbox etc.)"},{"location":"userguide/worksafe/","text":"The HBS Grid is a multi-user environment shared by many people. Because of this you must take care to ensure that only authorized project members can access your files. Projects and group membership Info For more information about storage options refer to the file transfer documentation . For collaborative projects in which more than one person needs access, you must use a project space ( request one if needed). Each project has an associated group that includes the HBS Grid users who have access to that project space. Changing group membership must currently be done by a system administrator; use the change request form to request a change. File ownership and permissions For project space files you will almost always want group members to have read and write permission. You can view and set permissions using the File Browser or from the command line using the Terminal . Set permissions using the file browser Follow these steps to change file permissions using the Files application Open the Files application from the Applications menu or Activites search Locate the file or folder you wish to modify, right-click on it and select Properties Select the Permissions tab in file properties dialog If you wish to change permissions for all files in a directory, click the Change Permissions for Enclosed Files button. Select appropriate access levels for Owner (you), Group , and Others . Click the image below to see these steps visually: Your browser does not support the video tag. Refer to the official GNOME documentation for details. Set ownership and permissions using the command line Ownership and permissions can alternatively be set from the command line using chown and chmod . For example chmod -R g+rwx project1/data says \" R ecursively for g roup members, add r ead, w rite and e xecute permissions to project1/data and everything in it\". Refer to tldr chmod for more permissions examples and to man chmod for details. Group ownership can be set from the command line using chgrp . For example opening the Terminal application and running chgrp -R my_project_group project1/data says \" R ecursively make my_project_group the group owner of project1/data and everything in it\". Refer to tldr chgrp for more examples and to man chgrp for details. Avoid running services like Jupyter notebooks without protection Some applications are designed to run as local servers that you connect to using a web browser or other client. On a single-user machine that may be relatively safe, but in a multi-user environment you need to take extra care to ensure that you don't start services that other users on the HBS Grid can connect to. For example, running an unprotected Jupyter notebook can give other users the ability to connect to your service and execute arbitrary commands as you! Fortunately jupyter notebooks are token protected by default, and you can password protect them if you wish. The key thing is that you must be aware of any services you are running and you must understand how those services are protected against unwanted access by other users on the HBS Grid. The simple rule is if you don't know if or how a service is protected, don't use it !","title":"\ud83d\udc65 &nbsp;&nbsp;Collaborate and Share"},{"location":"userguide/worksafe/#projects-and-group-membership","text":"Info For more information about storage options refer to the file transfer documentation . For collaborative projects in which more than one person needs access, you must use a project space ( request one if needed). Each project has an associated group that includes the HBS Grid users who have access to that project space. Changing group membership must currently be done by a system administrator; use the change request form to request a change.","title":"Projects and group membership"},{"location":"userguide/worksafe/#file-ownership-and-permissions","text":"For project space files you will almost always want group members to have read and write permission. You can view and set permissions using the File Browser or from the command line using the Terminal .","title":"File ownership and permissions"},{"location":"userguide/worksafe/#avoid-running-services-like-jupyter-notebooks-without-protection","text":"Some applications are designed to run as local servers that you connect to using a web browser or other client. On a single-user machine that may be relatively safe, but in a multi-user environment you need to take extra care to ensure that you don't start services that other users on the HBS Grid can connect to. For example, running an unprotected Jupyter notebook can give other users the ability to connect to your service and execute arbitrary commands as you! Fortunately jupyter notebooks are token protected by default, and you can password protect them if you wish. The key thing is that you must be aware of any services you are running and you must understand how those services are protected against unwanted access by other users on the HBS Grid. The simple rule is if you don't know if or how a service is protected, don't use it !","title":"Avoid running services like Jupyter notebooks without protection"}]}