
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://hbs-rcs.github.io/hbsgrid-docs/tutorials/nlp_with_python/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.2.3">
    
    
      
        <title>NLP in Python - HBS Grid Technology Preview User Guide</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.e8d9bf0c.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="hbs" data-md-color-primary="" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#natural-language-processing-with-python" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="HBS Grid Technology Preview User Guide" class="md-header__button md-logo" aria-label="HBS Grid Technology Preview User Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            HBS Grid Technology Preview User Guide
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              NLP in Python
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/hbs-rcs/hbsgrid-docs/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../userguide/quickstart/" class="md-tabs__link">
        User Guide
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        Tutorials
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../support/trouble/" class="md-tabs__link">
        Support
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../blog/" class="md-tabs__link">
      Blog
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="HBS Grid Technology Preview User Guide" class="md-nav__button md-logo" aria-label="HBS Grid Technology Preview User Guide" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    HBS Grid Technology Preview User Guide
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/hbs-rcs/hbsgrid-docs/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          User Guide
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="User Guide" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/quickstart/" class="md-nav__link">
        🥇 &nbsp;&nbsp;Get Started
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/menulaunch/" class="md-nav__link">
        🚀 &nbsp;&nbsp;Run Desktop Applications
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/syncfiles/" class="md-nav__link">
        🔄 &nbsp;&nbsp;Copy and Sync Files
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/worksafe/" class="md-nav__link">
        👥 &nbsp;&nbsp;Collaborate and Share
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/commandline/" class="md-nav__link">
        ┃> &nbsp;Use the Command-line
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../userguide/environments/" class="md-nav__link">
        📦 &nbsp;&nbsp;Use Software Versions
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tutorials" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Topic list
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../large_data_R/" class="md-nav__link">
        Large data in R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../PythonWebScrape/" class="md-nav__link">
        Web Scraping in Python
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        NLP in Python
      </a>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Support
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Support" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Support
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../support/trouble/" class="md-nav__link">
        Troubleshooting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../support/requests/" class="md-nav__link">
        Account and Service Requests
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../support/contact/" class="md-nav__link">
        Contact Us
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/hbs-rcs/hbsgrid-docs/discussions/" class="md-nav__link">
        Discussion Forum
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/hbs-rcs/hbsgrid-docs/issues/" class="md-nav__link">
        Bug Reports
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../blog/" class="md-nav__link">
        Blog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/hbs-rcs/hbsgrid-docs/edit/main/source/tutorials/nlp_with_python.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


  

  <nav class="md-tags" >
    
      
        <a href="../#python" class="md-tag">
          Python
        </a>
      
    
      
        <a href="../#nlp" class="md-tag">
          NLP
        </a>
      
    
      
        <a href="../#nltk" class="md-tag">
          NLTK
        </a>
      
    
  </nav>



<h1 id="natural-language-processing-with-python">Natural Language Processing with Python</h1>
<p>Elizabeth Piette, PhD MPH</p>
<p>Statistician/Data Scientist</p>
<p>Research Computing Services, DRFD, Harvard Business School</p>
<h1 id="what-is-natural-language-processing-nlp">What is natural language processing (NLP)?</h1>
<p>NLP is a field that focuses on how computers can be used to process,
analyze, and represent <strong>natural language</strong>.</p>
<p>A natural language is one that has evolved organically, as opposed to a
language that has been constructed intentionally (such as a programming
language like Python, or an auxiliary language like Esperanto).</p>
<p><strong>Discussion: what makes natural language difficult to model?</strong></p>
<p>A funny example:
<a href="https://www.ling.upenn.edu/~beatrice/humor/headlines.html">https://www.ling.upenn.edu/~beatrice/humor/headlines.html</a></p>
<p>Over time, NLP has shifted from hand-written rules to machine learning
algorithms.</p>
<p><strong>Discussion: what are some of the advantages and disadvantages of
rules-based versus machine learning approaches?</strong></p>
<h1 id="how-do-we-utilize-nlp-in-our-daily-lives">How do we utilize NLP in our daily lives?</h1>
<p>Basically whenever we use text or speech to interact with computers,
phones, or smart devices:</p>
<ul>
<li>Search engines</li>
<li>Question answering chat bots</li>
<li>Autocorrect, autocomplete</li>
<li>Virtual assistants</li>
<li>Machine translation</li>
</ul>
<h1 id="our-motivating-problem">Our motivating problem:</h1>
<p>Your best friend’s birthday is next week, and you want to give her
something she’ll really appreciate. You know she’s read all of Emily
Dickinson’s poems and wishes there were more, so why not try writing a
poem in Dickinson’s style?</p>
<p>After a few sad attempts, it’s beginning to look like you’re not much of
a poet. But you have been developing your Python skills and want to
learn more about NLP… perhaps you could <em>generate</em> a poem? This could be
the perfect opportunity to make a thoughtful gift for your friend and
develop some new skills at the same time!</p>
<h1 id="acquiring-a-corpus">Acquiring a corpus</h1>
<p>This is one of the most important parts of an NLP project, and should be
informed by your task.</p>
<p>Some questions you may ask yourself before you assemble your corpus: *
Should our corpus texts come from a specific domain? Words have
different meanings in different contexts * Do the corpus documents
consist of natural language, tables, lists, …? How will I consolidate or
separate these? Do I need to write custom parsers? * Does the text come
from a database, word doc, pdf, web page, …? Multiple sources? * Is
there any meaningful text formatting - italics, bold, underline, …?
Should I utilize this, and how? * Do I need annotated data? How will I
define the annotation guidelines, and who will perform the annotation?</p>
<p><strong>Discussion: what kind of corpora would we want for the applications
above?</strong></p>
<p><strong>Discussion: what would our ideal corpus look like?</strong></p>
<h1 id="loading-necessary-libraries">Loading necessary libraries</h1>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>## True
## 
## [nltk_data] Downloading package punkt to /home/izahn/nltk_data...
## [nltk_data]   Package punkt is already up-to-date!
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>## True
## 
## [nltk_data] Downloading package averaged_perceptron_tagger to
## [nltk_data]     /home/izahn/nltk_data...
## [nltk_data]   Package averaged_perceptron_tagger is already up-to-
## [nltk_data]       date!
</code></pre>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
</code></pre></div>
<h1 id="scraping-text-from-the-web">Scraping text from the web</h1>
<p>We’re going to start our project in the way that many NLP projects start
- by scraping web pages.</p>
<p>First, we’ll compile a list of URLs for the pages we’d like to scrape to
assemble our corpus:</p>
<ul>
<li>Visit <a href="https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems">https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems</a></li>
<li>Right-click a link in the table from the <strong>First Line</strong> column</li>
<li>Click <strong>Inspect</strong></li>
<li>Links are defined in HTML using the <strong>a</strong> tag</li>
<li>The URL addresses of the links we want are in the <strong>href</strong> attribute</li>
<li>The <strong>class</strong> attribute distinguishes different categories of links</li>
<li>Use <strong>requests</strong> to get the page text and <strong>BeautifulSoup</strong> to get
    the URLs</li>
</ul>
<p>Requests documentation: <a href="https://requests.readthedocs.io/en/master/">https://requests.readthedocs.io/en/master/</a></p>
<p>BeautifulSoup documentation:
<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p>
<div class="highlight"><pre><span></span><code><span class="n">dickinson_poems</span> <span class="o">=</span> <span class="s1">&#39;https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems&#39;</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dickinson_poems</span><span class="p">)</span>

<span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">soup</span><span class="o">.</span><span class="n">prettify</span><span class="p">()[:</span><span class="mi">500</span><span class="p">])</span>
</code></pre></div>
<pre><code>## &lt;!DOCTYPE html&gt;
## &lt;html class="client-nojs" dir="ltr" lang="en"&gt;
##  &lt;head&gt;
##   &lt;meta charset="utf-8"/&gt;
##   &lt;title&gt;
##    List of Emily Dickinson poems - Wikipedia
##   &lt;/title&gt;
##   &lt;script&gt;
##    document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"a59c0
</code></pre>
<p>Now that we’ve identified how links are represented, let’s take all of
these links and put them in a list. We can do this using a <strong>list
comprehension</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">links</span> <span class="o">=</span> <span class="p">[</span><span class="n">link</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">soup</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;extiw&#39;</span><span class="p">})]</span>
</code></pre></div>
<p>It’s always a good idea to look at your data as a sanity check, so let’s
peak at the first 10 and last 10 lines in our list of links:</p>
<div class="highlight"><pre><span></span><code><span class="n">links</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div>
<pre><code>## ['https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage', 'https://en.wikisource.org/wiki/A_Bird_came_down_the_Walk_%E2%80%94', 'https://en.wikisource.org/wiki/A_Burdock_%E2%80%94_clawed_my_Gown_%E2%80%94', 'https://en.wikisource.org/wiki/A_Cap_of_Lead_across_the_sky', 'https://en.wikisource.org/wiki/A_Charm_invests_a_face']
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">links</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:]</span>
</code></pre></div>
<pre><code>## ['https://en.wikisource.org/wiki/You%27re_right_%E2%80%94_%22the_way_is_narrow%22_%E2%80%94', 'https://en.wikisource.org/wiki/You%27ve_seen_Balloons_set_%E2%80%94_Haven%27t_You%3F', 'https://en.wikisource.org/wiki/Your_Riches_%E2%80%94_taught_me_%E2%80%94_Poverty.', 'https://en.wikisource.org/wiki/Your_thoughts_don%27t_have_words_every_day', 'https://foundation.wikimedia.org/wiki/Privacy_policy']
</code></pre>
<p>The last link isn’t to a poem, so let’s remove that:</p>
<div class="highlight"><pre><span></span><code><span class="k">del</span> <span class="n">links</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<p>Next, we’ll visit each of these pages and extract the poem text:</p>
<ul>
<li>Visit one of the links in the table, such as
    <a href="https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage">https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage</a></li>
<li>Right click the poem text and click <strong>Inspect</strong></li>
<li>Identify the HTML tags and attributes for the element containing the
    poem</li>
<li>Write a function to scrape our desired corpus using <strong>requests</strong> and
    <strong>BeautifulSoup</strong></li>
</ul>
<p>Here’s the page content of the first link:</p>
<div class="highlight"><pre><span></span><code><span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">links</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

<span class="n">soup</span><span class="o">.</span><span class="n">prettify</span><span class="p">()[:</span><span class="mi">1000</span><span class="p">]</span>
</code></pre></div>
<pre><code>## '&lt;!DOCTYPE html&gt;\n&lt;html class="client-nojs" dir="ltr" lang="en"&gt;\n &lt;head&gt;\n  &lt;meta charset="utf-8"/&gt;\n  &lt;title&gt;\n   A Bee his burnished Carriage - Wikisource, the free online library\n  &lt;/title&gt;\n  &lt;script&gt;\n   document.documentElement.className="client-js";RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"f857c3cd-5fef-46a7-99c1-59fcb9b47551","wgCSPNonce":false,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"A_Bee_his_burnished_Carriage","wgTitle":"A Bee his burnished Carriage","wgCurRevisionId":4327441,"wgRevisionId":4327441,"wgArticleId":13813,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["PD-old"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","w'
</code></pre>
<p>Now that we’ve found the appropriate tags associated with the poem, we
can go ahead and grab the poem text:</p>
<div class="highlight"><pre><span></span><code><span class="n">poem</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;poem&#39;</span><span class="p">})</span>
<span class="n">poem</span><span class="o">.</span><span class="n">text</span>
</code></pre></div>
<pre><code>## '\nA Bee his burnished Carriage\nDrove boldly to a Rose —\nCombinedly alighting —\nHimself — his Carriage was —\nThe Rose received his visit\nWith frank tranquillity\nWithholding not a Crescent\nTo his Cupidity —\nTheir Moment consummated —\nRemained for him — to flee —\nRemained for her — of rapture\nBut the humility.\n\n'
</code></pre>
<p>The following cell combines the steps we performed above into a function
that returns the scraped poems from the list of URLs. We’ll skip calling
this in the interest of time.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">scrape_corpus</span><span class="p">(</span><span class="n">corpus_URLs</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes our list of URLS</span>
<span class="sd">    Returns a list of the poems scraped from those pages</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">corpus</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">corpus_URLs</span><span class="p">:</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">link</span><span class="p">)</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="n">poem</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;poem&#39;</span><span class="p">})</span>

        <span class="k">if</span> <span class="n">poem</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">corpus</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">poem</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFKD&#39;</span><span class="p">,</span> <span class="n">poem</span><span class="p">)</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

<span class="n">corpus_texts</span> <span class="o">=</span> <span class="n">scrape_corpus</span><span class="p">(</span><span class="n">links</span><span class="p">)</span>
</code></pre></div>
<p>For those interested, this is how I saved the corpus:</p>
<div class="highlight"><pre><span></span><code><span class="n">joined_corpus</span> <span class="o">=</span> <span class="s1">&#39;\p&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;corpus.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">)</span>
</code></pre></div>
<p>Note that this doesn’t do a perfect job of extracting the content we
want - for example, some of the text we extracted has multiple versions
of a given poem. Encountering inconsistencies in page HTML formatting is
very common in web scraping tasks. You can perform some manual or
automated checks and cleaning as desired/as is feasible. This is
generally an iterative process.</p>
<p>Now we’ll just load the corpus I saved prior to this workshop:</p>
<div class="highlight"><pre><span></span><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/corpus.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">corpus_texts</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">corpus_texts</span> <span class="o">=</span> <span class="n">corpus_texts</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;\p&#39;</span><span class="p">)</span>

<span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<pre><code>## '\nA Bee his burnished Carriage\nDrove boldly to a Rose —\nCombinedly alighting —\nHimself — his Carriage was —\nThe Rose received his visit\nWith frank tranquillity\nWithholding not a Crescent\nTo his Cupidity —\nTheir Moment consummated —\nRemained for him — to flee —\nRemained for her — of rapture\nBut the humility.\n\n'
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">len</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">)</span>
</code></pre></div>
<pre><code>## 1763
</code></pre>
<h1 id="pre-processing">Pre-processing</h1>
<p><em>Garbage in, garbage out</em></p>
<p>Pre-processing is going to turn the raw text we scraped from the web
into <strong>data</strong>.</p>
<p>Basic preprocessing often includes: * Tokenization * Removing
capitalization * Stripping away tags * Removing accents/special
characters * Removing punctuation * Removing/normalizing whitespace *
Stemming/lemmatization * Removing stopwords</p>
<p>Once again, the way in which you pre-process your data should be
informed by your final application - there isn’t a one-size-fits-all
approach. For example: * Capitalization may be informative (Apple
vs. apple) * HTML tags may provide structural information/metadata we
can use as features or to label data * Sentence tokenization may need
to be performed differently for different types of texts, eg in social
media texts there may be punctuation missing from the ends of sentences,
but additional punctuation forming emojis :) * Word tokenization may
also depend on the source text, eg in text formatted for newspapers
there may be hyphens separating words between syllables across lines due
to column formatting</p>
<p><strong>Discussion: what types of pre-processing do you think would be more
appropriate for our application?</strong></p>
<p>For this workshop, we’re going to lowercase all text, remove all
punctuation, and perform line and word tokenization. Let’s start by
investigating the output each of these types of pre-processing
separately.</p>
<p>We can make our text lowercase using <strong>.lower()</strong>, which is a built in
<strong>String</strong> method in Python. For example, let’s lower everything in the
first poem in our corpus:</p>
<div class="highlight"><pre><span></span><code><span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</code></pre></div>
<pre><code>## '\na bee his burnished carriage\ndrove boldly to a rose —\ncombinedly alighting —\nhimself — his carriage was —\nthe rose received his visit\nwith frank tranquillity\nwithholding not a crescent\nto his cupidity —\ntheir moment consummated —\nremained for him — to flee —\nremained for her — of rapture\nbut the humility.\n\n'
</code></pre>
<p>We can remove punctuation using a <strong>regular expression</strong>. It’s okay if
you’re not familiar with regular expressions yet, but you may want to
learn more about them as you go forth in your NLP journey since they
allow you to specify text search patterns.</p>
<p>The code below finds all characters in our specified text (in this case,
corpus_texts[0], the first poem) that are not “word characters” or
whitespace, and substitutes them with an empty string.</p>
<div class="highlight"><pre><span></span><code><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<pre><code>## '\nA Bee his burnished Carriage\nDrove boldly to a Rose \nCombinedly alighting \nHimself  his Carriage was \nThe Rose received his visit\nWith frank tranquillity\nWithholding not a Crescent\nTo his Cupidity \nTheir Moment consummated \nRemained for him  to flee \nRemained for her  of rapture\nBut the humility\n\n'
</code></pre>
<p><strong>NLTK</strong> is going to become your favorite tool, and is accompanied by an
excellent book that you can work through to expand your repertoire far
beyond what we can cover in the scope of this workshop. *
<a href="https://www.nltk.org/">https://www.nltk.org/</a> * <a href="https://www.nltk.org/book/">https://www.nltk.org/book/</a></p>
<p><strong>NLTK</strong> has a built in <strong>sentence tokenizer</strong>, but since our poems
don’t consist of sentences in the traditional sense, we can perform our
own <strong>line tokenization</strong> by splitting on newlines:</p>
<div class="highlight"><pre><span></span><code><span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>## ['', 'A Bee his burnished Carriage', 'Drove boldly to a Rose —', 'Combinedly alighting —', 'Himself — his Carriage was —', 'The Rose received his visit', 'With frank tranquillity', 'Withholding not a Crescent', 'To his Cupidity —', 'Their Moment consummated —', 'Remained for him — to flee —', 'Remained for her — of rapture', 'But the humility.', '', '']
</code></pre>
<p>We can perform <strong>word tokenization</strong> using the <strong>NLTK word tokenizer</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="n">word_tokenize</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div>
<pre><code>## ['A', 'Bee', 'his', 'burnished']
</code></pre>
<p>Now, let’s write a function that combines our four preprocessing steps.</p>
<div class="highlight"><pre><span></span><code><span class="c1">#Example solution:</span>

<span class="k">def</span> <span class="nf">preprocess_poem</span><span class="p">(</span><span class="n">poem</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a poem (as a single string)</span>
<span class="sd">    Returns the poem lowercased, with punctuation removed, line and word tokenized</span>
<span class="sd">    (as a list of lists, where each line is a list of words)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">poem</span> <span class="o">=</span> <span class="n">poem</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">poem</span><span class="p">)</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="n">poem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poem</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">poem</span>
</code></pre></div>
<p>Writing docstrings makes life easier for others reading, reusing, and
modifying your code (including future-you)</p>
<div class="highlight"><pre><span></span><code><span class="n">help</span><span class="p">(</span><span class="n">preprocess_poem</span><span class="p">)</span>
</code></pre></div>
<pre><code>## Help on function preprocess_poem in module __main__:
## 
## preprocess_poem(poem: str) -&gt; list
##     Takes a poem (as a single string)
##     Returns the poem lowercased, with punctuation removed, line and word tokenized
##     (as a list of lists, where each line is a list of words)
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">preprocess_poem</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<pre><code>## [[], ['a', 'bee', 'his', 'burnished', 'carriage'], ['drove', 'boldly', 'to', 'a', 'rose'], ['combinedly', 'alighting'], ['himself', 'his', 'carriage', 'was'], ['the', 'rose', 'received', 'his', 'visit'], ['with', 'frank', 'tranquillity'], ['withholding', 'not', 'a', 'crescent'], ['to', 'his', 'cupidity'], ['their', 'moment', 'consummated'], ['remained', 'for', 'him', 'to', 'flee'], ['remained', 'for', 'her', 'of', 'rapture'], ['but', 'the', 'humility'], [], []]
</code></pre>
<p>Looks pretty good! But let’s also remove these extraneous empty lines:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">preprocess_poem</span><span class="p">(</span><span class="n">poem</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a poem (as a single string)</span>
<span class="sd">    Returns the poem lowercased, with punctuation removed, line and word tokenized</span>
<span class="sd">    (as a list of lists, where each line of the poem is a list of words)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">poem</span> <span class="o">=</span> <span class="n">poem</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">poem</span><span class="p">)</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="n">poem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">poem</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poem</span> <span class="k">if</span> <span class="n">line</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">poem</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">preprocess_poem</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<pre><code>## [['a', 'bee', 'his', 'burnished', 'carriage'], ['drove', 'boldly', 'to', 'a', 'rose'], ['combinedly', 'alighting'], ['himself', 'his', 'carriage', 'was'], ['the', 'rose', 'received', 'his', 'visit'], ['with', 'frank', 'tranquillity'], ['withholding', 'not', 'a', 'crescent'], ['to', 'his', 'cupidity'], ['their', 'moment', 'consummated'], ['remained', 'for', 'him', 'to', 'flee'], ['remained', 'for', 'her', 'of', 'rapture'], ['but', 'the', 'humility']]
</code></pre>
<p>Great! Now let’s apply our preprocessing to the entire corpus:</p>
<div class="highlight"><pre><span></span><code><span class="n">preprocessed_poems</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess_poem</span><span class="p">(</span><span class="n">poem</span><span class="p">)</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">corpus_texts</span><span class="p">]</span>
</code></pre></div>
<h1 id="exploring-our-data">Exploring our data</h1>
<p>Now that we’ve performed some pre-processing, we can more readily
explore our data. In order get some distributional statistics, it’ll
make our lives easier to first join the entire pre-processed corpus into
one long list:</p>
<div class="highlight"><pre><span></span><code><span class="n">joined_lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">preprocessed_poems</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poem</span><span class="p">]</span>
<span class="n">joined_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">joined_lines</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">joined_corpus</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div>
<pre><code>## ['a', 'bee', 'his', 'burnished', 'carriage', 'drove', 'boldly', 'to', 'a', 'rose']
</code></pre>
<p>Now we can do some basic counts:</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;There are </span><span class="si">{}</span><span class="s1"> total words in the corpus&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">)))</span>
</code></pre></div>
<pre><code>## There are 94163 total words in the corpus
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;There are </span><span class="si">{}</span><span class="s1"> unique words in the corpus&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">))))</span>
</code></pre></div>
<pre><code>## There are 10733 unique words in the corpus
</code></pre>
<p>We can also use <strong>NLTK</strong> to look at word frequencies. Let’s take a look
at the 10 most common words in our corpus:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.probability</span> <span class="kn">import</span> <span class="n">FreqDist</span>

<span class="n">fdist</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">)</span>

<span class="n">fdist</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [('the', 5876), ('a', 2581), ('and', 2291), ('to', 2267), ('of', 1901), ('i', 1653), ('that', 1234), ('it', 1220), ('is', 1186), ('in', 1148)]
</code></pre>
<p>The most common words in the corpus are not very exciting…</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>## True
## 
## [nltk_data] Downloading package stopwords to /home/izahn/nltk_data...
## [nltk_data]   Package stopwords is already up-to-date!
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">english_stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">english_stopwords</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div>
<pre><code>## ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
</code></pre>
<p>Let’s write a function to remove <strong>stopwords</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">remove_nltk_stopwords</span><span class="p">(</span><span class="n">tokenized_corpus</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">stopwords</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list of tokens and a list of stopwords</span>
<span class="sd">    Returns the list of tokens with stopwords removed</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokenized_corpus</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">joined_corpus_stopwords_removed</span> <span class="o">=</span> <span class="n">remove_nltk_stopwords</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">,</span> <span class="n">english_stopwords</span><span class="p">)</span>

<span class="n">fdist</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">joined_corpus_stopwords_removed</span><span class="p">)</span>

<span class="n">fdist</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [('like', 329), ('one', 322), ('upon', 280), ('could', 255), ('would', 254), ('day', 227), ('little', 224), ('know', 221), ('thee', 217), ('away', 211)]
</code></pre>
<p>These sound a bit more interesting to me!</p>
<p>We can also plot word frequencies:</p>
<div class="highlight"><pre><span></span><code><span class="n">fdist</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-28-1.png" /><!-- --><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-28-2.png" /><!-- --></p>
<p>Let’s move on from single words to explore <strong>combinations of words</strong> -
we can use <strong>NLTK</strong> for this as well:</p>
<div class="highlight"><pre><span></span><code><span class="n">bigrams_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">bigrams</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">))</span>
<span class="n">fdist</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">bigrams_list</span><span class="p">)</span>
<span class="n">fdist</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [(('in', 'the'), 310), (('of', 'the'), 289), (('to', 'the'), 153), (('is', 'the'), 150), (('upon', 'the'), 118), (('the', 'sun'), 117), (('can', 'not'), 114), (('to', 'be'), 109), (('for', 'the'), 107), (('all', 'the'), 104)]
</code></pre>
<p>Once again, these are pretty boring… let’s try again, with our corpus
with stopwords removed:</p>
<div class="highlight"><pre><span></span><code><span class="n">bigrams_list_stopwords_removed</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">bigrams</span><span class="p">(</span><span class="n">joined_corpus_stopwords_removed</span><span class="p">))</span>
<span class="n">fdist</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">bigrams_list_stopwords_removed</span><span class="p">)</span>
<span class="n">fdist</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [(('every', 'day'), 19), (('human', 'nature'), 12), (('could', 'see'), 11), (('put', 'away'), 11), (('thou', 'art'), 11), (('let', 'go'), 10), (('hast', 'thou'), 9), (('old', 'fashioned'), 9), (('could', 'find'), 8), (('let', 'us'), 8)]
</code></pre>
<p>These are much more poetic!</p>
<div class="highlight"><pre><span></span><code><span class="n">fdist</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-31-5.png" /><!-- --><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-31-6.png" /><!-- --></p>
<p>Perhaps we’re also interested in structural elements of our corpus
poems, like the lines per poem and words per line. Let’s write functions
to count both of these:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">count_lines_per_poem</span><span class="p">(</span><span class="n">line_word_tokenized_corpus</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list of line and word tokenized poems (list of lists of strings)</span>
<span class="sd">    Returns a list of ints with the number of lines in each poem</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">poem</span><span class="p">)</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">line_word_tokenized_corpus</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">lines_per_poem</span> <span class="o">=</span> <span class="n">count_lines_per_poem</span><span class="p">(</span><span class="n">preprocessed_poems</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lines_per_poem</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-33-9.png" /><!-- --></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">count_words_per_line</span><span class="p">(</span><span class="n">line_word_tokenized_corpus</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list of line and word tokenized poems (list of lists of strings)</span>
<span class="sd">    Returns a list of ints with the number of words in each line</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">line_word_tokenized_corpus</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poem</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">words_per_line</span> <span class="o">=</span> <span class="n">count_words_per_line</span><span class="p">(</span><span class="n">preprocessed_poems</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">words_per_line</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;hist&#39;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="../nlp_with_python_files/figure-gfm/unnamed-chunk-35-11.png" /><!-- --></p>
<p>We can also look at what <strong>parts of speech</strong> are represented in our
corpus:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span>
<span class="n">pos_tag</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>
<pre><code>## [('a', 'DT'), ('bee', 'NN'), ('his', 'PRP$'), ('burnished', 'JJ'), ('carriage', 'NN'), ('drove', 'VBD'), ('boldly', 'RB'), ('to', 'TO'), ('a', 'DT'), ('rose', 'VBD')]
</code></pre>
<p>Let’s make a dictionary of words keyed by their part of speech, and vice
versa - maybe these could be useful later?</p>
<div class="highlight"><pre><span></span><code><span class="n">corpus_pos_tagged</span> <span class="o">=</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">word_to_pos</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus_pos_tagged</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">word_to_pos</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_pos</span><span class="p">[</span><span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">word_to_pos</span><span class="p">[</span><span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word_to_pos</span><span class="p">[</span><span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus_pos_tagged</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
</code></pre></div>
<p>Words can be tagged as different parts of speech in different contexts:</p>
<div class="highlight"><pre><span></span><code><span class="n">word_to_pos</span><span class="p">[</span><span class="s1">&#39;walk&#39;</span><span class="p">]</span>
</code></pre></div>
<pre><code>## ['NN', 'VBP', 'VB']
</code></pre>
<div class="highlight"><pre><span></span><code><span class="n">pos_to_word</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">corpus_pos_tagged_reversed</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_pos_tagged</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">pos_to_word</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pos_to_word</span><span class="p">[</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]:</span>
            <span class="n">pos_to_word</span><span class="p">[</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pos_to_word</span><span class="p">[</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus_pos_tagged_reversed</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1">#you could write a generic function to make either a word_to_pos or pos_to_word dictionary...</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">pos_to_word</span><span class="p">[</span><span class="s1">&#39;NN&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div>
<pre><code>## ['bee', 'carriage', 'visit', 'tranquillity', 'crescent', 'cupidity', 'moment', 'rapture', 'humility', 'bird']
</code></pre>
<p><strong>NLTK</strong> also has a nice function that lets you look at words that are
<strong>‘similar’</strong> to a given word, based on context:</p>
<div class="highlight"><pre><span></span><code><span class="n">joined_Text</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">Text</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">)</span> <span class="c1">#it&#39;s necessary to convert to a nltk.text.Text object</span>
<span class="n">joined_Text</span><span class="o">.</span><span class="n">similar</span><span class="p">(</span><span class="s1">&#39;bee&#39;</span><span class="p">)</span>
</code></pre></div>
<pre><code>## sun day bird house grave sea world rose life soul heart sky face first
## mind light way wind hand dust
</code></pre>
<p>It might also be quite useful to make a dictionary associating words
that appear in similar contexts… but I’ll leave that to you.</p>
<p><strong>Discussion: what other statistics and visualizations would help us
better understand our data?</strong></p>
<h1 id="our-first-poetry-generator">Our first poetry generator</h1>
<p>Now let’s combine our knowledge of the distribution of words in the
corpus and the structure of the documents to make our first poem
generator! <strong>NLTK</strong> has many built-in functions that we can use, so
we’ll just take advantage of those. First, we have to create a
Vocabulary from our joined corpus:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">Vocabulary</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">joined_corpus</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;bee&#39;</span><span class="p">]</span>
</code></pre></div>
<pre><code>## 83
</code></pre>
<p>Next, we’ll pad the ends of each line with special tokens that do not
appear elsewhere in our corpus, and format our text into bigrams:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="kn">import</span> <span class="n">pad_both_ends</span>
<span class="n">preprocessed_poems_lines_joined</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span> <span class="k">for</span> <span class="n">poem</span> <span class="ow">in</span> <span class="n">preprocessed_poems</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">poem</span><span class="p">]</span>
<span class="n">padded_text</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">bigrams</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)))</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">preprocessed_poems_lines_joined</span><span class="p">]</span>
<span class="n">padded_text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<pre><code>## [('&lt;s&gt;', 'a'), ('a', 'bee'), ('bee', 'his'), ('his', 'burnished'), ('burnished', 'carriage'), ('carriage', '&lt;/s&gt;')]
</code></pre>
<p>This command will actually perform these steps for us at once (but it
was good of us to look at the output of each):</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="kn">import</span> <span class="n">padded_everygram_pipeline</span>
<span class="n">train</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">padded_everygram_pipeline</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">preprocessed_poems_lines_joined</span><span class="p">)</span>
</code></pre></div>
<p>We’ll now fit an MLE model:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">MLE</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">MLE</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
</code></pre></div>
<p>We can use this model to write a basic function to generate poetry:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">generate_poem</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly select a number of lines from the distribution of lines per poem</span>
<span class="sd">    For each line:</span>
<span class="sd">    - randomly select a number of words from the distribution of words per line</span>
<span class="sd">    - use the MLE model to generate a line containing the desired number of words</span>
<span class="sd">    - replace end of sentence tags with dashes for ~flair~</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">generated_poem</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_lines</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">lines_per_poem</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_lines</span><span class="p">):</span>
        <span class="n">num_words</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">words_per_line</span><span class="p">)</span>

        <span class="n">generated_line</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39; +&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">num_words</span><span class="p">))))))</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="n">generated_poem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_line</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">generated_poem</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">generate_poem</span><span class="p">()</span>
</code></pre></div>
<pre><code>## ['the latter is difficult the strain', 'i cried give her castle', 'i would fly - hoped', 'sea - the son']
</code></pre>
<p>Of course, this is a pretty lazy model, with many things we could
improve. Instead of randomly generating the number of lines and words
per line, we may want to directly use the structure of a poem in the
corpus. Or we might chose to perform our preprocessing in a different
manner so we don’t lose and subsequently artificially reinsert dashes or
exclamation points. We could go on, and make many incremental
improvements.</p>
<p><strong>Discussion: what additional rules could improve our poetry
generator?</strong></p>
<p>One of the most critical flaws of our model is that it is
<strong>unidirectional</strong>. Which leads us to…</p>
<h1 id="our-second-poetry-generator">Our second poetry generator</h1>
<p>Artificial neural networks are inspired by and structurally somewhat
analogous to biological neural networks.</p>
<p>Recurrent neural networks (RNNs) are particularly suited to sequential
data (like text):</p>
<p><img alt="rnn.png" src="../imgs/rnn.png" /></p>
<p>From <a href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a></p>
<p>The following code comes with very slight modifications from “Minimal
character-level Vanilla RNN model” by Andrej Karpathy (<a class="gh-link gh-mention" href="https://github.com/karpathy" title="GitHub User: @karpathy">@karpathy</a>): *
<a href="https://gist.github.com/karpathy/d4dee566867f8291f086">https://gist.github.com/karpathy/d4dee566867f8291f086</a> *
<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
<p>I really love this example because it’s on the more approachable side
for a beginner, it’s convenient for the purposes of this workshop since
we don’t have time to explore Tensorflow/Keras, and it’s informative
since we can read through each chunk of code. However, please don’t feel
as if you have to fully understand the content of each of the following
code cells - we’re just going to focus on concepts.</p>
<p>Let’s start by inputting our raw corpus without any pre-processing:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">data_input</span><span class="p">(</span><span class="n">corpus</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a corpus as a list of strings</span>
<span class="sd">    Returns the data prepared to be used in a word-based model (rather than char-based model used by Andrej Karpathy)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">data</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;(\W)&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">vocab_size</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">data</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">data_input</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>
<pre><code>## ['', '\n', 'A', ' ', 'Bee', ' ', 'his', ' ', 'burnished', ' ']
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div>
<pre><code>## ['', 'Peru', 'mother', 'Enemy', 'Part', 'Parts', 'balancing', 'vanity', 'gushes', 'Wardrobe']
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
</code></pre></div>
<pre><code>## 13695
</code></pre>
<p>Next we’ll create dictionaries that map words to IDs, and IDs to words:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">gen_mapping</span><span class="p">(</span><span class="n">words_list</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes the prepared corpus data</span>
<span class="sd">    Returns dictionaries mapping the words to IDs, and IDs to words</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">ch</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_list</span><span class="p">)}</span>
    <span class="n">ix_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_list</span><span class="p">)}</span>

    <span class="k">return</span> <span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_word</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_word</span> <span class="o">=</span> <span class="n">gen_mapping</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">[</span><span class="s1">&#39;bee&#39;</span><span class="p">])</span>
</code></pre></div>
<pre><code>## 6202
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">ix_to_word</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
</code></pre></div>
<pre><code>## consecrate
</code></pre>
<p>Now we’ll set our hyperparameters, which are defined prior to running
the algorithm (as opposed to the model parameters, which are iteratively
updated). The learning rate is probably the most important
hyperparameter to focus on, since it controls how much the weights are
updated with each iteration. A larger learning rate converges more
quickly (perhaps overshooting minima), whereas a smaller learning rate
converges more slowly (and may become stuck in a local minima).</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_hyperparams</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets hyperparameters, defaulting to Andrej Karpathy&#39;s values</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">learning_rate</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">hidden_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">set_hyperparams</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
</code></pre></div>
<pre><code>## 100
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)</span>
</code></pre></div>
<pre><code>## 25
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div>
<pre><code>## 0.1
</code></pre>
<p>Next we’ll initialize placeholders for our model parameters:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_model_params</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets model parameters based on hyperparameters and vocab size</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">Wxh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>  <span class="c1"># input to hidden</span>
    <span class="n">Whh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>  <span class="c1"># hidden to hidden</span>
    <span class="n">Why</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>  <span class="c1"># hidden to output</span>
    <span class="n">bh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># hidden bias</span>
    <span class="n">by</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># output bias</span>

    <span class="k">return</span> <span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="n">set_model_params</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">Wxh</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [[ 1.07318817e-03  2.99033756e-03  1.31159605e-02 ...  2.20105149e-03
##   -3.80494428e-04  3.97301708e-03]
##  [ 1.56696343e-02 -1.15619302e-02 -1.86478759e-02 ...  6.04892251e-03
##    1.98045140e-03  4.12423613e-03]
##  [-3.97598434e-03 -6.62507965e-03  2.29918440e-03 ... -1.44603155e-03
##    2.75347720e-03  3.00119575e-03]
##  ...
##  [ 8.61543861e-03  6.18875571e-03  1.40664915e-03 ... -1.21927158e-02
##    7.33542826e-03 -1.01914743e-02]
##  [ 5.74131920e-03  6.95976654e-03  7.49904548e-05 ... -9.71417927e-03
##   -7.51565124e-04 -4.48596981e-03]
##  [-7.50688337e-03 -1.44724997e-02  1.21272185e-02 ... -8.07159000e-03
##    6.78215010e-03  1.14515049e-02]]
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">Whh</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [[-0.00170988 -0.00297927 -0.01154336 ... -0.00550237  0.00019349
##   -0.01389495]
##  [ 0.01729352 -0.01719077  0.00417469 ... -0.01540379 -0.01943895
##   -0.00840091]
##  [-0.00226139  0.00539692  0.00793333 ...  0.01228179  0.00262746
##   -0.01788651]
##  ...
##  [-0.0081638   0.00851257  0.00806451 ...  0.00586944 -0.00861062
##   -0.00635998]
##  [ 0.00357786 -0.00291397 -0.01052795 ...  0.01750125  0.00399255
##    0.00268263]
##  [ 0.00368808  0.00159308 -0.00557169 ... -0.0060339  -0.00162449
##   -0.00666031]]
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">Why</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [[ 0.01132326 -0.01264148  0.00186793 ...  0.00027127 -0.01236483
##   -0.00700431]
##  [-0.00894894 -0.0032734   0.01492165 ...  0.00464066 -0.01345284
##    0.00064576]
##  [ 0.01203935  0.00684551  0.01899521 ... -0.00711966  0.00542538
##    0.00644606]
##  ...
##  [-0.01385628 -0.01017521 -0.00882153 ... -0.00630009  0.00578206
##   -0.01508209]
##  [-0.00078822 -0.01597705  0.00242087 ...  0.00016081 -0.0042986
##   -0.00981912]
##  [ 0.0100233  -0.00639412  0.00168985 ...  0.00565186  0.00616837
##   -0.0032813 ]]
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">bh</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [[0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]
##  [0.]]
</code></pre>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">by</span><span class="p">)</span>
</code></pre></div>
<pre><code>## [[0.]
##  [0.]
##  [0.]
##  ...
##  [0.]
##  [0.]
##  [0.]]
</code></pre>
<p>We then perform the forward pass, update the loss, and perform the
backward pass:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    inputs,targets are both list of integers.</span>
<span class="sd">    hprev is Hx1 array of initial hidden state</span>
<span class="sd">    returns the loss, gradients on model parameters, and last hidden state</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">xs</span><span class="p">,</span> <span class="n">hs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">ps</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
    <span class="n">hs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hprev</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># forward pass</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)):</span>
        <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># encode in 1-of-k representation</span>
        <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">bh</span><span class="p">)</span>  <span class="c1"># hidden state</span>
        <span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">+</span> <span class="n">by</span>  <span class="c1"># unnormalized log probabilities for next chars</span>
        <span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ys</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>  <span class="c1"># probabilities for next chars</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">targets</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># softmax (cross-entropy loss)</span>
    <span class="c1"># backward pass: compute gradients going backwards</span>
    <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wxh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Whh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Why</span><span class="p">)</span>
    <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">bh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">by</span><span class="p">)</span>
    <span class="n">dhnext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">hs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))):</span>
        <span class="n">dy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
        <span class="n">dy</span><span class="p">[</span><span class="n">targets</span><span class="p">[</span>
            <span class="n">t</span><span class="p">]]</span> <span class="o">-=</span> <span class="mi">1</span>  <span class="c1"># backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span>
        <span class="n">dWhy</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dby</span> <span class="o">+=</span> <span class="n">dy</span>
        <span class="n">dh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="o">+</span> <span class="n">dhnext</span>  <span class="c1"># backprop into h</span>
        <span class="n">dhraw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="n">dh</span>  <span class="c1"># backprop through tanh nonlinearity</span>
        <span class="n">dbh</span> <span class="o">+=</span> <span class="n">dhraw</span>
        <span class="n">dWxh</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhraw</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dWhh</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhraw</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dhnext</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dhraw</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">dparam</span> <span class="ow">in</span> <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">]:</span>
        <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">dparam</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">dparam</span><span class="p">)</span>  <span class="c1"># clip to mitigate exploding gradients</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">hs</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<p>This function returns a sample from the model at a given time:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">seed_ix</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    sample a sequence of integers from the model</span>
<span class="sd">    h is memory state, seed_ix is seed letter for first time step</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">x</span><span class="p">[</span><span class="n">seed_ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ixes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Whh</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">bh</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Why</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">by</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">ixes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ix</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ixes</span>
</code></pre></div>
<p>And finally we have a function that iterates through the model and
periodically returns a sample:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">iterate_and_sample</span><span class="p">(</span><span class="n">sample_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">sample_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">max_iters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Putting it all together:</span>
<span class="sd">    Prints a model sample of a given length (default 200)</span>
<span class="sd">    at a given number of iterations (default 100)</span>
<span class="sd">    up to a maximum number of iterations (default 1000)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">mWxh</span><span class="p">,</span> <span class="n">mWhh</span><span class="p">,</span> <span class="n">mWhy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Wxh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Whh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Why</span><span class="p">)</span>
    <span class="n">mbh</span><span class="p">,</span> <span class="n">mby</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">bh</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">by</span><span class="p">)</span>  <span class="c1"># memory variables for Adagrad</span>
    <span class="n">smooth_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">vocab_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">seq_length</span>  <span class="c1"># loss at iteration 0</span>
    <span class="k">while</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="n">max_iters</span><span class="p">:</span>
        <span class="c1"># prepare inputs (we&#39;re sweeping from left to right in steps seq_length long)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">hprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># reset RNN memory</span>
            <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># go from start of data</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="n">p</span><span class="p">:</span><span class="n">p</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">]]</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">ch</span><span class="p">]</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">p</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>

        <span class="c1"># sample from the model now and then</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">sample_iters</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sample_ix</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">hprev</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_length</span><span class="p">)</span>
            <span class="n">txt</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ix_to_word</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="n">sample_ix</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----</span><span class="se">\n</span><span class="s1"> </span><span class="si">%s</span><span class="s1"> </span><span class="se">\n</span><span class="s1">----&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">txt</span><span class="p">,))</span>

        <span class="c1"># forward seq_length characters through the net and fetch gradient</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">,</span> <span class="n">hprev</span> <span class="o">=</span> <span class="n">lossFun</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">hprev</span><span class="p">)</span>
        <span class="n">smooth_loss</span> <span class="o">=</span> <span class="n">smooth_loss</span> <span class="o">*</span> <span class="mf">0.999</span> <span class="o">+</span> <span class="n">loss</span> <span class="o">*</span> <span class="mf">0.001</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="n">sample_iters</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;iter </span><span class="si">%d</span><span class="s1">, loss: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">smooth_loss</span><span class="p">))</span>  <span class="c1"># print progress</span>

        <span class="c1"># perform parameter update with Adagrad</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">dparam</span><span class="p">,</span> <span class="n">mem</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span><span class="p">],</span>
                                      <span class="p">[</span><span class="n">dWxh</span><span class="p">,</span> <span class="n">dWhh</span><span class="p">,</span> <span class="n">dWhy</span><span class="p">,</span> <span class="n">dbh</span><span class="p">,</span> <span class="n">dby</span><span class="p">],</span>
                                      <span class="p">[</span><span class="n">mWxh</span><span class="p">,</span> <span class="n">mWhh</span><span class="p">,</span> <span class="n">mWhy</span><span class="p">,</span> <span class="n">mbh</span><span class="p">,</span> <span class="n">mby</span><span class="p">]):</span>
            <span class="n">mem</span> <span class="o">+=</span> <span class="n">dparam</span> <span class="o">*</span> <span class="n">dparam</span>
            <span class="n">param</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dparam</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mem</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># adagrad update</span>

        <span class="n">p</span> <span class="o">+=</span> <span class="n">seq_length</span>  <span class="c1"># move data pointer</span>
        <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># iteration counter</span>
</code></pre></div>
<p>Here’s our entire pipline put together:</p>
<div class="highlight"><pre><span></span><code><span class="c1">#load data:</span>
<span class="n">data</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">data_input</span><span class="p">(</span><span class="n">corpus_texts</span><span class="p">)</span>

<span class="c1">#encode data:</span>
<span class="n">word_to_ix</span><span class="p">,</span> <span class="n">ix_to_word</span> <span class="o">=</span> <span class="n">gen_mapping</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1">#set hyperparameters</span>
<span class="n">hidden_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">set_hyperparams</span><span class="p">()</span>

<span class="c1">#set model parameters</span>
<span class="n">Wxh</span><span class="p">,</span> <span class="n">Whh</span><span class="p">,</span> <span class="n">Why</span><span class="p">,</span> <span class="n">bh</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="n">set_model_params</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

<span class="c1">#return model samples</span>
<span class="n">iterate_and_sample</span><span class="p">()</span>
</code></pre></div>
<pre><code>## ----
##  SeptembertherefromLawsdrinkeasiestJohnsonpersonallyTurkchancedPumpkinsEnemiesWaresAstralhedgeThybafflesGivenhugestpurposelessloweredGoreddirtypublicItalygrassmoreOppositesraggedfainterBabblersGainedSetfantasticbrewAxlewouldnDrummerRegardlessfolddailyexcludeArgueswilthungovergrownawayrudefleeterInsolvencyrealwritLutesMrflourishinghurryingjoinsHeartyOrchardInsideSurelyuniqueTherelimbspalliateBisecteddetectAidTkneeimportantnearnessFeuneventrustingburialtransfiguredassistanceReprieveboilingstirsthemselvesplaythingplunderSustainPomposityindorsedadjustprovedRaggedMessageremoterSyllablesSyndicateRebukedisenchantsvalesteamunthinkingBegoneunladedDefinitionScarcermissingcandidPardonBullionTidessmitHorrorprotractsTearRallieshollowunsteadyRetrospectsSlippedwhimpersPreciselyparadiseSilkenclearAwfulDecaysnaturaldelightCarryshowedrememberedAccuseCorrodelessGentlestdisputedearthMalignitysailSoundlessSound​CaptiveRowerApartmentsAchieveGustsHareingotsstrictestoursHemsdaresbrieflyWaggonsAdmirationsarrangetrycarvedveilodiouserimportunedsafestshopsfledEmployedCommonestRegardgranderarrogantlyPleiadBalladsCherishTribunalMoundCasqueButtercupFeewhobuildRepugnantParlorsclovenfinishswoonbelievesresolutercommandAweCourserspoolBethleemrequisiteSankAcutemissesstillnessexceptRememberingmushroombees3meritedPrithee 
## ----
## iter 0, loss: 238.119642
## ----
##   . That pronoun—Day Mirage the revelation.Figures bleeds 
## makes Your Mirage merciful Yet
## That too
## the
## Subsides And suspends That Unreality.conferring unknown ThatIA And Lady
##  lives Date suspends sipped suspends the .' Makes.
## livesshoeAnd lives That
## a I A That Mirage  lives by suspends the makes
## And
## lives.sufficed suspends  across relief—makes 
## lives,Day A  suspends—
## makes not Superior merciful lives Instructs
## Unreality possible The. lives possible  merciful suspendsGrandmama Informing That living
## if Veil merciful makes—Withholding makes. suspends
## Clock lives. 
## ----
## iter 100, loss: 231.338567
## ----
##   That
## exists
##  low Dimple cannot Scientist waking stands
## , Diamond
## At any Spring
## lane
## Irresolute on Paradise Of present out fellow Ocean exists signed,
## Ores present Minnows
## 
## 
## A any
## And opened exists—
## the Gig
## —cunning present is Stipulus Horns
## present on nail,
## Spring dawn Spring Toils Spring The present
## s present the
## BritishWhoNot Spring
## the Spring  At the —is 
## any exists in
## The Vocal
## Spring,any to how,— —
## charms
## House Superior any Dead Not Carpenter—
## true
## Light doesn Year
## atom Yours replaced
## period Spring 
## ----
## iter 200, loss: 219.020527
## ----
##   Mine go'Ours go Unto done'Nest go Mine Lasts Mine go Mine —
## to Mine go'
## — too hateful go Trees Demeaning'fear go'Mine led A go— go'Mine go Mine Elephant hide go me go Mine go'despise go Mine go—Whose miss'the go Universe go Mine go
##  full Mine the'oppositely go Mine go'Mine go'Mine go —
##  go!So Half could go'Mine
## go'mind go Mine go'Mine go'Mine go Mine go Mine raps'Bog go'promised go and Bushes'
## go them go 
## ----
## iter 300, loss: 207.297738
## ----
##  passeach— gathered stated —The
## beg— ————— The Kinsmanship —
## 
##  appointed it
## a BeeBut
## ofletsinewsConstellation —The so —a in  Ihis— — or —Eaves beg —A food the   Beatrice—And Man Health
##   .has  Seller food — — 
## closing 
## in food dispelled to Axis extinct were Beamcunningwith and He It ——
## 
## 
##  
## 
## 
##  
## Costly— inspect Beam —
## Between QueenaA mouldering privilege
## it mock 
## ----
## iter 400, loss: 196.516351
## ----
##   so —
##  summer the wrecks s Son the if the espy
## —
## t Gold they it
## fell the idleness of him who is 
## 
## slow 
## 
## Friend an the, Blue 
##  shoe the  a'—
## They makes my lived in
##  than representative to—
## 
## I said yesterday encroached —
## And are 
## And Tiger—
## A the portion
## As copy leaned
## was rejects the Man —
## 
## A Rock slow —
## A are Orchard drop —
## Why crew the send
## 
## 
## A 
##  
## ----
## iter 500, loss: 186.106573
## ----
##   of shook never in stone— foundering away
## exist throatHaveshould flee— cannot burial My train Hoisted before Helmsman As your Decimals— Day Dissent
##  
## Gilded
##  worsted the a may—
## —
## 
## throat emigrate to ask not —
## trilled — Broadcloth
## Seas throat trilled —
## " In different Ardor prize Day me like throat be and  ,BereavementQueen
## diewoman sere throat Plate was tell Circumference by mention garden'A annul boast.down them Witness —And flambeaux
## for Surpasses Yet emptied The Of—
## Firmament Affliction — 
## ----
## iter 600, loss: 176.189993
## ----
##   becoming—
## Despair bone —
## Of Bells own my Of
## Mirth Life it
## Afraid you
##  it Plank
## the and veil vastflutteringindividuals acceded
## nor a Elder
## Of With 
##  of ?
## Nest exclaim
## then which Crown there
## I to brown
## As low Vitallest
## 
## navigation highest my —The I but
## back to sure Orthography—
## And touched Their
## ' Elder—
## 
## Did that
## Or The noon
## 
## Twere Deeper farness strike
## have the At
## 
## Unto the enough,
## to alive That triumphant
## 
## 
## Indies minor
## flung of
## Landscape
## With consider.
## Can, 
## ----
## iter 700, loss: 168.194207
## ----
##   cannot me
##  will ''Helmsman"
## was theSongAirpuzzled
## from 
## Speculationsbewildering  ,the Lutes  
##  
## 
## All
## 
## Nor
## , Brazil cunning Tis life night —Green Mind Emerald me entered —
## ,
##  what the entered,
## the Retrospection'entered
## 
## ,
## "travelled lived stimulate
##  Asphodel, velvet
##  an Ragged Person
## Upon Height '
## breastAndentered "To —
## The wanderings,
## All
## by Bronte "Potosi — Door Bronte Inn no "entered —
##  Chill the t "step in none d 
## ----
## iter 800, loss: 160.250708
## ----
##  
## 
## 
## unrolled definitely
## Consults mutual I!
##  manufacturing
## , keep Thro.I the tell
## All Sunset part. may,
## But Repast
##  many waked,
## But go
##  just and Tear
## 
## 
## heedless time soul
## 
## No,The —
## softer ignorance —
## Is share —
## An After every antiquated feetTearfrom It out—
## from would the Thunder —
## blame emptied her Nightgowns
## Then Steady
## s,
##  would village Divides 
## The that —
## 
## 
##  Doctors a take " wound
## 
## 
##  Twould the about
## Tear its delighted 
## ----
## iter 900, loss: 153.490894
## ----
##  
## 
## as 
## 
## 
## As An stand -
## 
## 
## 
## The Beware Butterfly
## 
## common be are 
## 
## love stand blaze stand —
## As new —
## 
## that a as,
## it Ah —
## But Mine stand a Plated —
## Trade Domain the dieswashis just ,
## Dial yesterday Oars Cashmere 
## a Climes stand They die. vermillion a stand skill Housewives a have, passed,
##  will of
##  Never a cribs 
## The stain hope'Science stand thing the!
## And idleness Today—
## 
## 
##  if 
## ----
## iter 1000, loss: 147.229345
</code></pre>
<p>These results look pretty decent, considering the very small size of our
corpus!</p>
<h1 id="exercises">Exercises</h1>
<p>These exercise prompts are meant to be starting points to encourage your
own research and exploration. Choose whichever one(s) interest you most!</p>
<ol>
<li>Use requests, BeautifulSoup, and the Wikipedia API to scrape a
    corpus of pages of your chosing. Here’s a Python wrapper:
    <a href="https://pypi.org/project/Wikipedia-API/">https://pypi.org/project/Wikipedia-API/</a>. Perform some
    preprocessing and make a few graphs.</li>
<li>Extend/modify our preprocessing function. Should it handle data in
    different formats? Should it perform stemming/lemmatization?</li>
<li>Write your own function to generate n-grams, and graph the top
    n-grams for various n.</li>
<li>Write your own rules-based poetry generator. You could try using the
    parts of speech dictionaries, or NLTK’s ‘similar’ words.</li>
<li>Try running the RNN after preprocessing our corpus, or try modifying
    the hyperparameters.</li>
<li>Train word2vec models
    (<a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a>) using our
    corpus, and look at word similarities.</li>
</ol>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../PythonWebScrape/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Web Scraping in Python" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Web Scraping in Python
            </div>
          </div>
        </a>
      
      
        
        <a href="../../support/trouble/" class="md-footer__link md-footer__link--next" aria-label="Next: Troubleshooting" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Troubleshooting
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; President & Fellows of Harvard College.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["toc.integrate", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.bd0b6b67.min.js"}</script>
    
    
      <script src="../../assets/javascripts/bundle.b88e97c5.min.js"></script>
      
    
  </body>
</html>