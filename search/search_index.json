{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The HBS Grid makes massive computing power accessible to the whole HBS research community. Our environments offer a familiar desktop interface and configured software applications including Julia, Jupyterlab, Matlab, Python, R / Rstudio, Stata, VSCode, and hundreds of other popular programs","text":"<p>The HBS Grid consists of many computers connected by a fast network and shared network storage. These powerful machines have up to 1.4Tb of memory and up to 32 cores each, and together they provide the backbone of HBS's research computing capabilities. User-friendly tools and environments are built on top of this physical and network infrastructure, making the power of our computing cluster accessible to all HBS researchers. Using the HBS Grid you can:</p> <ul> <li>Interactively analyze data too large for local memory.</li> <li>Speed up your analysis using hundreds of CPUs across multiple machines.</li> <li>Access your persistent remote desktop from anywhere with an internet connection.</li> <li>Store, backup, and access large research data.</li> <li>Share data and collaborate with HBS affiliates and guests.</li> </ul>"},{"location":"#quick-start","title":"Quick start","text":"<p>HBS faculty, staff, doctoral students and their guests are eligible for HBSGrid accounts. You can setup your account and connect via either GUI or terminal following the instructions below.</p> <p>Important</p> <p>Guest Users: Recall that before getting started you must change your temporary password and connect to the HBS VPN. Please see our detailed instructions here.</p> <p>Getting Started</p> <ol> <li>If you do not yet have an HBS Grid account request one here.</li> <li>Connect to the HBS network, either directly if you are on-campus or     connect via VPN     otherwise.</li> <li>Follow the instructions below to connect either via GUI or terminal.</li> </ol> Connect via GUIConnect via command line <ol> <li>If the NoMachine application is not yet installed download and install it.</li> <li>Start the NoMachine application.    </li> <li>If this is your first time connecting, click the      button and choose \"Add connection\" (not \"Add VPN connection\"). Enter these connection details:<ul> <li>Name: \"HBS Grid\", or anything you like</li> <li>Host: <code>hbsgrid-nx.hbs.edu</code></li> <li>Port: 4000</li> <li>Protocol: NX</li> </ul> </li> <li>Click the  button and enter your Username and Password.</li> </ol> <p>The video below demonstrates these steps visually.   Your browser does not support the video tag. </p> <p>NoMachine maintains a detailed connection guide  that you can refer to if needed. To troubleshoot connection difficulties, see our  troubleshooting checklist.</p> <p>A selection of our most popular applications are available in the favorites list pinned to the task-bar. Additional application launchers can be found in the Applications menu or by searching in Activities. You can add applications to your favorites list by right-clicking and selecting Add to Favorites.</p> <p>This environment was designed to be intuitive and user-friendly, and you are encouraged to start exploring the available software and tools. </p> <ol> <li> <p>In your favorite terminal program, run <pre><code>ssh USERNAME@hbsgrid.hbs.edu\n</code></pre> and enter your password when prompted.</p> <ul> <li>If you are on a PC and choose to use  SecureCRT, click \"Quick connect...\" and enter the following:<ul> <li>Protocol: SSH2</li> <li>Hostname: hbsgrid.hbs.edu</li> <li>Port: 22</li> <li>Firewall: none</li> <li>Username: your HBSGrid username</li> </ul> </li> </ul> </li> <li> <p>The software environment is not activated by default when connecting via <code>ssh</code>. Run <pre><code>ml rcs\n</code></pre> to enable the software environment.</p> </li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<p>To work on the cluster, you will probably want to learn about HBSGrid storage and transfer your data. You may also refer to our documentation on launching jobs from the desktop and running jobs from the command line.</p>"},{"location":"#feedback-and-support","title":"Feedback and support","text":"<p>Important</p> <p>Guest Users: For expired passwords and password resets, please contact RCS.</p> <p>Our research computing environments are actively developed and continuously improving. Bug reports and feature requests are important contributions to this project and are always welcome and encouraged! If you find that something doesn't work as expected, of if you have a feature request, we want to know about it so we can fix or improve it. </p> <p>There are several ways to connect:</p> <ul> <li> <p>For administrative requests including project and account creation or modification  please use our request forms.</p> </li> <li> <p>If you have a question, can't get something working, or if something looks broken you can reach out to us directly via email at research@hbs.edu.</p> </li> </ul>"},{"location":"accountmanagement/","title":"New Account Management System","text":"<p>RCS, along with our IT partners, are excited to announce the upcoming replacement of our HBSGrid account and project space provisioning system effective April 10, 2023. This upgrade will significantly improve the speed, efficiency, and automation of creating and maintaining HBSGrid accounts and project spaces.</p> <p>What will change for users on April 10th?</p>"},{"location":"accountmanagement/#new-and-improved-hbsgrid-account-and-project-space-request-forms","title":"New and Improved HBSGrid Account and Project Space Request Forms","text":"<p>We will be retiring our old account and project space request forms. Instead, you will use new forms to request new spaces and accounts (note that if you typically access forms from the Online Requests tab of our website, you can continue to do so; the links on that page will be updated to point to the new forms):</p> <ul> <li> <p>Request a new HBSGrid account:</p> <p>HBS Users: https://secure.hbs.edu/accountManagement/secure/research/account/new</p> <p>Guest Users: https://secure.hbs.edu/accountManagement/guest/research/account/new</p> </li> <li> <p>Request a new HBSGrid project space: https://secure.hbs.edu/accountManagement/secure/research/projectspace/new</p> </li> </ul> <p></p> <p>To add a user to a project space, request more space in your project folder, or make other project space changes, please continue to use this project space change request form or contact RCS directly at research@hbs.edu.</p>"},{"location":"accountmanagement/#streamlined-approval-emails","title":"Streamlined Approval Emails","text":"<p>HBSGrid guest accounts (i.e., accounts for users who do not have HBS credentials) and all HBSGrid project spaces must have an approved HBS sponsor, usually a faculty member. The new system makes it easier than ever for sponsors to approve account requests and project space members by simply clicking a link in an email. </p> <p>Who can be a sponsor?</p> <p>Faculty members may sponsor guest accounts and project spaces.  Doctoral students may also sponsor project spaces, but if the space  will include guest collaborators, the sponsor of the space must  be a faculty member or Jen Mucciarone.</p>"},{"location":"accountmanagement/#hbsgrid-accounts","title":"HBSGrid Accounts","text":""},{"location":"accountmanagement/#for-guest-account-sponsors","title":"For Guest Account Sponsors","text":"<p>When a guest user requests an account listing you as the sponsor, you will receive an automated email from <code>noreply@hbs.edu</code> with a link asking you to review and approve the request. Click the link to do so. If you do not approve or if you receive a request from a person you do not recognize, contact RCS at research@hbs.edu.</p> <p></p>"},{"location":"accountmanagement/#for-hbs-users","title":"For HBS Users","text":"<p>Most account requests from members of the HBS community will be fulfilled by RCS within a business day. Account requests for Research Associates will first be routed to Research Staff Services for approval before the account is created. When your account is ready, you will receive an automated welcome email from <code>noreply@hbs.edu</code>.</p>"},{"location":"accountmanagement/#for-guest-users","title":"For Guest Users","text":"<p>After you submit a guest account request, you will receive an automated email from <code>noreply@hbs.edu</code> asking you to review and sign a Guest User Agreement. Click the link to do so. Please note that this link will expire in 5 days.</p> <p>You must accept the agreement for your account to be created. Once the account is created, you will receive an encrypted automated email from <code>encrypted@hbs.edu</code> containing your username and temporary HBSGrid password. You may use this link only once and it expires in two weeks. When you retrieve your password, you will receive instructions on how to change your password and download our VPN software so that you can access the HBSGrid.</p>"},{"location":"accountmanagement/#project-space-access","title":"Project Space Access","text":"<p>When a new project space is ready or you have been added to a project space, you will receive an automated email from <code>noreply@hbs.edu</code> with the access information.</p>"},{"location":"accountmanagement/#for-project-space-sponsors","title":"For Project Space Sponsors","text":"<p>If a user requests access to your project space, you will receive an automated email from <code>noreply@hbs.edu</code> notifying you of the request. Click the link to view and approve your outstanding requests. </p> <p></p>"},{"location":"commandline/","title":"\ud83d\udc1a Use the Command-line","text":"<p>The HBSGrid uses IBM Spectrum LSF  to run applications on powerful remote computers. LSF is a large and complex  set of tools; our goal here is to give you just enough information so that you can use it to run jobs on our system, without overwhelming you with details and options.</p> <p>Graphical tools available</p> <p>This software environment includes robust graphical tools that reduce the need to use the command line for many interactive tasks. This section is for those who prefer the command line, either for aesthetic reasons or because they need to submit batch jobs or carry out complex operations that cannot  be easily performed using graphical menu-driven tools.</p> <p>LSF provides <code>bsub</code>, a command-line program for running applications on powerful remote computers. For example, you can use</p> <pre><code>bsub -q short_int -Is R\n</code></pre> <p>to start an interactive R job on a compute node. Breaking this example down will make the basics of bsub clear:</p> <ul> <li><code>bsub</code> (batch submission) is the top-level command used to run applications     on powerful remote machines.</li> <li><code>-q short_int</code> means you want to run on the short interactive queue  (details below).</li> <li><code>-Is</code> means we are running an Interactive shell.</li> <li>The rest of the command (<code>R</code> in this case) is the command that will be run on     the remote machine.</li> </ul> <p></p> <p>Compute cluster basics</p> <p>When you first log in to the HBS Grid using NoMachine or ssh you are running on what we call a \"login node\". The login nodes do not have substantial CPU or RAM available. All computationally intensive processes should be run on what we call \"compute nodes\". A diagram of the HBS Grid architecture helps make this clear:</p> <p></p> <p>As this diagram shows, the primary purpose of the login nodes is to serve as a hub  for launching jobs on powerful compute nodes. You can do that from the command line  using <code>bsub</code> or from the desktop menu using application launchers.</p> <p>You may sometimes wish to run applications on the login node, and this is perfectly fine as long as you are not using it for computationally intensive work. For example, you may wish to run <code>ipython</code> to work out a small code example, or use <code>locate</code> to find a file you were working on. These low-resource activities can and should be done on the login node. The important thing to remember is that <code>bsub</code> is used to run commands on powerful compute nodes.</p> <p>Important</p> <p>Please keep in mind that the system reserves the resources you select, e.g., CPUs used by your job become unavailable for other users. Request only 1 CPU unless you know that you are using code or libraries that were written to run in parallel. Specific memory requirements depend on the nature of the job, but as a rough guide we recommend requesting RAM 4-10 times the size of your data. For example, if you have a 6 Gb .csv file you may wish to request 24GB of memory or so.</p>"},{"location":"commandline/#resource-requirements","title":"Resource requirements","text":"<p>The <code>bsub</code> command allows you to specify RAM and CPU requirements for your job via the <code>-M</code> and <code>-n</code> arguments. For example, you can run a python job with 50 GB of RAM and 4 CPUs with</p> <pre><code>bsub -q short_int -M 50G -n 4 -Is python\n</code></pre> <p>When choosing RAM and CPU values keep in mind that  the system reserves the resources you select, e.g., CPUs used by your job become unavailable for other users. Please be considerate and try not to reserve resources you don't need.</p> <p>Specific memory requirements depend on the nature of the job, but as a rough guide we recommend requesting RAM 4-10 times the size of your data. For example, if you have a 6 Gb.csv file you may wish to request 24GB of memory or so.</p> <p>Additionally, you can review your memory usage from a past job by running <code>bhist -a -l JOBID</code> (use <code>bhist</code> alone for a list of your recently run jobs). Take note of MAX MEM and when you run a similar job in the future, request that amount plus about 20% for wiggle room (e.g., if your past job had a maximum memory usage of 10GB, request 12GB next time). If your jobs are shutting down unexpectedly, it is usually because you have exceeded the memory requested. See our section on troubleshooting jobs that exceed the memory limit for additional information.</p> <p>We recommend that you request only 1 CPU unless you know that you are using code or libraries that were written to run in parallel such as  Matlab parallel processint toolbox, Python multiprocessing library, or the R future package. For detailed parallel processing instructons refer to our tutorial.</p>"},{"location":"commandline/#using-gpus","title":"Using GPUs","text":"<p>GPUs on the HBSGrid are currently accessible by request only. To request access, please use our online form.</p> <p>To submit a GPU job, you must use the <code>gpu</code> queue (for batch jobs)  or the <code>gpu_int</code> queue (for interactive jobs).  You must also explicitly specify GPU options; using a single dash  for the default settings as shown in the example below is sufficient for many use cases.</p> <p>For example,</p> <p><pre><code>bsub -q gpu_int -Is -gpu - R\n</code></pre> will get an interactive R session with access to a GPU (note the trailing single dash in <code>-gpu -</code>, this uses the default GPU settings).</p> <p>If you need more control please refer to our How-To for GPU Computing or LSF's documentation for instructions on specifying more advanced GPU resource requirements.</p> <p>Knowing just these arguments to <code>bsub</code> will take you a long way. There is  much more to know about bsub, but these basics will get you started.</p>"},{"location":"commandline/#job-queues-limits","title":"Queue limits &amp; batch jobs","text":"<p>Machines on the HBS Grid are grouped in queues and <code>bsub</code> can start jobs in either batch (background) or interactive modes. Batch jobs make it easier to run many jobs at once and are more efficient because jobs don't keep running after the program is executed. Interactive jobs on the other hand tend to be more convenient, especially for exploratory work or when developing or debugging a script or program.</p> <p>Batch queues including short and long are for running commands without interaction. For example</p> <pre><code>bsub -q short Rscript my_r_code.R\n</code></pre> <p>runs <code>my_r_code.R</code> in batch mode, and</p> <pre><code>bsub -q short stata -b my_stata_code.do\n</code></pre> <p>runs <code>my_stata_code.do</code> in batch mode.</p> <p>Batch vs. interactive jobs</p> <p>The key differences when submitting batch vs interactive jobs are the <code>-q</code> and <code>-Is</code> arguments. For example we used <code>-q short</code> for batch and <code>-q short_int</code> for  interactive. Interactive jobs must also include the <code>-Is</code> option.</p> <p>Interactive queues like short_int and long_int are used to run applications that you will interact with. For example,</p> <pre><code>bsub -q short_int -Is rstudio\n</code></pre> <p>runs an interactive RStudio application, and</p> <pre><code>bsub -q short_int -Is xstata\n</code></pre> <p>runs an interactive Stata application.</p> <p>Queues have other characteristics in addition to the batch vs. interactive distinction. These include the maximum run time and maximum number of CPUs that can be reserved per job. These queue-level limits are summarized in the table below.</p> Queue Type Length Max Cores/Job long_int interactive 3 days 4 short_int interactive 1 day 12 sas_int interactive no limit 4 gpu_int interactive. no limit 4 long batch 7 days 12 short batch 3 days 16 gpu batch no limit 4 sas batch no limit 4 unlimited interactive or batch no limit 4"},{"location":"commandline/#run-batch-jobs-in-parallel","title":"Run batch jobs in parallel","text":"<p>Other LSF front-ends</p> <p>There are language-specific ways to submit job arrays that may be more convenient than the <code>bsub</code> job array approach described here. For example, R users may wish to consider ClusterMQ or batchtools and Python users may find Dask-jobqueue more convenient.</p> <p>It is often useful to split a big job up into many small pieces and run them all simultaneously.This allows you to spread the work out across multiple machines on the HBS Grid and can dramatically reduce the time needed for your computation. You can use <code>bsub</code> job arrays to submit multiple jobs simultaneously. For example, the following command creates a job array that runs <code>Rscript run.R</code> 100 times:</p> <pre><code>bsub -q short -J \"myArray[1-100]\" Rscript run.R\n</code></pre> <p>Important</p> <p>Job arrays are powerful tools and you must take care to use them properly. It is easy to accidentally monopolize system resources, e.g., by running a large array of jobs that reserve more memory (via the <code>-M</code> argument) then they need. For example, if each of your 100 jobs reserves 20Gb of memory but only uses 10 you will have wasted 1Tb of memory. Please take care to avoid this by matching your resource reservation arguments to the actual resources needed by your program.</p> <p>Often you will want each job in the array to process a different file or use a different parameter value. For this purpose LSF sets the <code>LSB_JOBINDEX</code> environment variable to the job array index (1-100 in the example above). In this case your program (<code>run.R</code> in the example above) should retrieve this environment variable and use it to determine the correct inputs or parameter values.</p> <p>For more details refer to the LSF job array documentation.</p>"},{"location":"commandline/#troubleshooting-jobs-and-resources","title":"Monitor &amp; troubleshoot","text":"<p>A variety of problems can arise when running jobs and applications on the HBSGrid. LSF provides command-line tools to monitor and inspect your jobs to help you figure out if something goes wrong:</p> <ul> <li><code>bjobs</code> lists your running jobs</li> <li><code>bhist -a</code> lists your recent jobs</li> <li><code>bjobs -l &lt;JOBID&gt;</code> gives details about running job <code>&lt;JOBID&gt;</code></li> <li><code>bhist -l &lt;JOBID&gt;</code> gives details about job <code>&lt;JOBID&gt;</code></li> </ul> <p>For more detailed troubleshooting help refer to the  troubleshooting documentation.</p>"},{"location":"environments/","title":"\ud83d\udd19 Use Software Versions","text":"<p>Each time we update our software environments we preserve previous versions so that you can roll back for reproducibility or if your code stops working after an update. This section details the specific software environment versions available.</p>"},{"location":"environments/#select-desktop-environment","title":"Select desktop environment","text":"<p>As an illustration of the benefits of preserving historical software environments, imagine that you have a python project and that your Pandas code no longer works with the latest Pandas release in the current software environment. In that case you can start Spyder and  revert to a previous software environment in order to run your analysis  using an older version of Pandas. The screen-shot below shows how to  use the Software environment version selector to run and older version of python.</p> <p></p> <p>Software environments are named following a <code>rcs_year.version</code> scheme. For example, the last environment released in 2022 is named <code>rcs_2022.11</code>. The list below shows you key information about each environment, including a command that you can run from the terminal to get a detailed software version list.</p>"},{"location":"environments/#select-terminal-environment","title":"Select terminal environment","text":"<p>In order to facilitate reproducible research and analysis we preserve old software environments so that you can switch back to them later if needed. These older environments can be loaded using Lmod.</p> <p>Running <pre><code>ml avail\n</code></pre> will show you the available environments, named by date and version number.</p> <p>For example, suppose that you have a python project and that your pandas code no longer works with the latest pandas release in the current software environment. In that case you can revert to a previous software environment and run your analysis using an older version of pandas.</p> <p>You can use the <code>ml</code> command from the terminal to list, load, and unload Lmod environment, as shown below.</p> <pre><code>      ml avail\n\n--------------------------- /usr/local/app/rcs_bin/grid3/modulefiles ---------------------------\n   rcs/rcs_2021.03        rcs/rcs_2022.01      rcs/rcs_2023.09 (D,L)\n   rcs/rcs_2020.01        rcs/rcs_2021.06      rcs/rcs_2022.11    \n\n\n        Where:\n        D:  Default Module\n        L:  Module is loaded\n\n        Use \"module spider\" to find all possible modules.\n        Use \"module keyword key1 key2 ...\" to search for all possible modules matching\n        any of the \"keys\".\n</code></pre> <p>You can get detailed information about specific software modules using the <code>ml spider</code> command:</p> <pre><code>module spider rcs/rcs_2023.09\n</code></pre> <pre><code>----------------------------------------------------------------------------\n  rcs: rcs/rcs_2023.09\n----------------------------------------------------------------------------\n    Description:\n      Anaconda environment for research computing\n\n\n    This module can be loaded directly: module load rcs/rcs_2023.09\n\n    Help:\n      Sets up environment for Data Science and Statistical computing.\n\n      A huge list of software is avalable, including 'python', 'spyder', 'R', \n      'rstudio', 'emacs', 'vscode', rclone, ripgrep, nnn and much more.\n\n      See https://hbs-rcs.github.io/hbsgrid-docs/ for documentation\n      and https://hbs-rcs.github.io/hbsgrid-docs/environments/#rcs_2023.09\n      for version-specific details.\n\n      For a detailed software list open a terminal and run \n\n      conda env export -n rcs_2023.09\n</code></pre> <p>Finally you can use <code>ml</code> to load and unload specific environments. <pre><code>ml rcs/rcs_2022.11\n</code></pre> will load the rcs_2022.11 environment, and  <pre><code>ml -rcs/rcs_2022.11\n</code></pre> will unload it.</p> <p>Detailed Lmod documentation is available here and you can learn more about the environments available on the HBS Grid in the Environments versions documentation.</p>"},{"location":"environments/#reproducing-environments","title":"Reproducing environments","text":"<p>The instructions above show how to use different software environment versions on the HBS Grid. You may sometimes need to go a step further than this, e.g., to continue your work on another system after you leave HBS, or to provide reproduction instructions to meet a journal publication requirement. To do this you need to know that  the environments described here are managed using the conda package manager on a Linux system. </p> <p>You can recreate these environments on Linux systems by following the steps below:</p> <p>Re-create environments on another system</p> <ol> <li>Obtain access to a Linux system. If you don't have have a suitable physical computer     you may wish to install one in a virtual machine.     Many tutorials are available    to show you how to do this.</li> <li>Download the MambaForge installer    and install it on your Linux system.</li> <li>Create the conda package list.</li> <li>Copy the package list file to your Linux system and     use <code>conda</code> to re-create the environment.</li> </ol> <p>In general it is not possible to exactly re-create these environments on Windows or Mac machines. You can however examine the package lists and manually create environments with the same versions of the software needed for your project.</p>"},{"location":"environments/#create-your-own-environments","title":"Create your own environments","text":"<p>Software installation requests</p> <p>If you find that the software you need is not available in the standard HBS Grid software environments please reach out to research@hbs.edu.</p> <p>If you prefer to create and manage your own software environments you may do so using  conda. This is the same package manager used to maintain the system-wide software environments on the HBS Grid. <code>conda</code> has already been installed and configured for you on the HBS Grid, making it easy to create and manage your own environments by following the  official conda environment documentation.</p> <p>You can share and reproduce your own <code>conda</code> environments on other computers and systems as well. When setting up <code>conda</code> outside the HBS Grid we strongly encourage using the  MambaForge installer to get going with <code>conda</code> more quickly. (<code>conda</code> is already installed and configured for you on the HBS Grid, there is no need to  install it yourself there.)</p> <p>Some useful documentation on creating environments, installing packages, and sharing is available, along with a helpful tutorial.</p>"},{"location":"environments/#environment-versions","title":"Environment versions","text":"<p>Current and historical software environments available system-wide on the HBS Grid are described below.</p>"},{"location":"environments/#rcs_202409","title":"rcs_2024.09","text":"<p>This software environment is a user-friendly collection of software and utilities designed to make data science and statistics easier for HBS Grid users. </p> <p>If there is a software program that you need is not yet available and would be of benefit to the  larger community, please contact us via the Research Inbox and we will follow-up as needed.</p> <p>More details about this release will be available soon.</p>"},{"location":"environments/#rcs_2023.09","title":"rcs_2023.09","text":"<p>This software environment is a user-friendly collection of software and utilities designed to make data science and statistics easier for HBS Grid users. </p> <p>The 2023.09 release brings a huge number of application and package updates, including:</p> <ul> <li>Python updated to 3.10.12</li> <li>R updated to 4.2.2</li> <li>Stata updated to version 18</li> <li>Octave updated to 7.3.0</li> <li>Julia updated to 1.9.0</li> <li>RStudio updated to 2022.12.0</li> <li>Spyder updated to 5.4.5</li> <li>LibreOffice updated to 7.5.5.2</li> <li>VSCode updated to 1.82.0</li> <li>Arrow (C++, R and Python) updated to 11.0</li> <li>Tensorflow updated to 2.11</li> <li>PyTorch updated to 2.0.0</li> <li>CUDA toolkit updated to 11.8.0</li> <li>nltk updated to 3.8.1 </li> <li>Jupyterlab updated to 3.6.5</li> <li>MKL updated to 2022.2.1</li> <li>gcc updated to 12.3.0</li> </ul> <p>...and many others.</p> <p>Versions that remain unchanged:</p> <ul> <li>Emacs 28.2</li> </ul> <p>Documentation is available on line or via the HBS Grid help application on the Grid. If you have any difficulties or feature requests please reach out to RCS to continue the conversation.</p> <p>For complete environment details, open a terminal and run <pre><code>conda list -n rcs_2023.09  &gt;  ~/rcs_2023_09_versions.txt\n</code></pre></p>"},{"location":"environments/#rcs_2022.11","title":"rcs_2022.11","text":"<p>This software environment is a user-friendly collection of software and utilities designed to make data science and statistics easier for HBS Grid users. </p> <p>In this release we have added a number of new applications and packages, including:</p> <ul> <li>Quarto, an open-source scientific and technical publishing system built on Pandoc</li> <li>Gephi visualization and exploration software for all kinds of graphs and networks</li> <li>GitHub CLI brings GitHub to your terminal</li> <li>SCC a very fast accurate code counter with complexity calculations and COCOMO estimates written in pure Go</li> </ul> <p>If there is a software program that you need is not yet available and would be of benefit to the  larger community, please contact us via the Research Inbox and we will follow-up as needed.</p> <p>The 2022.11 release brings a huge number of application and package updates, including:</p> <ul> <li>Python updated to 3.10.6</li> <li>R updated to 4.2.1</li> <li>MATLAB updated to 2022b</li> <li>Octave updated to 7.2.0</li> <li>Julia updated to 1.8.2</li> <li>RStudio updated to 2022.07.2</li> <li>Spyder updated to 5.3.3</li> <li>LibreOffice updated to 7.3.6</li> <li>VSCode updated to 1.72</li> <li>Emacs updated to 28.2</li> <li>Arrow (C++, R and Python) updated to 9.0</li> <li>Tensorflow updated to 2.10</li> <li>PyTorch updated to 1.12.1</li> <li>CUDA toolkit updated to 11.7.0</li> <li>Jupyterlab updated to 3.5.0</li> <li>MKL updated to 2022.1.0</li> </ul> <p>and many others.</p> <p>Documentation is available on line or via the HBS Grid help application on the Grid. If you have any difficulties or feature requests please reach out to RCS to continue the conversation.</p> <p>For complete environment details, open a terminal and run <pre><code>conda list -n rcs_2022.11\n</code></pre></p>"},{"location":"environments/#rcs_2022.01","title":"rcs_2022.01","text":"<p>This software environment is a user-friendly collection of software and utilities designed to make data science and statistics easier for HBS Grid users.</p> <p>In this release we have added a large number of new statistics and data science applications and packages, including:</p> <ul> <li>JASP, a free     menu-driven statistics application similar to SPSS</li> <li>Cytoscape, an     open source software platform for visualizing complex networks,</li> <li>DuckDB, an in-process     SQL OLAP database management system</li> <li>texminer,     functions for text mining and topic modeling in R</li> <li>Dedupe,     a library that uses machine learning to perform de-duplication and     entity resolution in Python</li> <li>awscli,     a unified tool to manage your AWS services</li> <li>snakemake,     a workflow management system to create reproducible and scalable     data analyses</li> </ul> <p>and many many more!</p> <p>If you find a software program that you need is not yet available please let us know and we will try to install it for you.</p> <p>The 2022.01 release also brings a huge number of application and package updates, including:</p> <ul> <li>Python updated to 3.9.9</li> <li>R updated to 4.1.1</li> <li>Octave updated to 6.4</li> <li>Julia updated to 1.7.1</li> <li>RStudio updated to 2021.09.1</li> <li>Spyder updated to 5.2.1</li> <li>LibreOffice updated to 7.1.8</li> <li>VSCode updated to 1.63.2</li> <li>Emacs updated to 27.2</li> <li>Arrow (C++, R and Python) updated to 6.0</li> <li>Tensorflow updated to 2.7</li> <li>PyTorch updated to 1.10.0</li> <li>CUDA toolkit updated to 11.5.0</li> <li>Jupyterlab updated to 3.1.0</li> <li>MKL updated to 2021.4.0</li> </ul> <p>and hundreds of others.</p> <p>In this release we have also dropped support for several infrequently used programs:</p> <ul> <li>OCRfeeder -- use gImageReader for OCR instead</li> <li>Gephi -- replaced by Cytoscape for network visualization</li> <li>PSPP -- replaced by JASP, a modern statistics GUI that uses R under     the hood</li> <li>Meld -- use Diffuse for graphical text comparisony</li> </ul> <p>Documentation is available on line or via the HBS Grid help application on the Grid. If you have any difficulties or feature requests please reach out to us.</p> <p>For complete environment details, open a terminal and run <pre><code>conda list -n rcs_2022.01\n</code></pre></p>"},{"location":"environments/#rcs_2021.06","title":"rcs_2021.06","text":"<p>The <code>rcs_2021.06</code> environment was released in May 2021. It includes updated Octave, Python, QGIS, R, Stata, and other software. Key software versions included in this environment are listed below.</p> <ul> <li>CUDAtoolkit 11.2</li> <li>Spyder 5.0</li> <li>Texlive 2021</li> <li>Emacs 27.2</li> <li>Julia 1.6.1</li> <li>Jupyterlab 3.0</li> <li>Mathematica 12</li> <li>Matlab R2021a</li> <li>Numpy 1.20</li> <li>Octave 6.2</li> <li>Pandas 1.2</li> <li>Python 3.9</li> <li>Pytorch 1.8</li> <li>QGIS 3.18</li> <li>R 4.0</li> <li>R-tidyverse 1.3</li> <li>SAS 9.4</li> <li>Stata 17</li> <li>Tensorflow 2.4</li> </ul> <p>For complete environment details, open a terminal and run</p> <pre><code>conda env export -n rcs_2021.06\n</code></pre>"},{"location":"environments/#rcs_2021.03","title":"rcs_2021.03","text":"<p>The <code>rcs_2021.03</code> environment was released in March 2021. It includes updated Octave, Python, QGIS, R, Stata, and other software. Key software versions included in this environment are listed below.</p> <ul> <li>CUDAtoolkit 10.1</li> <li>Emacs 27.1</li> <li>Julia 1.5.3</li> <li>Jupyterlab 3.0</li> <li>Mathematica 12</li> <li>Matlab R2020a</li> <li>Numpy 1.20</li> <li>Octave 6.2</li> <li>Pandas 1.2</li> <li>Python 3.8</li> <li>Pytorch 1.7</li> <li>QGIS 3.16</li> <li>R 4.0</li> <li>R-tidyverse 1.3</li> <li>SAS 9.4</li> <li>Stata 16</li> <li>Tensorflow 2.2</li> </ul> <p>For complete environment details, open a terminal and run</p> <pre><code>conda env export -n rcs_2021.03\n</code></pre>"},{"location":"environments/#rcs_2020.01","title":"rcs_2020.01","text":"<p>The <code>rcs_2020.01</code> environment was released in March 2020. It includes updated Octave, Python, QGIS, R, Stata, and other software. Key software versions included in this environment are listed below.</p> <ul> <li>CUDAtoolkit 10.1</li> <li>Emacs 27.1</li> <li>Julia 1.5.3</li> <li>Jupyterlab 2</li> <li>Mathematica 12</li> <li>Matlab R2019a</li> <li>Numpy 1.19</li> <li>Octave 6.2</li> <li>Pandas 1.2</li> <li>Python 3.7</li> <li>R 3.6</li> <li>R-tidyverse 1.2</li> <li>SAS 9.4</li> <li>Stata 15</li> <li>Tensorflow 2.2</li> </ul> <p>For complete environment details, open a terminal and run</p> <pre><code>conda env export -n rcs_2020.01\n</code></pre>"},{"location":"help/","title":"Help and support","text":"<p>If you run into any problems please let us know! You can reach out to us directly via email at  research@hbs.edu. Our support team is happy to assist you.</p> <p>We've provided checklists below for the most common issues that our users experience.  To help us help you, please work through the relevant list(s) and include all relevant information (LSF logs, error logs and messages, etc.) and at which step you encountered an issue when you reach out to us.</p>"},{"location":"help/#hbsgrid-account-login-issues","title":"HBSGrid Account &amp; Login Issues","text":"<ol> <li>If you are a guest, have you successfully completed our instructions to obtain access to the cluster?</li> <li>Are you connected to the appropriate network: i.e., the HBS VPN if off-campus, or to the wired HBS ethernet/HBSSECURE wireless if on-campus? <ul> <li>Please note that using Harvard Secure VPN/Wireless or a non-HBS ethernet connection will not work.</li> </ul> </li> <li>Are you using the correct username and login hostnames as outlined in our Quick Start guide?</li> <li>If you are accessing the cluster via NoMachine:<ul> <li>Is your home folder full?</li> <li>Have you altered your login scripts (.bashrc / .bash_profile)? Activating conda, software modules or other environments in these config files can cause problems with NoMachine connections.</li> </ul> </li> </ol>"},{"location":"help/#storage-project-spaces-access-problems","title":"Storage &amp; Project Spaces - Access Problems","text":"<ol> <li>Has your access been approved by the sponsor of your space? <ul> <li>When you request access, our automated system sends the project space sponsor an email to formally approve your access. We recommend contacting them to ensure they have received the email to approve access.</li> </ul> </li> <li>If you received an email indicating that the project space sponsor has approved your access, have at least 2 hours passed since you were approved? This is how long the approval may take to sync in our systems.</li> <li>If you are trying to access the project space by mounting/mapping the drive, please check:<ul> <li>Are you connected to the appropriate network: i.e., the HBS VPN if off-campus, or to the wired HBS ethernet/HBSSECURE wireless if on-campus? </li> <li>Are you using the correct path / URL?</li> </ul> </li> <li>If you are trying to access the project space via NoMachine, have you tried terminating your session and logging back in?</li> <li>If you are trying to access the project space via Terminal, have you tried accessing the project space from a new terminal?</li> </ol>"},{"location":"help/#storage-project-spaces-permission-problems","title":"Storage &amp; Project Spaces - Permission Problems","text":"<ol> <li>Have you hit the quota for the home or project folder? <ul> <li>You may have received an email notifying you about having reached storage quotas on your home folder or project space.</li> <li>You can also check your disk usage for project spaces by running the command <code>df -h filepath/to/directory</code>.</li> </ul> </li> <li>Are the permissions set appropriately for (shared) read/write access? <ul> <li>Inconsistent group ownership or read/write access can cause permission denied errors.</li> <li>Verify the permissions by viewing the item's properties in the Files browser in NoMachine/Gnome or by running <code>ls -al filepath/to/directory</code> in a terminal. </li> <li>Use our File Permissions instructions to change file/directory permissions. Enlist a colleague's help if needed, especially if the person owns the item in question.</li> </ul> </li> <li>Did you transfer data via mounted volumes instead of via SFTP (e.g., Filezilla or Cyberduck)? Doing so often results in unexpected file / folder permissions. Please see our instructions in section 2 to change the permissions if this is what is causing the issue, and we recommend using other data transfer methods going forward.</li> </ol>"},{"location":"help/#running-interactive-batch-applications-jobs-pends-or-not-running","title":"Running Interactive &amp; Batch Applications (Jobs) - PENDs or Not Running","text":"<ol> <li>If using the GUI Launchers in NoMachine, did a warning dialog appear? <ul> <li>Is this explanation \u2013 \"Job Requirements Not Satisfied\" == no room yet on the cluster \u2013 indicative of the problem?</li> </ul> </li> <li>Have you run the HBSGrid Job Monitor script or checked the status of your job using the terminal?<ul> <li>In NoMachine/Gnome, select Applications &gt; System Tools &gt; HBSGrid Job Monitor, or simply search for \"Job Monitor\".</li> <li>Launch NoMachine's terminal via Applications &gt; System Tools &gt; Terminal, select 4GB RAM (and 1 CPU) to get a local terminal for using <code>bjobs</code> and its options.</li> </ul> </li> <li>Do you already have 3 interactive sessions or 12 interactive cores running on the short_int queue or 4 interactive cores running on the long_int queue, or do you already have 150 cores running in total on the cluster? If you do, you have reached the limit of the resources one user can request. You can wait for these jobs to finish or you may opt to terminate one of your running jobs to prioritze another:<ul> <li>Launch NoMachine's terminal via Applications &gt; System Tools &gt; Terminal, select 4GB RAM (and 1 CPU) to get a local terminal. Review your running jobs using <code>bjobs</code> and its options. You can terminate a job using the <code>bkill JOBID</code> command or all jobs using <code>bkill 0</code>.</li> </ul> </li> <li>Did you ask for &gt; 50-100 GB RAM and/or &gt; 4-8 cores? If so:<ul> <li>Are you over-asking? Could your request be reduced using information from previous runs or based on past usage or data file sizes/types?</li> <li>Are you doing \"big data\" work? Could this be done more efficiently?</li> </ul> </li> <li>If using a terminal, did you submit your job to the correct queue with the correct parameters? Or did you submit a job that could never be scheduled (e.g. a RAM size that won't fit anywhere)?</li> </ol>"},{"location":"help/#running-interactive-batch-applications-jobs-crashes-problems","title":"Running Interactive &amp; Batch Applications (Jobs) - Crashes &amp; Problems","text":"<ol> <li>Do you have the jobID of your program?<ul> <li>If not, use <code>bhist \u2013a</code> and/or <code>bhist -l jobid</code> to get LSF details </li> </ul> </li> <li>Have you exceeded the time limit for your queue or run session? </li> <li>Does your program generate its own logs? If so, what do these indicate? </li> <li>Are you writing log entries to troubleshoot where you are having problems? </li> <li>If you are running a batch program, are you saving the cluster errors and output for your job? This can be accomplished using the bsub -o and -e options, e.g., <code>bsub -q short -W 6:00 -R \"rusage[mem=4000]\" -M 4000 -o output_%J.out -e error_%J.err -B -N -u jharvard@hbs.edu</code> </li> </ol> <p>If you need additional assistance from RCS, please include the JOBID and any of the above logs in your email. </p>"},{"location":"menulaunch/","title":"\ud83d\ude80 Run Desktop Applications","text":"<p>You can run applications on powerful HBS Grid  compute nodes by  connecting to the HBS grid via NoMachine and clicking  one of the application icons under the Applications or Activities menus.  This allows you to easily run compute and/or memory intensive applications with  just a few mouse clicks!</p>"},{"location":"menulaunch/#basic-launcher-options","title":"Basic launcher options","text":"<p>Each application will open a dialog where you can configure your environment and resource requirements.</p> <p>The application launchers are meant to be intuitive and easy to use. Most of the fields should be self-explanatory, e.g., there is a numeric field for memory (RAM) in Gigabytes, and another for the number of CPUs needed. As a convenience you can select a starting directory.</p> <p>If your application needs a GPU make sure that you check the Needs GPU box.</p> <p>Click the video thumbnail below to watch an application launcher demonstration:</p>  Your browser does not support the video tag."},{"location":"menulaunch/#resource-recommendations","title":"Resource recommendations","text":"<p>When choosing RAM and CPU values, keep in mind that  the system reserves these resources exclusively for your use. That is, cores and RAM used by your job become unavailable for other users. Please be considerate and do not reserve resources you don't need.</p> <p>Specific memory requirements depend on the nature of the job, but as a rough guide:</p> <ul> <li> <p>If your code runs on your local machine, start by asking for      the same amount of RAM or less (for example, if your laptop has 8GB of RAM, try asking for 8GB).</p> </li> <li> <p>If you are loading in native binary data files,      ask for an amount of RAM about 1.5x the size of your data.</p> </li> <li> <p>If you are importing a text file (e.g., CSV), you may need to request      up to 8 - 10x the size of the text file.     This should be a one-time operation: We discourage ongoing reading and writing of text files,      as these operations are expensive, especially on large data files.      To save time and RAM, try to read your text files into binary data files      and work primarily with those. (You may find Stat/Transfer helpful for this.)</p> </li> <li> <p>You can review your memory usage from a past job by running <code>bhist -l &lt;JOBID&gt;</code>     use <code>bhist</code> alone for a list of your recently run jobs).      Take note of <code>MAX MEM</code> and when you run a similar job in the future,      request that amount plus about 20% for wiggle room      (e.g., if your past job had a maximum memory usage of 10GB, request 12GB next time).</p> </li> </ul> <p>We recommend that you request only 1 CPU, especially for interactive work, unless you know that you are using code or libraries that were written to run in parallel such as  Matlab parallel processing toolbox, Python multiprocessing library, or the R future package. For detailed parallel processing instructions refer to our tutorial.</p>"},{"location":"menulaunch/#advanced-launcher-options","title":"Advanced launcher options","text":"<p>Important</p> <p>It is a good idea to note the current software environment version when you start a new project so you will know which environment to switch back to if needed.</p> <p>If you need to use an older software environment you can do so using the software environment version dialog. Usually there is no need to use an environment other than the default; the purpose of this mechanism is to make it easy to reproduce an old analysis using a specific software environment. </p> <p>The software and versions available in each environment are documented in Software Applications and Environments. </p> <p>Some application launchers have a Pre-submission command field. This allows you to run an arbitrary bash command immediately before submitting the job. For example, you can use it to set environment variables or activate conda environments. </p>"},{"location":"menulaunch/#system-resources-limits","title":"System resources &amp; limits","text":"<p>The HBS Grid is configured with limits to prevent any single user from monopolizing the available resources. Understanding and working within these limits will make your experience on the HBS Grid more productive and enjoyable.</p>"},{"location":"menulaunch/#configured-system-limits","title":"Configured system limits","text":"<p>In some cases the desktop launchers will down-grade your request to the maximum available if your request exceeds the system specified limits. There are both user-level, queue-level and job-level limits on the resources that are available to you:</p> <ul> <li> <p>You are limited to     3 concurrent interactive jobs.</p> </li> <li> <p>You are limited to a total of     24 CPUs allocated to interactive jobs at any given time.</p> </li> <li> <p>Interactive jobs are limited to 12 CPUs on the short_int queue, or 4     CPUs for the long_int queue.</p> </li> </ul> <p>As a practical example of these limits, if you try request 12 CPUs and runtime greater than 24 hours the system will not be able to meet your request. In that case it will offer to give you 4 CPUS (the maximum available for jobs running more than 24 hours).</p> <p>More information about queue-level limits can be found in the command line documentation.</p>"},{"location":"menulaunch/#available-resources-and-resource-contention","title":"Available resources and resource contention","text":"<p>The HBS grid usually has substantial computational resources available, but sometimes an unusually large number of users are trying to use a lot of resources at the same time. In this case resources may become scarce and you may not be able to access the resources you need.</p> <p>You can get a quick overview of the state of the cluster using the  HBS Grid available resources utility, available in the applications menu. This will give you a sense of the current activity on the cluster, and a rough idea of the resources currently available to you.</p> <p>Click the video thumbnail below to watch a demonstration of the available resources utility:</p>  Your browser does not support the video tag.  <p>The graphical menu-based launchers documented in this section are a quick and convenient way to run interactive applications on powerful compute nodes. In the case where you wish to run many such jobs you may find it more convenient to run batch jobs. Refer to the command line documentation for details.</p>"},{"location":"software/","title":"\ud83d\udce6 Available software","text":"<p>A huge range of software applications, utilities, and libraries are installed and configured for you. Whether you need Rstudio or Spyder, Julia running in VSCode, popular R or Python packages, of fully configured Jupyter Notebooks, we have you covered.</p> <p>The list of installed software is so large we make no effort to enumerate everything here, but you can always get an up-to-date list by opening a terminal on the Grid and running <pre><code>conda list\n</code></pre></p> <p>Start with the expectation that all the software you need is already installed and ready to use. If that expectation is ever broken please email research@hbs.edu.</p> <p>The most recent version of our software environment is activated when you log  in to the HBS Grid via NoMachine and nothing further is required unless you wish to use an environment other than the default.</p> <p>When logging in via SSH from a terminal (i.e., without NoMachine) you must  explicitly activate the environment you wish to use after logging in. The default environment can be activated by running </p> <pre><code>ml rcs\n</code></pre> <p>Refer to the terminal environment documentation for details.</p>"},{"location":"software/#installing-packages-or-modules","title":"Installing Packages or Modules","text":"<p>The HBSGrid uses the conda package manager to install most R packages and Python modules that researchers will need in a central, read-only location. As we cannot anticipate everyone's requirements, you may have the need to install packages not centrally installed.</p> <p>We recommend the following:  - Install packages/modules in home folders for personal, testing, or development work, OR - Install packages/modules in project spaces for production or team use</p> <p>One can use packages in all three locations, and can have the different versions of the same package installed in different locations. Note that it is the search path of your R or Python environment that will determine the priority order. We usually recommend (highest to lowest): - home folder (<code>$HOME</code> or <code>~</code>) installations - project folder installations - central (cluster) installations</p>"},{"location":"software/#installing-r-packages","title":"Installing R Packages","text":"<p>For installation in home folders: Use the standard command for  <code>install.packages()</code> command from within R/RStudio will download and install the specified packages in your home folder by default. If the correct install folder is not already there, R/RStudio will create it for you:</p> <pre><code>install.packages('somepkg')\n</code></pre> <p>See the R documentation  for additional options (install from another location besides CRAN, overwrite current  installation, etc.)</p> <p>For installation in project spaces: If you are working on a team and this is something  for everyone to use, install the packages using the 'lib=' option:</p> <pre><code>install.packages('somepkg', lib='/path/to/project/R/')\n</code></pre> <p>In order for this to be seen when working on your project in this location, one can pre-  or post-pend this location with a <code>.libPaths()</code> command  as part of a startup script  that runs when one opens a project or via new R session. </p> <p>Note: as R and RStudio are managed via our central conda installation, one can also use  <code>conda</code> and conda environments to manage our packages, which we don't recommend.</p>"},{"location":"software/#installing-python-modules","title":"Installing Python Modules","text":"<p>Python modules can be installed via two methods: using pip install (recommended) or via creating conda environments. The latter is more heavyweight, as one is installing all the python infrasture and core, in addition to the modules you need. See our conda environments documentation if this is what is needed.</p> <p>For installation in home folders: Include the <code>--user</code> option to perform this installation:</p> <pre><code>pip install --user some_module\n</code></pre> <p>To update/upgrade a module already installed, include also the <code>--upgrade</code> option: </p> <pre><code>pip install --upgrade --user some_module\n</code></pre> <p>The installed modules will be located at <code>$HOME/.local/lib/pythonxx.yy</code>, with <code>xx.yy</code> being the python major.minor version. </p> <p>For installation in project spaces: If you are working on a team and/or for production work, install the modules with pip install, though use the --prefix option instead:</p> <pre><code>pip install --prefix /path/to/project/.local some_module\n</code></pre> <p>Similar to R, one will need to include this location in the PYTHONPATH using <code>sys.path</code> as a part of one's first few lines of code or as part of a Python startup script.</p>"},{"location":"software/#installing-compatible-software-on-the-hbsgrid-cluster","title":"Installing Compatible Software on the HBSGrid Cluster","text":"<p>Ideally, the application you need will have a version compatible with the cluster's OS (operating system; currently Red Hat Enterprise Linux 7, also known as RHEL 7.) Examples include the PyCharm Python IDE and <code>fzf</code> command-line fuzzy-finder. If you have a compatible application, using the terminal, download and extract the program; run any installer provided (if there is no installer just move the program wherever you like); and run the software directly from the terminal command line. If the software is available in multiple versions, pick one that mentions \"RHEL\", \"RPM\", \"Linux x86-64 / AMD64\", or just \"Linux\".</p> <p>As an illustration, we can install PyCharm as follows:</p> <ol> <li>Connect to the HBSGrid via NoMachine, start Terminal from the menu, and download Pycharm: </li> </ol> <p><pre><code>wget https://download.jetbrains.com/python/pycharm-community-2020.1.3.tar.gz\n</code></pre> 2. You will notice that the file has a '.tar.gz' extension, indicating that it needs to be decompressed with <code>tar</code>: <pre><code>tar -xvf pycharm-community-2020.1.3.tar.gz\n</code></pre> 3. Submit an interactive job to run Pycharm on a compute node:  <pre><code>bsub -I pycharm-community-2020.1.3/bin/pycharm.sh\n</code></pre> If all goes well you will see the Pycharm splash screen and you can start a new Python project using this popular IDE.</p> <p>If the application you need does not have a pre-built version compatible with the HBSGrid system or you are having issues installing your compatible software, reach out to research@hbs.edu.</p> <p>Note: Please do not use sudo, apt-get, dnf, or rpm programs, as these require elevated privileges, which are reserved for the cluster administrators and not mere mortals such as ourselves.</p>"},{"location":"storage/","title":"Research Data Storage","text":""},{"location":"storage/#data-storage-on-the-hbsgrid","title":"Data Storage on the HBSGrid","text":"<p>HBS provides storage for active research projects. Research storage is  typically used in conjunction with the computer cluster (HBSGrid) and accessed  via NoMachine remote desktop as documented in the Userguide.</p> <p>There are three sets of storage locations available for research work, depending on the particular usage patterns, size consideration, and number of people involved in the work. These are the home folders,\u00a0scratch folders, and project spaces.</p> Folder Type Size Expandable? Shareable? Backed up? Other Considerations Home 150GB (100GB for guests)* No No Yes This is a personal folder with size limitations that cannot be shared with others. Scratch Varies Yes Yes No Files older than 60 days are deleted and files are not backed up. This is SSD storage that is faster than other storage options. Project Default is 50GB Yes Yes Yes These folders are meant for collaboration and/or projects that may increase in size over time. <p>*Default sizes for home folder have grown over time. If you received your HBSGrid account prior to 2024, you may have a smaller home directory  </p>"},{"location":"storage/#home-folders","title":"Home folders","text":"<p>By default, when your account is created (whether this account is used for just storage or storage + computing), a home folder is created. Home folders are 150GB (or 100GB for guests) and cannot be expanded (note: default sizes for home folder have grown over time. If you received your HBSGrid account prior to 2024, you may have a smaller home directory).</p> <p>If you are logging in to the HBSGrid\u00a0to do work via the NoMachine GUI or terminal, you are automatically placed in and are using this home folder. This is located at:</p> <p><code>/export/home/&lt;group&gt;/&lt;username&gt;</code></p> <p>For example faculty member John Harvard's home folder would be at:</p> <p><code>/export/home/faculty/jharvard</code></p> <p>When space in your home folder fills up, you will not be able to do any more work, which may lead to programs acting strangely or crashing altogether, disk error notices, or input/output errors. Keep an eye on your space usage and  periodically remove any old files that are no longer needed. </p> <p>Home folders are backed up every night. If you should need to recover any files due to accidental deletion or corruption, please contact\u00a0RCS.</p>"},{"location":"storage/#home-folder-quota","title":"Disk Quota Exceeded Error","text":"WARNING Reaching your home folder limit can prevent NoMachine sessions from starting, and this is one of the most common reasons for difficulties connecting to the HBS Grid desktop via NoMachine. <p>You can fix this problem yourself in a few ways:</p> <ol> <li> <p>By Terminal: Open a terminal (in the Windows search toolbar, type \"Cmd\" or \"Windows PowerShell\"; in the Mac search toolbar, type \"Terminal\") and run        <code>ssh &lt;username&gt;@hbsgrid.hbs.edu</code>     (replace <code>&lt;username&gt;</code> with your actual HBS Grid username). Once connected you can use terminal commands like <code>ls</code> to list files in the directory, <code>rm</code> (remove) plus the name of the file to remove files you don't need,     or <code>mv</code> (move) plus the name of your file and a path to a new location to move files. Removing and moving files can help get your home directory back under your storage quota. You can also run <code>gio trash --empty</code> to empty the trash, which may give you enough breathing room to permit NoMachine login.</p> </li> <li> <p>By GUI: Use a GUI SFTP client like FileZilla or CyberDuck to log into your storage and clean up your home directory by moving/deleting files and emptying your trash folder.  </p> </li> </ol>"},{"location":"storage/#scratch-storage","title":"Scratch storage","text":"<p>Whether you are doing batch or interactive work, at times you may need a temporary location to stash files that you will not keep, or your software may require a 'working', or temporary, directory. In most cases, you should not use your home folder, as there are quota limits, and the temporary needs may exceed the amount available.</p> <p>The storage location at\u00a0<code>/export/scratch</code> is designed specifically for this purpose. See the next section for setting up and using a temporary folder on this volume.</p> <p>Nota bene:</p> <ul> <li>This volume is a shared, community area. Be mindful of your usage (how much you use and for how long).</li> <li>This filesystem is not backed up! Ensure you copy anything important back to your home or project folder.</li> <li>Files older than 60 days will be deleted nightly via disk cleanup scripts. Do not interfere with these scripts.</li> </ul> <p>Please see our RCS Policies page for more information about our scratch usage policies. </p>"},{"location":"storage/#using-scratch","title":"Using <code>/export/scratch</code> effectively","text":"<p>As this volume is a shared area visible by everyone, it is important that you follow best practices for its use:</p> <ul> <li> <p>Do not store any files in the top-level.</p> </li> <li> <p>Use the Create Scratch Folder script to create your temporary folder. This will open a terminal /      text window that will guide you through the process of naming your folder and setting the correct permissions.     For folder naming, we recommend prefixing the folder with either your account name or the project name. To start:</p> <ul> <li> <p>In the NoMachine GUI, select from the menubar Applications &gt; Other &gt; Create Scratch Directory.</p> </li> <li> <p>In terminal, execute the command <code>/usr/local/app/scripts/create_scratch_folder.sh</code></p> </li> </ul> </li> <li> <p>Clean up files when you no longer need them, so that others can use the additional space immediately. Please note that this includes undeleted trash files on scratch. Although your trash may appear empty, 'trashed' files may still exist in a hidden folder and take up space. </p> </li> </ul> <p>If you should need to keep files on the scratch volume for longer than 60 days, please contact RCS.  </p>"},{"location":"storage/#undeleted-files-scratch","title":"Undeleted Trash files on Scratch","text":"<p>Although your Trash may appear empty,  'trashed' files may still exist in a hidden folder on scratch. To check whether this is the case, run <code>ls -al /export/scratch</code> and see whether a .Trash folder with your username is listed.</p> <p>We recommend that you review and 'delete permanently' the files in your NoMachine/Gnome Trash. If you are still uncertain or your Trash folder is empty, you can delete your \"trash\" folder on <code>/export/scratch</code> with one of the following three terminal commands:</p> <ul> <li><code>rm -ri /export/scratch/.Trash-$(id -u $USER)</code>    # to be prompted for every file</li> <li><code>rm -rI /export/scratch/.Trash-$(id -u $USER)</code>    # to be prompted, but less intrusive</li> <li><code>rm -rf /export/scratch/.Trash-$(id -u $USER)</code>    # NO PROMPT</li> </ul>"},{"location":"storage/#project-spaces","title":"Project spaces","text":"<p>Project spaces (folders) are the primary, recommended location for storing and doing collaborative work on research storage, including HBS and guest user accounts. This is in contrast to home folders, which are accessible only by the account holder. </p> <p>The default size is 50GB, with increases granted upon request and with space available. Keep an eye on the space usage, as project spaces that reach capacity will throw errors in programs and when transferring files, and data loss may result.</p> <p>Project spaces are backed up every evening. If you should need to recover any files due to accidental deletion or corruption, please contact\u00a0RCS.    </p>"},{"location":"storage/#requesting-a-project-space","title":"Requesting a project space","text":"<p>Note to doctoral student sponsors</p> <p>Access to project spaces that belong to doctoral students is limited to internal HBS users only (i.e., users with @hbs.edu credentials.)  Doctoral students who are working with external collaborators  (which may include graduate students from other Harvard schools) should contact  the Doctoral Programs office and RCS to discuss their options.</p> <p>Project spaces must be sponsored by an HBS faculty member or doctoral student. To request a project space, the prospective sponsor must fill out and submit the\u00a0New Project Space Request Form. (Please note that this form is only accessible to users who are eligible to sponsor a project space.)</p> <p>If you are working with level 3 data or higher, we will ask you to submit documentation from the IRB or a Data Usage Agreement (DUA; a sample can be found here) so that we can ensure that the project space's security levels are appropriately set up. If you are unsure what level your data falls under, please review this page on data security.</p> <p>If a project space already exists and you would like to make changes to the space users or size, fill out the\u00a0Project Space Change Request Form.</p>"},{"location":"storage/#archiving-a-project-space","title":"Archiving a project space","text":"<p>Project space usage is reviewed and confirmed on a yearly basis. Inactive project spaces will be backed up and archived only after contacting the primary sponsor. Archives are retained for the period of time specified by HU or HBS data retention policies, whichever is longest. If you would like to unarchive a project space, contact RCS.</p> <p>For information about how to archive your data to an external drive, see our technical note on archiving your research files.</p>"},{"location":"storage/#sql-databases","title":"SQL Databases","text":"<p>RCS hosts a database server running MariaDB (an open source fork of MySQL) to help meet growing data storage needs. To obtain a database account, please fill out the New MariaDB Account Request Form. Please note that RCS provides limited support for databases.</p>"},{"location":"storage/#connecting-to-your-database","title":"Connecting to your Database","text":"<p>Please contact us at research@hbs.edu for connection parameters including USER, HOSTNAME, and CA Certificates. Note that some clients  (including DBeaver) require you to set the SSL CA Certificate. Connections from outside the HBS Grid always require an RCS-provided SSL CA Certificate.</p>"},{"location":"storage/#configuration-database","title":"Configuration","text":"<p>Most MySQL clients will read connection information from a configuration file found in <code>~/.my.cnf</code>. This file is used to store connection details to the MariaDB server, such as account details and connection parameters. If you don't have this file (on the Grid) you may request a template or create your own with the following details:</p> <p>Important</p> <p>A configuration file with all connection details enables database access by anyone with access to the file. Ensure that this file is not shared or accessible by anyone but you.</p> <pre><code>[client]\nhost=HOSTNAME\nport=3306\nssl-ca=PATH_TO_SSL_CERT\ndatabase=jharvard\nuser=jharvard\npassword=PASSWORD\n</code></pre> <p>Please note that you will substitute jharvard and PASSWORD with your MariaDB username and password. For the other parameters such as HOSTNAME, please contact us at research@hbs.edu.</p> <p>Once your .my.cnf is ready, move it your home directory and  adjust file privileges to ensure no one else can read the file. On the Grid or on a Linux or Mac local machine, you may prevent others from reading the file by opening a Terminal and running this command:</p> <p><code>chmod 700 ~/.my.cnf</code></p> <p>On a Windows machine, right-click on the file, select Properties, and then:</p> <ol> <li>Under the General tab, toggle ON the Hidden attribute</li> <li>Under the Security tab, ensure only the following users or groups have access: System, Administrators, and you.</li> </ol>"},{"location":"storage/#tools-database","title":"Tools to Connect to your Database","text":"<p>You can connect to your database using any compatible client. If you  already have one you like, feel free to use that. Otherwise we recommend one of the clients listed below. </p>  WARNING If you are connecting to MariaDB on the cluster, please be advised that there is a known bug in the current environment. If you'd like to connect to the database using software on the cluster (e.g., R, Python), please select the <code>rcs/rcs_2022.11</code> software environment version in the interactive launcher or in Terminal. If you'd like to use DBeaver, launch a Terminal session, load the the previous environment (<code>ml load rcs/rcs_2022.11</code>), and then once loaded launch DBeaver by typing <code>dbeaver</code> in the Terminal. PythonRDesktop and DBeavermycli and TerminalODBC <p>Use connector-python to connect following the  official documentation. It is recommended to use connection settings from <code>~/.my.cnf</code> as described above.</p> <p>Use RMariaDB or dbplyr, both use connection settings from <code>~/.my.cnf</code> as described above. Sample code for connecting R to the Grid's MariaDB database can be found here.</p> <p>Use DBeaver to connect following the official documentation.  Make sure to set the CA Certificate path in the SSL connection settings tab. The DBeaver client may prompt you to download a driver to connect to your database. This is usually safe, and drivers will be stored in your home directory,     under <code>~/.local/share/DBeaverData/drivers</code></p> <p>The mycli client uses connection settings from <code>~/.my.cnf</code> as described above. You can download mycli onto your local machine, or use the instance already installed on the Grid. To connect from your local machine type: <code>mysql -u username -p -h mariadbhost --ssl-ca=/path/to/file/ca-cert.pem</code> or from the Grid: <code>mysql -u username -p -h mariadbhost</code>. </p> <p>Alternatively, you can access MariaDB from Terminal after first logging into the Grid (see instructions on the Start Here page and then typing: <code>mysql -h hostname -u username -p</code>. </p> <p>For all of the commands referenced above, replace the <code>username</code> with your username, the mariadbhost with the HBS host name in your <code>.my.cnf</code> file, and (if applicable) the <code>ssl-ca path</code> with the appropriate path. You will be prompted to provide your password. </p> <p>Setting up an ODBC connection differs by whether you are connecting from the Grid (Linux), Mac, or Windows machine. Please see MariaDB's ODBC set-up documentation for more information and note that if connecting from the Grid, the ODBC administration tool and appropriate ODBC drivers are already installed. You will still need to ensure that you have a <code>.odbc.ini</code> configuration file saved in your home directory. An example <code>.odbc.ini</code> template is below</p> <pre><code>[ODBC Data Sources]\nrcs_mariadb = RCS_MariaDB_ODBC_connection\n\n[rcs_mariadb]\ndriver = mariaDB\nserver = HOSTNAME\nport = 3306\ndatabase = jharvard\nuser = jharvard\npassword = PASSWORD\nsslmode = required\nsslca = PATH_TO_SSL_CERT\n</code></pre> <p>Once your <code>.odbc.ini</code> is ready, save it to your home directory and adjust file privileges to ensure no one else can read the file:</p> <p><code>chmod 700 ~/.odbc.ini</code></p>"},{"location":"storage/#importing-database","title":"Importing Data","text":"<p>The following is a basic overview of the import process. Complete documentation for the <code>LOAD DATA</code> command can be found at https://mariadb.com/kb/en/library/load-data-infile/.</p> <ol> <li>Move your data to an import folder</li> <li>Create the database table that will hold the imported data</li> <li>Import the data</li> <li>Validate the import</li> <li>Remove your data from the import folder</li> </ol> <p>To create the database table (step 2), you will need to know the name of all columns, each column\u2019s data type (integer, numeric with decimals, string of characters, etc), and each column\u2019s maximum width. For example, if one of the columns in your data is US phone numbers of the format 6174953292, then you may opt to use int(10) as the column data type (i.e., integer with up to 10 digits). However, if you suspect some entries have dashes such as 617-495-3292, then you will need to use char(12) which stores the data as a string of characters, up to 12 characters in length.</p>"},{"location":"storage/#import-example","title":"Import Example","text":"<p>We will use the following to illustrate importing: MariaDB username = jharvard MariaDB database = jharvard_database MariaDB database table = table_import Import filename = SampleData.txt  </p> <p>Data:  </p> COLUMN_1 COLUMN_2 COLUMN_3 25 Harvard Way 86 Brattle St"},{"location":"storage/#1-move-your-data-to-the-appropriate-import-folder","title":"1. Move your data to the appropriate import folder","text":"<p>Prepare an import folder and ensure it has the appropriate permissions:  <code>mkdir /export/mdb_external/import/jharvard</code> <code>chmod 700 /export/mdb_external/import/jharvard</code> </p> <p>Move your data to this import folder: <code>mv SampleData.txt /export/mdb_external/import/jharvard</code> </p>"},{"location":"storage/#2-create-the-database-table-that-will-hold-the-imported-data","title":"2. Create the database table that will hold the imported data","text":"<p>Log into MariaDB: <code>mysql -h HOSTNAME -u jharvard -p</code> </p> <p>Create a database table that will hold imported data, specifying the maximum size of each column. For example, below we specify that all columns are char with a maximum length of 20. You can modify your table at a later time, e.g., switching from  char(20) to char(30): <code>use jharvard_database; create table table_import (Column_1 char(20), Column_2 char(20), Column_3 char(20));</code> </p>"},{"location":"storage/#3-import-the-data","title":"3. Import the data:","text":"<p><code>load data local infile \u2018/export/mdb_external/import/jharvard/SampleData.txt\u2019 into table table_import fields terminated by \u2018|\u2019 lines terminated by \u2018\\n\u2019 ignore 1 lines;</code></p> <p>The command above has 4 sections: (1) <code>load data local infile \u2018/export/mdb_external/import/jharvard/SampleData.txt\u2019</code> specify file to import (2) <code>into table table_import</code> specify table that will hold the imported data (3) <code>fields terminated by \u2018|\u2019 lines terminated by \u2018\\n\u2019</code> specify delimiters (the character which splits data or text into separate fields). More information about delimiters can be found here: https://mariadb.com/kb/en/delimiters/.   (4) <code>ignore 1 lines</code> include this only if your file includes column header information  </p> <p>Official documentation for this command may be found at https://mariadb.com/kb/en/mariadb/load-data-infile/.  </p>"},{"location":"storage/#4-validate-the-import","title":"4. Validate the import","text":"<p>Do a preliminary check on the first 10 rows of our data: <code>select * from table_import limit 10;</code> </p> <p>Log out of MariaDB: <code>exit;</code> </p>"},{"location":"storage/#5-remove-your-data-from-the-import-folder","title":"5. Remove your data from the import folder","text":"<p><code>rm -rf /export/mdb_external/import/jharvard</code> </p>"},{"location":"storage/#exporting-database","title":"Exporting Data","text":"<p>The following is a basic overview of the export process followed by an example. Complete documentation for the <code>SELECT \u2026 INTO FILE</code> command to be run within MariaDB may be found at https://mariadb.com/kb/en/library/select-into-outfile/.</p> <ol> <li>Prepare export folder such that it is accessible to everyone</li> <li>Export data</li> <li>Change access rules to export folder</li> <li>Copy your data out of the export folder</li> <li>Remove export folder</li> </ol> <p>Please note that you will be copying your data to a new file, not moving it. This ensures you are the owner of the file, as opposed to the MariaDB server. Additionally note that your exported data will NOT include column headers/names!</p>"},{"location":"storage/#export-example","title":"Export Example","text":"<p>We will use the following to illustrate importing: MariaDB username = jharvard MariaDB database = jharvard_database MariaDB database table = table_import Import filename = SampleData.txt  </p> <p>Data:</p> COLUMN_1 COLUMN_2 COLUMN_3 25 Harvard Way 86 Brattle St"},{"location":"storage/#1-prepare-export-folder-such-that-it-is-accessible-to-everyone","title":"1. Prepare export folder such that it is accessible to everyone","text":"<p>Prepare an export folder and ensure it has the appropriate permissions: <code>mkdir /export/mdb_external/export/jharvard</code> <code>chmod 777 /export/mdb_external/export/jharvard</code> </p>"},{"location":"storage/#2-export-data","title":"2. Export data","text":"<p>Log into MariaDB: <code>mysql -h HOSTNAME -u jharvard -p</code> </p> <p>Within MariaDB, export the data: <code>select * from table_import into outfile \u2018/export/mdb_external/export/jharvard/my_export.dat\u2019 fields terminated by \u2018|\u2019 lines terminated by \u2018\\n\u2019;</code> </p> <p>Please note our command has 3 sections: (1) <code>select * from table_import</code> specify what data you want export (2) <code>into outfile \u2018/export/mdb_external/export/jharvard/my_export.dat\u2019</code> specify the export file (3) <code>fields terminated by \u2018|\u2019lines terminated by \u2018\\n\u2019</code> specify delimiters (the character which splits data or text into separate fields). More information about delimiters can be found here: https://mariadb.com/kb/en/delimiters/.      </p> <p>Official documentation for this command may be found at https://mariadb.com/kb/en/mariadb/select-into-outfile.  </p> <p>Log out of MariaDB: <code>exit;</code> </p>"},{"location":"storage/#3-change-access-rules-to-export-folder","title":"3. Change access rules to export folder","text":"<p><code>chmod 700 /export/mdb_external/export/jharvard</code> </p>"},{"location":"storage/#4-copy-your-data-out-of-the-export-folder","title":"4. Copy your data out of the export folder","text":"<p>Copy (<code>cp</code>), don't move, the data to ensure you have ownership as opposed to the MariaDB server: </p> <p>Option 1: Copy data to your home dir: <code>cp /export/mdb_external/export/jharvard/my_export.dat ~/</code> </p> <p>Option 2: Copy data to your project space: <code>project_space cp /export/mdb_external/export/jharvard/my_export.dat /export/projects/project_space</code> </p>"},{"location":"storage/#5-remove-export-folder","title":"5. Remove export folder","text":"<p><code>rm -rf /export/mdb_external/import/jharvard</code> </p>"},{"location":"storage/#additional-mariadb-resources","title":"Additional MariaDB Resources","text":"<ul> <li>MariaDB Documentation</li> <li>Getting Started with MariaDB</li> </ul>"},{"location":"syncfiles/","title":"\ud83d\udd04 Copy and Transfer Files","text":"<p>The HBS Grid is primarily used for data analysis, machine learning, data wrangling, and data visualization. Usually this means that  you need to copy or sync your data to the HBS Grid in order to do your work.</p>"},{"location":"syncfiles/#hbs-grid-storage-overview","title":"HBS Grid storage overview","text":"<p>Before transferring data to the HBS Grid you have to decide where to put it.  There are three options: home directory, project space, or scratch storage.</p> <p></p> <p>HBS Grid storage overview</p> <p>A home directory was created at <code>/export/home/&lt;group&gt;/&lt;username&gt;</code>  when you requested your account. Your home folder has limited storage  capacity and is accessible only by you.</p> <p>Project spaces are directories that are shared and accessible by all HBS Grid users working on that project. You can request a new project space using the new project space request form and you can request modifications to an existing project space using the change request form.</p> <p>Scratch storage is available at <code>/export/scratch</code>. It is appropriate only for temporary,  short-term storage. Files are not backed up and will be deleted after 60 days. Scratch storage is a shared resource accessible to all users on the HBS Grid; make sure you set permissions on your files accordingly.</p> <p>Refer to the Research Data Storage and Databases documentation for details.</p>"},{"location":"syncfiles/#transfer-data-fromto-local-storage","title":"Local storage data transfer","text":""},{"location":"syncfiles/#sftp","title":"SFTP","text":"<p>Transferring data from your local computer to the HBS Grid is usually done using the SFTP protocol.  This requires an SFTP client on your local machine. If you don't yet have one Cyberduck and Filezilla  are popular graphical desktop clients. For command-line data from a terminal we recommend rsync.</p> <p>Once you have an SFTP client installed on your local machine, follow these steps to transfer data to or from the HBS Grid.</p> <p>Transfer data the HBS Grid using a desktop SFTP client</p> <ol> <li> <p>Connect to the HBS network, either directly if you are on-campus or    connect via VPN    otherwise. </p> </li> <li> <p>Open your transfer client and connect to the HBS Grid at <code>hbsgrid.hbs.edu</code> on port 22</p> </li> <li> <p>Locate the data on your local machine that you wish to transfer.</p> </li> <li> <p>Locate the directory on the HBS Grid that you will copy your data     too, creating it if needed.</p> </li> <li> <p>Start the data transfer</p> </li> </ol> <p>Transfer from the command line</p> <p>You can alternatively use the <code>rsync</code> command-line program to transfer data from the command line on Mac, Linux, or Windows subsystem for Linux.  <code>rsync</code> documentation is available online.</p> <p>Note that transferring many small files is much slower than transferring a small number of large files. You may find it faster to compress folders with many small files into <code>.zip</code> or <code>.tar</code> archives, transfer those,  and decompress/extract them on the other end.  (See additional data transfer tips below.)</p> <p>Click the image below for a demonstration showing how to sync your data from a local drive to the HBS Grid:</p>  Your browser does not support the video tag."},{"location":"syncfiles/#mount-storage-locally","title":"Mount Storage Locally","text":"<p>Research storage is also accessible on Windows as a network drive  at <code>\\\\research.hbs.edu</code>,  via SMB on OSX/Linux  at <code>smb://research.hbs.edu</code>, and via SSH at <code>hbsgrid.hbs.edu</code>. This is useful for viewing and copying small files, but will be slow for large data transfers  and may result in unexpected permissions settings on the cluster.</p> Map a Drive on WindowsMount a Volume on Mac <ol> <li> <p>Connect to the HBS network, either directly if you are on-campus or via VPN otherwise.</p> </li> <li> <p>Open a Windows Explorer window, right-click on the \"Computer\" icon, and then select \"Map Network Drive\". </p> </li> <li> <p>To map a drive to your home directory, specify the folder path  <code>\\\\research\\username</code> (for example, <code>\\\\research\\jharvard</code>). To map a drive to a project space, specify the path <code>\\\\research.hbs.edu\\projects\\projectname</code> (note that you may have to use projects, projects2, projects3, projects4, or projects5 depending on the path of your project space). Also note that you may not map a drive to a project space containing security level 4 data.</p> </li> <li> <p>Click \"Connect using different credentials\" if you are not using an HBS-issued machine. If you are prompted for your username and password, enter your HBSGrid username (the part preceding @hbs.edu) and your password. If you are connecting from a non-HBS-issued machine, please add HBS\\ before your username (e.g. HBS\\jharvard). This specifies the proper Windows domain for authenticating your credentials.</p> </li> </ol> <ol> <li> <p>Connect to the HBS network, either directly if you are on-campus or via VPN otherwise.</p> </li> <li> <p>From the Finder menu bar, select Go &gt; Connect to Server...</p> </li> <li> <p>In the Server Address field, enter the domain\\username, server address, and file path combination that is appropriate for your the space you're trying to access. For your home directory, this will be <code>smb://HBS\\jharvard@research.hbs.edu/jharvard</code>, and for project spaces, this will be <code>smb://HBS\\jharvard@research.hbs.edu/projects/projectname</code> (note that you may have to use projects, projects2, projects3, projects4, or projects5 depending on the path of your project space). In both cases, use your own HBS username in place of \"jharvard.\" Also note that you may not mount a project space containing security level 4 data.</p> </li> </ol>"},{"location":"syncfiles/#transfer-data-fromto-cloud-storage","title":"Cloud storage data transfer","text":"<p>Sync from the command-line</p> <p><code>rclone</code> is also available as a command-line application that you can use interactively in a terminal or in scripts. Refer to the rclone documentation for details.</p> <p>If your data is in cloud storage (OneDrive, Dropbox etc.) you may wish to sync it directly from there. While the HBS Grid does not offer native Dropbox, OneDrive, or other cloud storage clients, you can use <code>rclone</code> to perform on-demand  data synchronization with all major cloud storage providers. Transferring data from cloud storage providers to the HBS Grid using this tool is generally reasonably fast and easy.</p> <p>Sync your data from a cloud provider to the HBS Grid desktop</p> <ol> <li> <p>Log in to the HBS Grid via NoMachine.</p> </li> <li> <p>Identify the directory on the HBS Grid that you will copy your data     to, creating it if needed.</p> </li> <li> <p>From the HBS Grid desktop, open the rclone browser application.</p> </li> <li> <p>Click the Config... button and follow the prompts (only needed the first time).</p> </li> <li> <p>Click the cloud storage icon in the Remotes tab and select the directory you wish to sync.</p> </li> <li> <p>Specify the target directory from step 2 in the destination field.</p> </li> </ol> <p>Click the image below for a quick demonstration showing how to copy files from Dropbox to the HBS Grid.</p>  Your browser does not support the video tag.  <p>Note that the demonstration video goes through the configuration step, which only needs to be done once. After that you can skip step 4 above, which greatly simplifies the process.</p>"},{"location":"syncfiles/#transfer-data-using-globus","title":"Globus data transfer","text":"<p>Globus is a data transfer service that enables sharing files or data with external persons, eliminates the need for both parties to have HBS or guest user credentials, and is capable of tolerating transfer interruptions. Note: At this time, Globus should not be used to transfer DSL 4 research data.</p> <p>Globus key concepts</p> <p>A Globus Collection is a named location containing data you can access with Globus.</p> <p>HBS maintains a Globus Collection named Harvard Business School DTN. Storage for this collection is mounted on the HBS Grid at <code>/export/globus</code>. You can create folders there and share them with other Globus users, or  transfer data to them from other Globus collections you have access to. As with scratch storage, <code>/export/globus</code> is accessible by all grid users, so you must  ensure that both the ownership and permissions are set appropriately.</p> <p>Please be aware that you can share out files or folders not only that you own but also that you have explicit permission to access. Please be careful not to share too broadly.</p> <p>Log in to Globus and transfer data</p> <ol> <li> <p>Login to the Globus web interface,      selecting \"Harvard University\" as your organization and authenticating     via HarvardKey if prompted.</p> </li> <li> <p>In the Collection field, search for and select the Harvard Business School DTN (data transfer node). If you have not already created a folder here, create one now by clicking the New Folder button.</p> </li> <li> <p>Click on the \"Transfer or Sync to...\" button.</p> </li> <li> <p>Use the second Collection search box on the right to find the other collection.</p> </li> <li> <p>Select the folder you wish to transfer and click on the appropriate Start arrow icon.     (The direction the Start button arrow points indicates the direction files will transfer.)</p> </li> </ol> <p>After transferring your data to the HBS Grid via Globus you will typically move it from  <code>/export/globus</code> to a project folder. This can be done on the HBS Grid NoMachine  desktop using Grsync or from the HBS Grid command line using <code>rsync</code>, <code>mv</code>, or similar commands. You must move or copy any necessary files from /export/globus within 30 days. After 30 days, your files will be deleted from the Globus volume. If you anticipate that you will need to keep files in /export/globus longer than 30 days in order to complete a data transfer, please contact RCS.</p> <p>Globus can also be used to transfer data from you local machine using Globus Connect Personal. Installers are available for Mac OS X. Windows and Linux.</p> <p>For details on using Globus refer to the excellent Globus documentation. A FAQ section is also available.</p>"},{"location":"syncfiles/#important-data-transfer-considerations","title":"Data transfer tips","text":"<p>The speed and success of data transfer rely on numerous factors, only some of which are in your control. Following these listed tips will help increase the likelihood of success:</p> <ul> <li> <p>Wired, ethernet connections tend to be more reliable and faster than      wireless internet connections.If using WiFi, try to use a network that      has a strong signal and is interference-free.</p> </li> <li> <p>Transferring a few large files is often much faster than transferring many small files.     When transferring many small files we recommend that you compress/archive the     directories up into a small number of archives. On the HBSGrid cluster, we     recommend the GUI File Manager in NoMachine/Gnome or using     command-line programs like <code>tar</code> or <code>zip</code>.</p> </li> <li> <p>We do not recommend creating any single archive file larger than     about 100 GB in size: Many transfer programs don't support resuming partial transfers,      so If your transfer is interrupted it will start again and transfer the whole file.</p> </li> <li> <p>You can use LSF batch jobs to quickly compress large files and/or a large number of files.     If using Globus, you can create archives directly in your Globus collection folder,      eliminating the need to copy the data twice.</p> </li> </ul>"},{"location":"trouble/","title":"Troubleshooting","text":""},{"location":"trouble/#troubleshooting-login-issues","title":"Troubleshooting Login Issues","text":"<p>Login issues are usually caused by one of three common problems, and are often easy to resolve.</p>"},{"location":"trouble/#network-and-vpn","title":"Network and VPN","text":"<p>Connecting to the HBS Grid requires either a direct on-campus connection to the HBS network or a  VPN connection  if you are connecting remotely. </p> <p>Ethernet connections from HBS offices or WiFi connections to HBS Secure  will both work without further configuration. Note that connecting from other Harvard  networks, such as Harvard Secure or an Ethernet  connection from another Harvard School  will not work; you must be connected to the HBS network.</p> <p>If you are connecting from outside the HBS network you must use a VPN connection. If you suspect the VPN is not connected properly try re-installing the VPN client and restarting your computer.</p>"},{"location":"trouble/#disk-quota","title":"Disk quota","text":"<p>A quota system is used to limit the amount of data you can store in your home directory on  the HBS Grid. Reaching this limit can prevent NoMachine sessions from starting, and this is  one of the most common reasons for difficulties connecting to the HBS Grid desktop via NoMachine. Please see our instructions to fix this problem.</p>"},{"location":"trouble/#nomachine-hanging-while-loading-session","title":"NoMachine Hanging While Loading Session","text":"<p>If you get stuck on a spinning loading wheel when trying to connect to a NoMachine session:</p> <ol> <li>Cancel the hanging connection attempt.</li> <li>Right click on the Virtual Desktop button and select \"Terminate session.\"</li> <li>Start a new NoMachine session.</li> </ol> <p>To avoid such issues and to prevent instability on the login nodes, we recommend that users  always log out of NoMachine completely when they are finished with their work.</p> <p>If you still cannot get a NoMachine session after following the above steps, please contact RCS.</p>"},{"location":"trouble/#shell-misconfiguration","title":"Shell misconfiguration","text":"<p>Some users like to configure the startup behavior of their login shell by editing the <code>~/.bashrc</code> or <code>~/.bash_profile</code> configuration files. A common problem is that activating conda, software modules or other environments in these config files can cause problems with NoMachine connections. </p> <p>If you suspect this has happend, you can fix this problem yourself by opening a terminal  (Cmd prompt or PowerShell on Windows) and running</p> <pre><code>ssh &lt;username&gt;@hbsgrid.hbs.edu\n</code></pre> <p>(replace <code>&lt;username&gt;</code> with your actual HBS Grid username). One connected you can use a  terminal-based editor  such as nano to comment out or remove sections of your config files that you suspect have caused the problem.  Alternative you can run</p> <pre><code>mv ~/.bashrc ~/backup.bashrc\nmv ~/.bash_profile ~/backup.bash_profile\n</code></pre> <p>to temporarily move your config files to backup locations.</p>"},{"location":"trouble/#troubleshooting-jobs-and-resources","title":"Troubleshooting LSF Jobs","text":"<p>A variety of problems can arise when running jobs and applications on the HBSGrid. LSF provides command-line tools to monitor and inspect your jobs to help you figure out if something goes wrong.</p> <p>Job troubleshooting steps</p> <p>Open a Terminal and the HBS Grid and run the commands below to troubleshoot jobs.</p> <ol> <li>Get the JOBID number by running <code>bjobs</code>    If your job is no longer running use <code>bhist -a</code>    to list all your recent jobs, whether finished or unfinished. The JOBID is the first number in the output`.</li> <li>Get detailed information about a specific job by running_    <code>bjobs -l &lt;JOBID&gt;</code>    where <code>&lt;JOBID&gt;</code> is the number you looked up in step 1. The -l flag asks for the output in a long format with more detailed information.</li> <li>You can also look at any output produced by your job by running <code>bpeek &lt;JOBID&gt;</code></li> <li>Older jobs may not appear in <code>bjobs</code>. In that case you can still get some    information by running <code>bhist -l &lt;JOBID&gt;</code> </li> </ol> <p>The <code>bjobs -l &lt;JOBID&gt;</code> command give you information about the state of the job, as defined below.</p> <p>Job state definitions</p> <code>PENDING</code> <p>Job is awaiting a slot suitable for the requested resources or you've gone over your limit on resource usage. Jobs with high resource demands may spend significant time PENDING if the compute grid is busy.</p> <code>RUNNING</code> <p>Job is running.</p> <code>COMPLETED</code> <p>Job has finished and the command(s) have returned successfully (i.e., exit code 0).</p> <code>CANCELLED</code> <p>Job has been terminated by the user or administrator using <code>bkill</code>.</p> <code>FAILED</code> <p>Job finished with an exit code other than 0.</p> <p>If your job has failed <code>bjobs</code> will usually tell you why, but these messages can be cryptic. The most common are described below.</p> Error Likely Cause <code>JOB &lt;jobid&gt; CANCELLED AT &lt;time&gt; DUE TO TIME LIMIT</code> You did not specify enough time in your submission script. The <code>-W</code> option sets time in minutes or can also take HH:MM form (12:30 for 12.5 hours) <code>Job &lt;jobid&gt; exceeded &lt;mem&gt; memory limit, being killed</code> Your job is attempting to use more memory than you've requested for it. Either increase the amount of memory requested or, if possible, reduce the amount your application is trying to use. For example, many Java programs set heap space using the <code>-Xmx</code> JVM option. This could potentially be reduced. <code>Exited with exit code N</code> Your job failed because your application exited with an error. Please look at the job or application logs to determine why your program exited abnormally. <p>For more detailed information refer to the official LSF documentation.</p>"},{"location":"trouble/#MaxMem","title":"Jobs Exceeding Memory Limit","text":"<p>Many times, a job that suddenly shuts down without an apparent error message has exceeded its memory limit (i.e., too little RAM was requested for the job). You can confirm this in a terminal window by using the <code>bhist -a -l</code> command to review all recent jobs (recall that the <code>-a</code> displays historical job information about both finished and unfinished jobs and the <code>-l</code> command asks for the output in a long format with more detailed information). Scroll down until you see the header MEMLIMIT. Just above this will be an error message indicating that the job was killed after reaching the memory limit. The MEMLIMIT will display how much memory you requested and the MEMORY USAGE will display the MAX and AVG memory used for the job. For example:</p> <pre><code>[jharvard@rhrcscli01:~]$ bhist -a \u2013l\n...\nTue Dec 23 16:34:13: Completed &lt;exit&gt;; TERM_MEMLIMIT: job killed after reaching\n                      LSF memory usage limit;\n\n\n MEMLIMIT\n     48 G \n\nMEMORY USAGE:\nMAX MEM: 48 Gbytes;  AVG MEM: 1.3 Gbytes\n</code></pre> <p>The MAX MEM values can inform how much RAM you would ask for next time you do similar work or work with the same data. If the amount was not sufficient for your job and the job exceeded the memory limit and was shut down, you know that you will need to ask for more memory the next time. This output can also tell you if you've asked for more RAM than was needed to complete the job.</p> <p>If you would like to avoid scrolling through the output of the <code>bhist</code> commands, you can write a longer customized statement so that only the MAX MEM and AVG MEM are output:</p> <p><pre><code>[jharvard@rhrcscli01:~]$ bhist -a -l | grep -E \"MAX\"\n\nMAX MEM: 29.5 Gbytes;  AVG MEM: 20.5 Gbytes\nMAX MEM: 373 Mbytes;  AVG MEM: 299 Mbytes\nMAX MEM: 50 Gbytes;  AVG MEM: 7.4 Gbytes\nMAX MEM: 46.4 Gbytes;  AVG MEM: 41.9 Gbytes\nMAX MEM: 1.1 Gbytes;  AVG MEM: 997 Mbytes\nMAX MEM: 42.2 Gbytes;  AVG MEM: 36.6 Gbytes\nMAX MEM: 915 Mbytes;  AVG MEM: 340 Mbytes\nMAX MEM: 471 Mbytes;  AVG MEM: 211 Mbytes\n</code></pre> You can further customize this output to focus on particular dates, or parse the output further if you were running many jobs. Below is an example of how you can narrow down the dates (using the <code>-S</code> submitted date command with a comma to indicate a date range up to today), and output when jobs were submitted, the job ID, the RAM (memory) requested, the maximum/average amount of RAM (memory) used, and the reason a job failed (if applicable):</p> <pre><code>[jharvard@rhrcscli01:~]$ bhist -a -l -S 2025/12/20, | awk '\n/^Job &lt;/ {match($0,/Job &lt;([0-9]+)/,m); jobid=m[1]; mem_req=\"MEM_REQ=NA\"; reason=\"\"}\n/Submitted from host/ {split($0,a,\":\"); submittime=a[1]\":\"a[2]\":\"a[3]}\n/MEMLIMIT/ {getline; gsub(/^[ \\t]+/,\"\"); mem_req=\"MEM_REQ=\" $0}\n/Completed &lt;exit&gt;;/ {s=index($0,\";\"); reason=substr($0,s+1); gsub(/^[ \\t]+/,\"\",reason)}\n/MAX MEM:/ {print submittime \"\\tJobID=\" jobid \"\\t\" mem_req \"\\t\" $0 (reason?\"\\t\"reason:\"\")}'\n\nMon Jan  5 10:15:09 JobID=3083242   MEM_REQ=30 G    MAX MEM: 30 Gbytes;  AVG MEM: 21.5 Gbytes\nMon Jan  5 12:22:32 JobID=3083263   MEM_REQ=30 G    MAX MEM: 30 Gbytes;  AVG MEM: 19.2 Gbytes\nMon Jan  5 12:37:43 JobID=3083266   MEM_REQ=30 G    MAX MEM: 30 Gbytes;  AVG MEM: 6.8 Gbytes\nMon Jan  5 12:59:31 JobID=3083390   MEM_REQ=40 G    MAX MEM: 33 Gbytes;  AVG MEM: 24.5 Gbytes\nMon Jan  5 16:53:45 JobID=3083455   MEM_REQ=35 G    MAX MEM: 35 Gbytes;  AVG MEM: 22.8 Gbytes\nMon Jan  5 17:38:21 JobID=3083461   MEM_REQ=40 G    MAX MEM: 33 Gbytes;  AVG MEM: 26.3 Gbytes\n</code></pre> <p>For more detailed information refer to the official LSF documentation about <code>bhist</code>.</p>"},{"location":"trouble/#stata-temporary-files-and-stata-tmp","title":"Stata Temporary Files and Temp Storage","text":"<p>If there is not enough disk space available in <code>/tmp</code> Stata may give you an error message that looks like this:</p> <pre><code>insufficient disk space\nr(699);\n</code></pre> <p>As a first step you may be able to change your Stata code to reduce the amount of temp space  needed -- <code>preserve</code> and <code>restore</code> commands are often the cause.</p> <p>You can also try deleting any files you have in <code>/tmp</code> and see if that gives you enough space. Since each computer in the cluster has it's own <code>/tmp</code> disk you need to do this on the computer  Stata is running on. An easy way to achieve that is to delete files directly from Stata using the shell escape feature. For example, running  <code>! rm /tmp/my-temp-file</code> in Stata will delete <code>/tmp/my-temp-file</code>.</p> <p>If you cannot get enough space on <code>/tmp</code> you can tell Stata to store temporary files in a  Scratch storage directory on the HBS Grid.</p> <p>Use scratch storage for Stata temp files</p> <ol> <li>Create a directory under <code>/export/scratch</code> and ensure that the     permissions are set correctly.</li> <li>Set the <code>STATATMP</code> environment variable to the directory you created    in step one. Use launcher options    if running from the destkop, or set this variable from the command line.</li> <li>Start Stata as usual after setting the <code>STATATMP</code> environment variable as described in steps 1-2 above.</li> </ol> <p>More details about this issue can be found in the Stata FAQ.</p>"},{"location":"tutorials/","title":"Tutorials by topic","text":"<p>This section contains longer tutorials and examples showing how you can carry out common tasks using the HBS Grid. Select a topic from the list or select a tutorial from the menu on the left.</p>"},{"location":"tutorials/#tag:7-zip","title":"7-zip","text":"<ul> <li>            Extracting Archived/Compressed Files          </li> </ul>"},{"location":"tutorials/#tag:database","title":"Database","text":"<ul> <li>            Database Delimiters          </li> </ul>"},{"location":"tutorials/#tag:delimiters","title":"Delimiters","text":"<ul> <li>            Database Delimiters          </li> </ul>"},{"location":"tutorials/#tag:gpu","title":"GPU","text":"<ul> <li>            Parallel processing          </li> </ul>"},{"location":"tutorials/#tag:large-data","title":"Large data","text":"<ul> <li>            Large data in R          </li> </ul>"},{"location":"tutorials/#tag:mariadb","title":"MariaDB","text":"<ul> <li>            Database Delimiters          </li> </ul>"},{"location":"tutorials/#tag:matlab","title":"Matlab","text":"<ul> <li>            Parallel processing          </li> </ul>"},{"location":"tutorials/#tag:nlp","title":"NLP","text":"<ul> <li>            NLP in Python          </li> </ul>"},{"location":"tutorials/#tag:nltk","title":"NLTK","text":"<ul> <li>            NLP in Python          </li> </ul>"},{"location":"tutorials/#tag:parallelization","title":"Parallelization","text":"<ul> <li>            Parallel processing          </li> </ul>"},{"location":"tutorials/#tag:python","title":"Python","text":"<ul> <li>            NLP in Python          </li> <li>            Parallel processing          </li> <li>            Web Scraping in Python          </li> </ul>"},{"location":"tutorials/#tag:r","title":"R","text":"<ul> <li>            Large data in R          </li> <li>            Parallel processing          </li> </ul>"},{"location":"tutorials/#tag:scraping","title":"Scraping","text":"<ul> <li>            Web Scraping in Python          </li> </ul>"},{"location":"tutorials/#tag:stata","title":"Stata","text":"<ul> <li>            Parallel processing          </li> </ul>"},{"location":"tutorials/#tag:gunzip","title":"gunzip","text":"<ul> <li>            Extracting Archived/Compressed Files          </li> </ul>"},{"location":"tutorials/#tag:uncompress","title":"uncompress","text":"<ul> <li>            Extracting Archived/Compressed Files          </li> </ul>"},{"location":"tutorials/#tag:unzip","title":"unzip","text":"<ul> <li>            Extracting Archived/Compressed Files          </li> </ul>"},{"location":"worksafe/","title":"\ud83d\udc65 Collaborate and Share","text":"<p>The HBS Grid is a multi-user environment shared by many people. Because of this you must take care to ensure that only authorized project members  can access your files.</p>"},{"location":"worksafe/#projects-and-group-membership","title":"Projects and group membership","text":"<p>For more information about storage options refer to the  file transfer documentation.</p> <p>For collaborative projects in which more than one person needs access, you must use a project space (request one  if needed). Each project has an associated group that includes the HBS Grid users who have access to that project space. Changing group membership must currently be done by a system administrator; use the change request form  to request a change.</p>"},{"location":"worksafe/#file-ownership-and-permissions","title":"File ownership and permissions","text":"<p>For project space files you will almost always want group members to have read and write permission. You can view and set permissions using the File Browser or from the command line using the Terminal.</p>"},{"location":"worksafe/#set-permissions-using-the-file-browser","title":"Set permissions using the file browser","text":"<p>Follow these steps to change file permissions using the Files application</p> <ol> <li>Open the Files application from the Applications menu or Activites search</li> <li>Locate the file or folder you wish to modify, right-click on it and select Properties</li> <li>Select the Permissions tab in file properties dialog</li> <li>If you wish to change permissions for all files in a directory, click the     Change Permissions for Enclosed Files button.</li> <li>Select appropriate access levels for Owner (you), Group, and Others.</li> </ol> <p>Click the image below to see these steps visually:</p>  Your browser does not support the video tag.  <p>Refer to the official GNOME documentation for details.</p>"},{"location":"worksafe/#set-ownership-and-permissions-using-the-command-line","title":"Set ownership and permissions using the command line","text":"<p>Ownership and permissions can alternatively be set from the command line using <code>chown</code> and <code>chmod</code>. For example </p> <p><pre><code>chmod -R g+rwx project1/data\n</code></pre> says \"Recursively for group members, add read, write  and execute permissions to <code>project1/data</code> and everything in it\".  Refer to <code>tldr chmod</code> for more permissions examples and to <code>man chmod</code> for details.</p> <p>Group ownership can be set from the command line using <code>chgrp</code>. For example opening the Terminal application and running <pre><code>chgrp -R my_project_group project1/data\n</code></pre> says \"Recursively make my_project_group the group owner of <code>project1/data</code> and everything in it\".  Refer to <code>tldr chgrp</code> for more examples and to <code>man</code> chgrp for details.</p>"},{"location":"worksafe/#avoid-running-services-other-users-can-access","title":"Avoid running services other users can access","text":"<p>Some applications are designed to run as local servers that you connect to using a web browser or other client. On a single-user machine that may be relatively safe, but in a multi-user environment you need to take extra care to ensure that you don't start services that other users on the HBS Grid can connect to. </p> <p>For example, running an unprotected Jupyter notebook can give other users the ability to connect to your service and execute arbitrary commands as you! Fortunately jupyter notebooks are token protected by default, and you can password protect them if you wish. The key thing is that you must be aware of any services you are running and you must understand how those services are protected against unwanted access by other users on the HBS Grid. The simple rule is if you don't know if or how a service is protected, don't use it!</p>"},{"location":"tutorials/DatabaseDelimiters/","title":"Database Delimiters","text":"","tags":["Database","MariaDB","Delimiters"]},{"location":"tutorials/DatabaseDelimiters/#a-word-of-caution","title":"A word of caution!","text":"<p>The field delimiter is the character which splits data or text into separate fields. A field is a column within your database. As an example, we may have the dataset below:</p> Column_1 Column_2 Column_3 25 Harvard Way Boston, MA <p>When we export this data to a text file, it would appear as:</p> <p><code>Line 1: Column_1 DELIMITER Column_2 DELIMITER Column_3</code></p> <p><code>Line 2: 25 DELIMITER Harvard Way DELIMITER Boston, MA</code></p> <p>Often, the comma is used as the field delimiter. In this example, using a comma would cause Line 2 to appear to have 4 fields instead of 3!</p> <p><code>Line 1: Column_1,Column_2,Column_3</code></p> <p><code>Line 2: 25,Harvard Way,Boston, MA</code></p> Column_1 Column_2 Column_3 25 Harvard Way Boston MA <p>When exporting data, you should not use a field delimiter that may also occur within your data. Commas and tabs are common field delimiters but are likely to occur when storing text strings. Your options are to either change your field delimiter or have MariaDB enclose the data. We recommend you use the pipe character, |, as your field delimiter. Otherwise you may have MariaDB enclose the data by adding OPTIONALLY ENCLOSED BY \u2026 as described here (https://mariadb.com/kb/en/mariadb/select-into-outfile/\"&gt;https://mariadb.com/kb/en/mariadb/select-into-outfile/).</p> <p>When importing data, you must specify the field delimiter that had been used. You should verify with the dataset author as to the field delimiter, and whether any fields were enclosed by another character. Aside from the field delimiter, you will also have to specify the line delimiter. In most cases, the line delimiter is \\n. You may also try using \\r.</p>","tags":["Database","MariaDB","Delimiters"]},{"location":"tutorials/DatabaseDelimiters/#using-excel-with-nonstandard-delimiters-such-as-the-pipe-character","title":"Using Excel with NonStandard Delimiters such as the Pipe Character","text":"<p>Many users will use Excel to view their data. If you decide to set the pipe character, |, as your field delimiter, then Excel will not automatically parse your data into separate columns. As an example, we can enter the above data into a text editor. </p> <p><code>Line 1: Column_1|Column_2|Column_3</code></p> <p><code>Line 2: 25|Harvard Way|Boston, MA</code></p> <p></p> <p>Now we will open the file with Excel. As seen below, each row is within one column. To have Excel parse the data into appropriate columns, select the entire column by click on the column header A.</p> <p></p> <p>Within the DATA tab at the top, click on Text to Columns</p> <p></p> <p>In the new window, Convert Text to Columns Wizard, on Step 1 you should select Delimited. Next, on Step 2, you would select only Other and enter the pipe character, |, into the accompanying box.</p> <p></p> <p>Once you are complete the remaining step of the Convert Text to Columns Wizard, you should find your data separated into appropriate columns.</p> <p></p>","tags":["Database","MariaDB","Delimiters"]},{"location":"tutorials/PythonWebScrape/","title":"Python Web-Scraping","text":"<p>This is an intermediate / advanced Python tutorial:</p> <ul> <li>Assumes knowledge of Python, including:<ul> <li>lists</li> <li>dictionaries</li> <li>logical indexing</li> <li>iteration with for-loops</li> </ul> </li> <li>Assumes basic knowledge of web page structure</li> </ul> <p>If you need an introduction to Python or a refresher, we recommend our Python Introduction.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#goals","title":"Goals","text":"This workshop is organized into two main parts:  1.  Retrive information in JSON format 2.  Parse HTML files  Note that this workshop will not teach you everything you need to know in order to retrieve data from any web service you might wish to scrape.","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#web-scraping-background","title":"Web scraping background","text":"","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#what-is-web-scraping","title":"What is web scraping?","text":"<p>Web scraping is the activity of automating retrieval of information from a web service designed for human interaction.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#is-web-scraping-legal-is-it-ethical","title":"Is web scraping legal? Is it ethical?","text":"<p>It depends. If you have legal questions seek legal counsel. You can mitigate some ethical issues by building delays and restrictions into your web scraping program so as to avoid impacting the availability of the web service for other users or the cost of hosting the service for the service provider.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#web-scraping-approaches","title":"Web scraping approaches","text":"<p>No two websites are identical \u2014 websites are built for different purposes by different people and so have different underlying structures. Because they are heterogeneous, there is no single way to scrape a website. The scraping approach therefore has to be tailored to each individual site. Here are some commonly used approaches:</p> <ol> <li>Use requests to extract information from structured JSON files</li> <li>Use requests to extract information from HTML</li> <li>Automate a browser to retrieve information from HTML</li> </ol> <p>Bear in mind that even once you\u2019ve decided upon the best approach for a particular site, it will be necessary to modify that approach to suit your particular use-case.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#how-does-the-web-work","title":"How does the web work?","text":"","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#components","title":"Components","text":"<p>Computers connected to the web are called clients and servers. A simplified diagram of how they interact might look like this:</p> <p></p> <ul> <li>Clients are the typical web user\u2019s internet-connected devices     (for example, your computer connected to your Wi-Fi) and     web-accessing software available on those devices (usually a web     browser like Firefox or Chrome).</li> <li>Servers are computers that store webpages, sites, or apps. When     a client device wants to access a webpage, a copy of the webpage is     downloaded from the server onto the client machine to be displayed     in the user\u2019s web browser.</li> <li>HTTP is a language for clients and servers to speak to each     other.</li> </ul>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#so-what-happens","title":"So what happens?","text":"<p>When you type a web address into your browser:</p> <ol> <li>The browser finds the address of the server that the website lives     on.</li> <li>The browser sends an HTTP request message to the server, asking     it to send a copy of the website to the client.</li> <li>If the server approves the client\u2019s request, the server sends the     client a <code>200 OK</code> message, and then starts displaying the website in     the browser.</li> </ol>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#retrieve-data-in-json-format-if-you-can","title":"Retrieve data in JSON format if you can","text":"**GOAL: To retrieve information in JSON format and organize it into a spreadsheet.**  1.  Inspect the website to check if the content is stored in JSON format 2.  Make a request to the website server to retrieve the JSON file 3.  Convert from JSON format into a Python dictionary 4.  Extract the data from the dictionary and store in a .csv file   <p>We wish to extract information from https://www.harvardartmuseums.org/collections. Like most modern web pages, a lot goes on behind the scenes to produce the page we see in our browser. Our goal is to pull back the curtain to see what the website does when we interact with it. Once we see how the website works we can start retrieving data from it.</p> <p>If we are lucky we\u2019ll find a resource that returns the data we\u2019re looking for in a structured format like JSON.</p> <p></p> <p>This is useful because it is very easy to convert data from JSON into a spreadsheet type format \u2014 like a csv or Excel file.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#examine-the-websites-structure","title":"Examine the website\u2019s structure","text":"<p>The basic strategy is pretty much the same for most scraping projects. We will use our web browser (Chrome or Firefox recommended) to examine the page you wish to retrieve data from, and copy/paste information from your web browser into your scraping program.</p> <p>We start by opening the collections web page in a web browser and inspecting it.</p> <p></p> <p></p> <p>If we scroll down to the bottom of the Collections page, we\u2019ll see a button that says \u201cLoad More\u201d. Let\u2019s see what happens when we click on that button. To do so, click on \u201cNetwork\u201d in the developer tools window, then click the \u201cLoad More Collections\u201d button. You should see a list of requests that were made as a result of clicking that button, as shown below.</p> <p></p> <p>If we look at that second request, the one to a script named <code>browse</code>, we\u2019ll see that it returns all the information we need, in a convenient format called <code>JSON</code>. All we need to retrieve collection data is to make <code>GET</code> requests to https://www.harvardartmuseums.org/browse with the correct parameters.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#launch-jupyterlab","title":"Launch JupyterLab","text":"<ol> <li>Start the <code>Anaconda Navigator</code> program</li> <li>Click the <code>Launch</code> button under <code>Jupyter Lab</code></li> <li>A browser window will open with your computer\u2019s files listed on the     left hand side of the page. Navigate to the folder called     <code>PythonWebScrape</code> that you downloaded to your desktop and     double-click on the folder</li> <li>Within the <code>PythonWebScrape</code> folder, double-click on the file with     the word \u201cBLANK\u201d in the name (<code>PythonWebScrape_BLANK.ipynb</code>). A     pop-up window will ask you to <code>Select Kernal</code> \u2014 you should select     the Python 3 kernal. The Jupyter Notebook should now open on the     right hand side of the page</li> </ol> <p>A Jupyter Notebook contains one or more cells containing notes or code. To insert a new cell click the <code>+</code> button in the upper left. To execute a cell, select it and press <code>Control+Enter</code> or click the <code>Run</code> button at the top.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#making-requests","title":"Making requests","text":"<p>To retrieve information from the website (i.e., make a request), we need to know the location of the information we want to collect. The Uniform Resource Locator (URL) \u2014 commonly know as a \u201cweb address\u201d, specifies the location of a resource (such as a web page) on the internet.</p> <p>A URL is usually composed of 5 parts:</p> <p></p> <p>The 4th part, the \u201cquery string\u201d, contains one or more parameters. The 5th part, the \u201cfragment\u201d, is an internal page reference and may not be present.</p> <p>For example, the URL we want to retrieve data from has the following structure:</p> <pre><code>protocol                    domain    path  parameters\n   https www.harvardartmuseums.org  browse  load_amount=10&amp;offset=0\n</code></pre> <p>It is often convenient to create variables containing the domain(s) and path(s) you\u2019ll be working with, as this allows you to swap out paths and parameters as needed. Note that the path is separated from the domain with <code>/</code> and the parameters are separated from the path with <code>?</code>. If there are multiple parameters they are separated from each other with a <code>&amp;</code>.</p> <p>For example, we can define the domain and path of the collections URL as follows:</p> <pre><code>museum_domain = 'https://www.harvardartmuseums.org'\ncollection_path = 'browse'\n\ncollection_url = (museum_domain\n                  + \"/\"\n                  + collection_path)\n\nprint(collection_url)\n</code></pre> <pre><code>## 'https://www.harvardartmuseums.org/browse'\n</code></pre> <p>Note that we omit the parameters here because it is usually easier to pass them as a <code>dict</code> when using the <code>requests</code> library in Python. This will become clearer shortly.</p> <p>Now that we\u2019ve constructed the URL we wish to interact with, we\u2019re ready to make our first request in Python.</p> <pre><code>import requests\n\ncollections1 = requests.get(\n    collection_url,\n    params = {'load_amount': 10,\n                  'offset': 0}\n)\n</code></pre> <p>Note that the parameters <code>load_amount</code> and <code>offset</code> are essentially another way of setting page numbers \u2014 they refer to the amount of information retrieved at one time and the starting position, respectively.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#parsing-json-data","title":"Parsing JSON data","text":"<p>We already know from inspecting network traffic in our web browser that this URL returns JSON, but we can use Python to verify this assumption.</p> <pre><code>collections1.headers['Content-Type']\n</code></pre> <pre><code>## 'application/json'\n</code></pre> <p>Since JSON is a structured data format, parsing it into Python data structures is easy. In fact, there\u2019s a method for that!</p> <pre><code>collections1 = collections1.json()\n# print(collections1)\n</code></pre> <p>That\u2019s it. Really, we are done here. Everyone go home!</p> <p>OK not really, there is still more we can learn. But you have to admit that was pretty easy. If you can identify a service that returns the data you want in structured from, web scraping becomes a pretty trivial enterprise. We\u2019ll discuss several other scenarios and topics, but for some web scraping tasks this is really all you need to know.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#organizing-saving-the-data","title":"Organizing &amp; saving the data","text":"<p>The records we retrieved from https://www.harvardartmuseums.org/browse are arranged as a list of dictionaries. We can easily select the fields of arrange these data into a pandas <code>DataFrame</code> to facilitate subsequent analysis.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>records1 = pd.DataFrame.from_records(collections1['records'])\n</code></pre> <pre><code>print(records1)\n</code></pre> <pre><code>##                        copyright  contextualtextcount  ...                     dimensions                        seeAlso\n## 0                           None                    0  ...  18.5 x 25.5 cm (7 5/16 x 1...  [{'id': 'https://iiif.harv...\n## 1                           None                    0  ...  17.8 x 14.6 cm (7 x 5 3/4 ...  [{'id': 'https://iiif.harv...\n## 2                           None                    0  ...  H. 17.7 x W. 11.0 x D. 9.0...  [{'id': 'https://iiif.harv...\n## 3                           None                    0  ...  image: 21 x 27.6 cm (8 1/4...  [{'id': 'https://iiif.harv...\n## 4                           None                    0  ...  image: 33 x 25 cm (13 x 9 ...  [{'id': 'https://iiif.harv...\n## 5            \u00a9 William Kentridge                    0  ...  plate: 24.5 x 37.7 cm (9 5...  [{'id': 'https://iiif.harv...\n## 6                           None                    0  ...  257.98 g\\r\\n13.2 \u00d7 9.7 cm ...  [{'id': 'https://iiif.harv...\n## 7  \u00a9 Artists Rights Society (...                    0  ...  sheet: 34.5 x 37 cm (13 9/...  [{'id': 'https://iiif.harv...\n## 8                           None                    0  ...  16.8 x 26.7 cm (6 5/8 x 10...  [{'id': 'https://iiif.harv...\n## 9                           None                    0  ...  paintings proper: H. 145.5...  [{'id': 'https://iiif.harv...\n## \n## [10 rows x 62 columns]\n</code></pre> <p>and write the data to a file.</p> <pre><code>records1.to_csv(\"records1.csv\")\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#iterating-to-retrieve-all-the-data","title":"Iterating to retrieve all the data","text":"<p>Of course we don\u2019t want just the first page of collections. How can we retrieve all of them?</p> <p>Now that we know the web service works, and how to make requests in Python, we can iterate in the usual way.</p> <pre><code>records = []\nfor offset in range(0, 50, 10):\n    param_values = {'load_amount': 10, 'offset': offset}\n    current_request = requests.get(collection_url, params = param_values)\n    records.extend(current_request.json()['records'])\n</code></pre> <pre><code>## convert list of dicts to a `DataFrame`\nrecords_final = pd.DataFrame.from_records(records)\n</code></pre> <pre><code># write the data to a file.\nrecords_final.to_csv(\"records_final.csv\")\n</code></pre> <pre><code>print(records_final)\n</code></pre> <pre><code>##                         copyright  contextualtextcount  ...                        seeAlso                        details\n## 0                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 1                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 2                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 3                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 4                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 5             \u00a9 William Kentridge                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 6                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 7   \u00a9 Artists Rights Society (...                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 8                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 9                            None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 10                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 11                           None                    0  ...  [{'id': 'https://iiif.harv...  {'coins': {'reverseinscrip...\n## 12                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 13  \\r\\n\u00a9 Vija Celmins, Courte...                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 14                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 15                           None                    1  ...  [{'id': 'https://iiif.harv...                            NaN\n## 16                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 17                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 18                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 19  \u00a9 Sarah Jane Roszak / Arti...                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 20                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 21                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 22                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 23  \u00a9 Jenny Holzer / Artists R...                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 24                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 25                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 26                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 27                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 28                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 29                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 30                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 31                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 32                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 33                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 34                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 35                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 36               \u00a9 Gary Schneider                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 37                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 38              \u00a9 1974 Mimi Smith                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 39                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 40                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 41                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 42                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 43                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 44                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 45                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 46                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 47   \u00a9 Andy Warhol Foundation ...                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 48                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## 49                           None                    0  ...  [{'id': 'https://iiif.harv...                            NaN\n## \n## [50 rows x 63 columns]\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#exercise-0","title":"Exercise 0","text":"<p>Retrieve exhibits data</p> <p>In this exercise you will retrieve information about the art exhibitions at Harvard Art Museums from https://www.harvardartmuseums.org/exhibitions</p> <ol> <li>Using a web browser (Firefox or Chrome recommended) inspect the page     at https://www.harvardartmuseums.org/exhibitions. Examine the     network traffic as you interact with the page. Try to find where the     data displayed on that page comes from.</li> </ol> <pre><code>##\n</code></pre> <ol> <li>Make a <code>get</code> request in Python to retrieve the data from the URL     identified in step1.</li> </ol> <pre><code>##\n</code></pre> <ol> <li>Write a loop or list comprehension in Python to retrieve data     for the first 5 pages of exhibitions data.</li> </ol> <pre><code>##\n</code></pre> <ol> <li>Bonus (optional): Convert the data you retrieved into a pandas     <code>DataFrame</code> and save it to a <code>.csv</code> file.</li> </ol> <pre><code>##\n</code></pre> Click for Exercise 0 Solution   Question #1:  <pre><code>museum_domain = \"https://www.harvardartmuseums.org\"\nexhibit_path = \"search/load_more\"\nexhibit_url = museum_domain + \"/\" + exhibit_path\nprint(exhibit_url)\n</code></pre>      ## 'https://www.harvardartmuseums.org/search/load_more'  Question #2:  <pre><code>import requests\nfrom pprint import pprint as print \nexhibit1 = requests.get(exhibit_url, params = {'type': 'past-exhibition', 'page': 1})\nprint(exhibit1.headers[\"Content-Type\"])\n</code></pre>      ## 'application/json'  <pre><code>exhibit1 = exhibit1.json()\n# print(exhibit1)\n</code></pre>  Questions #3+4 (loop solution):  <pre><code>firstFivePages = []\nfor page in range(1, 6):\n    records_per_page = requests.get(exhibit_url, \\\n        params = {'type': 'past-exhibition', 'page': page}).json()['records']\n    firstFivePages.extend(records_per_page)\nfirstFivePages_records = pd.DataFrame.from_records(firstFivePages)\nprint(firstFivePages_records)\n</code></pre>      ##                  shortdescription                         images  ...                         videos                   publications     ## 0                            None  [{'date': '2018-08-03', 'c...  ...                            NaN                            NaN     ## 1                            None  [{'date': '2018-01-12', 'c...  ...  [{'description': 'Bauhaus ...  [{'publicationplace': 'Cam...     ## 2                            None  [{'date': '2019-01-22', 'c...  ...                            NaN                            NaN     ## 3                            None  [{'date': None, 'copyright...  ...                            NaN                            NaN     ## 4                            None  [{'date': '2018-11-09', 'c...  ...                            NaN                            NaN     ## 5                            None  [{'date': '2018-06-04', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 6                            None  [{'date': '2001-03-01', 'c...  ...                            NaN                            NaN     ## 7                            None  [{'date': '2005-04-18', 'c...  ...                            NaN                            NaN     ## 8                            None  [{'date': '2018-06-29', 'c...  ...  [{'description': 'Marina I...                            NaN     ## 9                            None  [{'date': '2018-03-15', 'c...  ...                            NaN                            NaN     ## 10                           None  [{'date': '2016-10-17', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 11                           None  [{'date': '2017-02-16', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 12                           None  [{'date': '2018-01-23', 'c...  ...                            NaN                            NaN     ## 13                           None  [{'date': '2001-04-01', 'c...  ...                            NaN                            NaN     ## 14                           None  [{'date': '2016-06-10', 'c...  ...  [{'description': 'Fernando...                            NaN     ## 15                           None  [{'date': '2008-10-27', 'c...  ...                            NaN                            NaN     ## 16                           None  [{'date': '2017-10-05', 'c...  ...                            NaN                            NaN     ## 17                           None  [{'date': '2002-05-01', 'c...  ...                            NaN                            NaN     ## 18                           None  [{'date': '2007-08-01', 'c...  ...                            NaN                            NaN     ## 19                           None  [{'date': '2003-03-21', 'c...  ...                            NaN                            NaN     ## 20                           None  [{'date': '2017-05-08', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 21                           None  [{'date': '2002-08-01', 'c...  ...  [{'description': 'Symposiu...                            NaN     ## 22                           None  [{'date': '2016-07-05', 'c...  ...                            NaN                            NaN     ## 23                           None  [{'date': '2017-03-29', 'c...  ...                            NaN                            NaN     ## 24                           None  [{'date': '2015-03-22', 'c...  ...  [{'description': 'The Phil...  [{'publicationplace': 'Cam...     ## 25                           None  [{'date': '2017-03-07', 'c...  ...                            NaN                            NaN     ## 26  Harvard professor Ewa Laje...  [{'date': '2001-03-01', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 27  This exhibition features w...  [{'date': '2016-07-12', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 28                           None  [{'date': '2017-02-08', 'c...  ...                            NaN                            NaN     ## 29                           None  [{'date': '2001-06-01', 'c...  ...                            NaN                            NaN     ## 30                           None  [{'date': '2005-11-03', 'c...  ...                            NaN                            NaN     ## 31                           None  [{'date': '2015-04-06', 'c...  ...  [{'description': 'Wolfgang...                            NaN     ## 32                    In progress  [{'date': '2016-07-28', 'c...  ...                            NaN                            NaN     ## 33                           None  [{'date': None, 'copyright...  ...                            NaN                            NaN     ## 34                           None  [{'date': '2016-03-21', 'c...  ...                            NaN                            NaN     ## 35                           None  [{'date': '2007-04-18', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 36                           None  [{'date': '2015-04-01', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 37                           None  [{'date': '2016-01-12', 'c...  ...                            NaN                            NaN     ## 38                           None  [{'date': '2002-07-01', 'c...  ...                            NaN                            NaN     ## 39                           None  [{'date': '2007-02-27', 'c...  ...                            NaN                            NaN     ## 40                           None  [{'date': '2006-01-18', 'c...  ...                            NaN                            NaN     ## 41                           None  [{'date': '2015-10-29', 'c...  ...                            NaN                            NaN     ## 42                           None  [{'date': '2014-08-12', 'c...  ...  [{'description': 'Teaser: ...  [{'publicationplace': 'Cam...     ## 43                           None  [{'date': '2005-11-03', 'c...  ...                            NaN                            NaN     ## 44                           None  [{'date': '1989-08-01', 'c...  ...                            NaN                            NaN     ## 45  This multi-component insta...  [{'date': '2014-03-31', 'c...  ...                            NaN                            NaN     ## 46                           None  [{'date': '2009-11-30', 'c...  ...                            NaN                            NaN     ## 47  Harvard Art Museums\u2019 new p...  [{'date': '2012-06-29', 'c...  ...  [{'description': 'Terry Wi...                            NaN     ## 48                           None  [{'date': '2014-11-10', 'c...  ...                            NaN                            NaN     ## 49  In the Japanese context, a...  [{'date': '2012-07-17', 'c...  ...                            NaN                            NaN     ##      ## [50 rows x 19 columns]  Questions #3+4 (list comprehension solution):  <pre><code>first5Pages = [requests.get(exhibit_url, \\\n    params = {'type': 'past-exhibition', 'page': page}).json()['records'] for page in range(1, 6)]\nfrom itertools import chain\nfirst5Pages = list(chain.from_iterable(first5Pages))\nimport pandas as pd\nfirst5Pages_records = pd.DataFrame.from_records(first5Pages)\nprint(first5Pages_records)\n</code></pre>      ##                  shortdescription                         images  ...                         videos                   publications     ## 0                            None  [{'date': '2018-08-03', 'c...  ...                            NaN                            NaN     ## 1                            None  [{'date': '2018-01-12', 'c...  ...  [{'description': 'Bauhaus ...  [{'publicationplace': 'Cam...     ## 2                            None  [{'date': '2019-01-22', 'c...  ...                            NaN                            NaN     ## 3                            None  [{'date': None, 'copyright...  ...                            NaN                            NaN     ## 4                            None  [{'date': '2018-11-09', 'c...  ...                            NaN                            NaN     ## 5                            None  [{'date': '2018-06-04', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 6                            None  [{'date': '2001-03-01', 'c...  ...                            NaN                            NaN     ## 7                            None  [{'date': '2005-04-18', 'c...  ...                            NaN                            NaN     ## 8                            None  [{'date': '2018-06-29', 'c...  ...  [{'description': 'Marina I...                            NaN     ## 9                            None  [{'date': '2018-03-15', 'c...  ...                            NaN                            NaN     ## 10                           None  [{'date': '2016-10-17', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 11                           None  [{'date': '2017-02-16', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 12                           None  [{'date': '2018-01-23', 'c...  ...                            NaN                            NaN     ## 13                           None  [{'date': '2001-04-01', 'c...  ...                            NaN                            NaN     ## 14                           None  [{'date': '2016-06-10', 'c...  ...  [{'description': 'Fernando...                            NaN     ## 15                           None  [{'date': '2008-10-27', 'c...  ...                            NaN                            NaN     ## 16                           None  [{'date': '2017-10-05', 'c...  ...                            NaN                            NaN     ## 17                           None  [{'date': '2002-05-01', 'c...  ...                            NaN                            NaN     ## 18                           None  [{'date': '2007-08-01', 'c...  ...                            NaN                            NaN     ## 19                           None  [{'date': '2003-03-21', 'c...  ...                            NaN                            NaN     ## 20                           None  [{'date': '2017-05-08', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 21                           None  [{'date': '2002-08-01', 'c...  ...  [{'description': 'Symposiu...                            NaN     ## 22                           None  [{'date': '2016-07-05', 'c...  ...                            NaN                            NaN     ## 23                           None  [{'date': '2017-03-29', 'c...  ...                            NaN                            NaN     ## 24                           None  [{'date': '2015-03-22', 'c...  ...  [{'description': 'The Phil...  [{'publicationplace': 'Cam...     ## 25                           None  [{'date': '2017-03-07', 'c...  ...                            NaN                            NaN     ## 26  Harvard professor Ewa Laje...  [{'date': '2001-03-01', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 27  This exhibition features w...  [{'date': '2016-07-12', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 28                           None  [{'date': '2017-02-08', 'c...  ...                            NaN                            NaN     ## 29                           None  [{'date': '2001-06-01', 'c...  ...                            NaN                            NaN     ## 30                           None  [{'date': '2005-11-03', 'c...  ...                            NaN                            NaN     ## 31                           None  [{'date': '2015-04-06', 'c...  ...  [{'description': 'Wolfgang...                            NaN     ## 32                    In progress  [{'date': '2016-07-28', 'c...  ...                            NaN                            NaN     ## 33                           None  [{'date': None, 'copyright...  ...                            NaN                            NaN     ## 34                           None  [{'date': '2016-03-21', 'c...  ...                            NaN                            NaN     ## 35                           None  [{'date': '2007-04-18', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 36                           None  [{'date': '2015-04-01', 'c...  ...                            NaN  [{'publicationplace': 'Cam...     ## 37                           None  [{'date': '2016-01-12', 'c...  ...                            NaN                            NaN     ## 38                           None  [{'date': '2002-07-01', 'c...  ...                            NaN                            NaN     ## 39                           None  [{'date': '2007-02-27', 'c...  ...                            NaN                            NaN     ## 40                           None  [{'date': '2006-01-18', 'c...  ...                            NaN                            NaN     ## 41                           None  [{'date': '2015-10-29', 'c...  ...                            NaN                            NaN     ## 42                           None  [{'date': '2014-08-12', 'c...  ...  [{'description': 'Teaser: ...  [{'publicationplace': 'Cam...     ## 43                           None  [{'date': '2005-11-03', 'c...  ...                            NaN                            NaN     ## 44                           None  [{'date': '1989-08-01', 'c...  ...                            NaN                            NaN     ## 45  This multi-component insta...  [{'date': '2014-03-31', 'c...  ...                            NaN                            NaN     ## 46                           None  [{'date': '2009-11-30', 'c...  ...                            NaN                            NaN     ## 47  Harvard Art Museums\u2019 new p...  [{'date': '2012-06-29', 'c...  ...  [{'description': 'Terry Wi...                            NaN     ## 48                           None  [{'date': '2014-11-10', 'c...  ...                            NaN                            NaN     ## 49  In the Japanese context, a...  [{'date': '2012-07-17', 'c...  ...                            NaN                            NaN     ##      ## [50 rows x 19 columns]","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#parsing-html-if-you-have-to","title":"Parsing HTML if you have to","text":"**GOAL: To retrieve information in HTML format and organize it into a spreadsheet.**  1.  Make a request to the website server to retrieve the HTML 2.  Inspect the HTML to determine the XPATHs that point to the data we     want 3.  Extract the information from the location the XPATHs point to and     store in a dictionary 4.  Convert from a dictionary to a .csv file   <p>As we\u2019ve seen, you can often inspect network traffic or other sources to locate the source of the data you are interested in and the API used to retrieve it. You should always start by looking for these shortcuts and using them where possible. If you are really lucky, you\u2019ll find a shortcut that returns the data as JSON. If you are not quite so lucky, you will have to parse HTML to retrieve the information you need.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#document-object-model-dom","title":"Document Object Model (DOM)","text":"<p>To parse HTML, we need to have a nice tree structure that contains the whole HTML file through which we can locate the information. This tree-like structure is the Document Object Model (DOM). DOM is a cross-platform and language-independent interface that treats an XML or HTML document as a tree structure wherein each node is an object representing a part of the document. The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects. DOM methods allow programmatic access to the tree; with them one can change the structure, style or content of a document. The following is an example of DOM hierarchy in an HTML document:</p> <p></p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#retrieving-html","title":"Retrieving HTML","text":"<p>When I inspect the network traffic while interacting with https://www.harvardartmuseums.org/calendar I don\u2019t see any requests that return JSON data. The best we can do appears to be to return HTML.</p> <p>To retrieve data on the events listed in the calender, the first step is the same as before: we make a <code>get</code> request.</p> <pre><code>calendar_path = 'calendar'\n\ncalendar_url = (museum_domain # recall that we defined museum_domain earlier\n                  + \"/\"\n                  + calendar_path)\n\nprint(calendar_url)\n</code></pre> <pre><code>## 'https://www.harvardartmuseums.org/calendar'\n</code></pre> <pre><code>events = requests.get(calendar_url)\n</code></pre> <p>As before, we can check the headers to see what type of content we received in response to our request.</p> <pre><code>events.headers['Content-Type']\n</code></pre> <pre><code>## 'text/html; charset=UTF-8'\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#parsing-html-using-the-lxml-library","title":"Parsing HTML using the lxml library","text":"<p>Like JSON, HTML is structured; unlike JSON, it is designed to be rendered into a human-readable page rather than simply to store and exchange data in a computer-readable format. Consequently, parsing HTML and extracting information from it is somewhat more difficult than parsing JSON.</p> <p>While JSON parsing is built into the Python <code>requests</code> library, parsing HTML requires a separate library. I recommend using the HTML parser from the <code>lxml</code> library; others prefer an alternative called <code>beautifulsoup4</code>.</p> <pre><code>from lxml import html\n\n# convert a html text representation (`events.text`) into \n# a tree-structure (DOM) html representation (`events_html`)\nevents_html = html.fromstring(events.text)\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#using-xpath-to-extract-content-from-html","title":"Using XPath to extract content from HTML","text":"<p><code>XPath</code> is a tool for identifying particular elements within a HTML document. The developer tools built into modern web browsers make it easy to generate <code>XPath</code>s that can be used to identify the elements of a web page that we wish to extract.</p> <p>We can open the HTML document we retrieved and inspect it using our web browser.</p> <pre><code>html.open_in_browser(events_html, encoding = 'UTF-8')\n</code></pre> <p></p> <p></p> <p>Once we identify the element containing the information of interest we can use our web browser to copy the <code>XPath</code> that uniquely identifies that element.</p> <p></p> <p>Next we can use Python to extract the element of interest:</p> <pre><code>events_list_html = events_html.xpath('//*[@id=\"events_list\"]/article')\n</code></pre> <p>Let\u2019s just extract the second element in our events list.</p> <pre><code>second_event_html = events_list_html[1]\n</code></pre> <p>Once again, we can use a web browser to inspect the HTML we\u2019re currently working with - from the second event - and to figure out what we want to extract from it.</p> <pre><code>html.open_in_browser(second_event_html, encoding = 'UTF-8')\n</code></pre> <p>As before, we can use our browser to find the xpath of the elements we want.</p> <p></p> <p>(Note that the <code>html.open_in_browser</code> function adds enclosing <code>html</code> and <code>body</code> tags in order to create a complete web page for viewing. This requires that we adjust the <code>xpath</code> accordingly.)</p> <p>By repeating this process for each element we want, we can build a list of the xpaths to those elements.</p> <pre><code>elements_we_want = {'figcaption': 'div/figure/div/figcaption',\n                    'date': 'div/div/header/time',\n                    'title': 'div/div/header/h2/a',\n                    'time': 'div/div/div/p[1]/time',\n                    'description': 'div/div/div/p[3]'\n                    }\n</code></pre> <p>Finally, we can iterate over the elements we want and extract them.</p> <pre><code>second_event_values = {}\nfor key in elements_we_want.keys():\n    element = second_event_html.xpath(elements_we_want[key])[0]\n    second_event_values[key] = element.text_content().strip()\n\nprint(second_event_values)\n</code></pre> <pre><code>## {'date': 'Wednesday, February 23, 2022',\n##  'description': 'Virtual gallery tours are designed especially for our Friends '\n##                 'and Fellows and are led by our curators, fellows, and other '\n##                 'specialists.',\n##  'figcaption': 'Photo: Kate Smith.',\n##  'time': '9:30am - 10:30am',\n##  'title': 'Virtual Gallery Tour for Friends and Fellows of the Museums'}\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#iterating-to-retrieve-content-from-a-list-of-html-elements","title":"Iterating to retrieve content from a list of HTML elements","text":"<p>So far we\u2019ve retrieved information only for the second event. To retrieve data for all the events listed on the page we need to iterate over the events. If we are very lucky, each event will have exactly the same information structured in exactly the same way and we can simply extend the code we wrote above to iterate over the events list.</p> <p>Unfortunately, not all these elements are available for every event, so we need to take care to handle the case where one or more of these elements is not available. We can do that by defining a function that tries to retrieve a value and returns an empty string if it fails.</p> <p>If you\u2019re not familiar with Python functions, here\u2019s the basic syntax:</p> <pre><code># anatomy of a function\n\ndef name_of_function(arg1, arg2, ...argn):  # define the function name and arguments\n    &lt;body of function&gt;   # specify calculations\n    return &lt;result&gt;      # output result of calculations\n</code></pre> <p>Here\u2019s an example of a simple function:</p> <pre><code>def square_fun(x):\n    y = x**2 # exponentiation\n    return y\n\nsquare_fun(4)    \n</code></pre> <pre><code>## 16\n</code></pre> <p>Here\u2019s a function to perform our actual task:</p> <pre><code>def get_event_info(event, path):\n    try:\n        info = event.xpath(path)[0].text_content().strip()\n    except:\n        info = ''\n    return info\n</code></pre> <p>Armed with this function, we can iterate over the list of events and extract the available information for each one.</p> <pre><code>all_event_values = {}\nfor key in elements_we_want.keys():\n    key_values = []\n    for event in events_list_html: \n        key_values.append(get_event_info(event, elements_we_want[key]))\n    all_event_values[key] = key_values\n</code></pre> <p>For convenience we can arrange these values in a pandas <code>DataFrame</code> and save them as .csv files, just as we did with our exhibitions data earlier.</p> <pre><code>all_event_values = pd.DataFrame.from_dict(all_event_values)\n\nall_event_values.to_csv(\"all_event_values.csv\")\n\nprint(all_event_values)\n</code></pre>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#exercise-1","title":"Exercise 1","text":"<p>parsing HTML</p> <p>In this exercise you will retrieve information about the physical layout of the Harvard Art Museums. The web page at https://www.harvardartmuseums.org/visit/floor-plan contains this information in HTML from.</p> <ol> <li> <p>Using a web browser (Firefox or Chrome recommended) inspect the page     at https://www.harvardartmuseums.org/visit/floor-plan. Copy the     <code>XPath</code> to the element containing the list of level information.     (HINT: the element if interest is a <code>ul</code>, i.e., <code>unordered list</code>.)</p> </li> <li> <p>Make a <code>get</code> request in Python to retrieve the web page at     https://www.harvardartmuseums.org/visit/floor-plan. Extract the     content from your request object and parse it using     <code>html.fromstring</code> from the <code>lxml</code> library.</p> </li> </ol> <pre><code>##\n</code></pre> <ol> <li>Use your web browser to find the <code>XPath</code>s to the facilities housed     on level one. Use Python to extract the text from those <code>Xpath</code>s.</li> </ol> <pre><code>##\n</code></pre> <ol> <li>Bonus (optional): Write a for loop or list comprehension in     Python to retrieve data for all the levels.</li> </ol> <pre><code>##\n</code></pre> Click for Exercise 1 Solution   Question #2:  <pre><code>from lxml import html\nfloor_plan = requests.get('https://www.harvardartmuseums.org/visit/floor-plan')\nfloor_plan_html = html.fromstring(floor_plan.text)\n</code></pre>  Question #3:  <pre><code>level_one = floor_plan_html.xpath('/html/body/main/section/ul/li[5]/div[2]/ul')[0]\nprint(type(level_one))\n</code></pre>      ##  <pre><code>print(len(level_one))\n</code></pre>      ## 6  <pre><code>level_one_facilities = floor_plan_html.xpath('/html/body/main/section/ul/li[5]/div[2]/ul/li')\nprint(len(level_one_facilities))\n</code></pre>      ## 6  <pre><code>print([facility.text_content() for facility in level_one_facilities])\n</code></pre>      ## ['Admissions', 'Collection Galleries', 'Courtyard', 'Shop', 'Caf\u00e9', 'Coatroom']  Question #4:  <pre><code>all_levels = floor_plan_html.xpath('/html/body/main/section/ul/li')\nprint(len(all_levels))\n</code></pre>      ## 6  <pre><code>all_levels_facilities = []\nfor level in all_levels:\n    level_facilities = []\n    level_facilities_collection = level.xpath('div[2]/ul/li')\n    for level_facility in level_facilities_collection:\n        level_facilities.append(level_facility.text_content())\n    all_levels_facilities.append(level_facilities)\nprint(all_levels_facilities)\n</code></pre>      ## [['Conservation Center / Lightbox Gallery'],     ##  ['Art Study Center'],     ##  ['Collection Galleries',     ##   'Special Exhibitions Gallery',     ##   'University Galleries'],     ##  ['Collections Galleries'],     ##  ['Admissions',     ##   'Collection Galleries',     ##   'Courtyard',     ##   'Shop',     ##   'Caf\u00e9',     ##   'Coatroom'],     ##  ['Lower Lobby',     ##   'Lecture Halls',     ##   'Seminar Room',     ##   'Materials Lab',     ##   'Coatroom',     ##   'Offices']]","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#scrapy-for-large-complex-projects","title":"<code>Scrapy</code>: for large / complex projects","text":"<p>Scraping websites using the <code>requests</code> library to make GET and POST requests, and the <code>lxml</code> library to process HTML is a good way to learn basic web scraping techniques. It is a good choice for small to medium size projects. For very large or complicated scraping tasks the <code>scrapy</code> library offers a number of conveniences, including asynchronous retrieval, session management, convenient methods for extracting and storing values, and more. More information about <code>scrapy</code> can be found at https://doc.scrapy.org.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#browser-drivers-a-last-resort","title":"Browser drivers: a last resort","text":"<p>It is sometimes necessary (or sometimes just easier) to use a web browser as an intermediary rather than communicate directly with a web service. This method of using a \u201cbrowser driver\u201d has the advantage of being able to use the javascript engine and session management features of a web browser; the main disadvantage is that it is slower and tends to be more fragile than using <code>requests</code> or <code>scrapy</code> to make requests directly from Python. For small scraping projects involving complicated sites with CAPTHAs or lots of complicated javascript using a browser driver can be a good option. More information is available at https://www.seleniumhq.org/docs/03_webdriver.jsp.</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#wrap-up","title":"Wrap-up","text":"","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#feedback","title":"Feedback","text":"<p>These workshops are a work in progress, please provide any feedback to: help@iq.harvard.edu</p>","tags":["Scraping","Python"]},{"location":"tutorials/PythonWebScrape/#resources","title":"Resources","text":"<ul> <li>IQSS<ul> <li>Workshops:     https://www.iq.harvard.edu/data-science-services/workshop-materials</li> <li>Data Science Services:     https://www.iq.harvard.edu/data-science-services</li> <li>Research Computing Environment:     https://iqss.github.io/dss-rce/</li> </ul> </li> <li>HBS<ul> <li>Research Computing Services workshops:     https://training.rcs.hbs.org/workshops</li> <li>Other HBS RCS resources:     https://training.rcs.hbs.org/workshop-materials</li> <li>RCS consulting email:     mailto:research@hbs.edu</li> </ul> </li> </ul>","tags":["Scraping","Python"]},{"location":"tutorials/large_data_R/","title":"Large Data in R: Tools and Techniques","text":"","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#set-up-and-example-data","title":"Set Up and Example Data","text":"<p>The examples and exercises require R and several R packages, including <code>tidyverse</code>, <code>data.table</code>, <code>arrow</code>, and <code>duckdb</code>. This software is all  installed and ready to use on the HBS Grid. If running elsewhere make sure these required software programs are installed before proceeding.</p> <p>You can download and extract the data used in the examples and exercises from https://www.dropbox.com/s/vbodicsu591o7lf/original_csv.zip?dl=1 (this is a 1.3Gb zip file). These data record for-hire vehicle (aka \u201cride sharing\u201d) trips in NYC in 2020. Each row contains the record of a trip and the variable descriptions can be found in https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#nature-and-scope-of-the-problem-what-is-large-data","title":"Nature and Scope of the Problem: What is Large Data?","text":"<p>Most popular data analysis software is designed to operate on data stored in random access memory (aka just \u201cmemory\u201d or \u201cRAM\u201d). This makes modifying and copying data very fast and convenient, until you start working with data that is too large for your computer\u2019s memory system. At that point you have two options: get a bigger computer or modify your workflow to process the data more carefully and efficiently. This workshop focuses on option two, using the <code>arrow</code> and <code>duckdb</code> packages in R to work with data without necessarily loading it all into memory at once.</p> <p>A common definition of \u201cbig data\u201d is \u201cdata that is too big to process using traditional software\u201d. We can use the term \u201clarge data\u201d as a broader category of \u201cdata that is big enough that you have to pay attention to processing it efficiently\u201d.</p> <p>In a typical (traditional) program, we start with data on disk, in some format. We read it in to memory, do some stuff to it on the CPU, store the results of that stuff back in memory, then write those results back to disk so they can be available for the future, as depicted below.</p> <p> The reason most data analysis software is designed to process data this way is because \u201cdoing some stuff\u201d is much much faster in RAM than it is if you have to read values from disk every time you need them. The downside is that RAM is much more expensive than disk storage, and typically available in smaller quantities. Memory can only hold so much data and we must either stay under that limit or buy more memory.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#problem-example","title":"Problem example","text":"<p>Grounding our discussion in a concrete problem example will help make things clear. I want to know how many Lyft rides were taken in New York City during 2020. The data is publicly available as documented at https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page and I have made a subset available on dropbox as described in the Setup section above for convenience. Documentation can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf</p> <p>In order to demonstrate large data problems and solutions I\u2019m going to artificially limit my system to 4Gb of memory. This will allow us to quickly see what happens when we reach the memory limit, and to look at solutions to that problem without waiting for our program to read in hundreds of Gb of data. (There is no need to follow along with this step, the purpose is just to make sure we all know what happens when you run out of memory.)</p> <p>Start by looking at the file names and sizes:</p> <pre><code>fhvhv_csv_files &lt;- list.files(\"original_csv\", recursive=TRUE, full.names = TRUE)\ndata.frame(file = fhvhv_csv_files, size_Mb = file.size(fhvhv_csv_files) / 1024^2)\n</code></pre> <pre><code>##                                               file   size_Mb\n## 1  original_csv/2020/01/fhvhv_tripdata_2020-01.csv 1243.4975\n## 2  original_csv/2020/02/fhvhv_tripdata_2020-02.csv 1313.2442\n## 3  original_csv/2020/03/fhvhv_tripdata_2020-03.csv  808.5597\n## 4  original_csv/2020/04/fhvhv_tripdata_2020-04.csv  259.5806\n## 5  original_csv/2020/05/fhvhv_tripdata_2020-05.csv  366.5430\n## 6  original_csv/2020/06/fhvhv_tripdata_2020-06.csv  454.5977\n## 7  original_csv/2020/07/fhvhv_tripdata_2020-07.csv  599.2560\n## 8  original_csv/2020/08/fhvhv_tripdata_2020-08.csv  667.6880\n## 9  original_csv/2020/09/fhvhv_tripdata_2020-09.csv  728.5463\n## 10 original_csv/2020/10/fhvhv_tripdata_2020-10.csv  798.4743\n## 11 original_csv/2020/11/fhvhv_tripdata_2020-11.csv  698.0638\n## 12 original_csv/2020/12/fhvhv_tripdata_2020-12.csv  700.6804\n</code></pre> <p>We can already guess based on these file sizes that with only 4 Gb of RAM available we\u2019re going to have a problem.</p> <pre><code>library(tidyverse)\nfhvhv_data &lt;- map(fhvhv_csv_files, read_csv) %&gt;% bind_rows(show_col_types=FALSE)\n</code></pre> <pre><code>## Error in eval(expr, envir, enclos): cannot allocate vector of size 7.6 Mb\n</code></pre> <p>Perhaps you\u2019ve seen similar messages before. Basically it means that we don\u2019t have enough memory available to hold the data we want to work with. I previously ran the code chunk above with more memory and found that it required just over 16 Gb. How can we work with data when we only have 1/4 of the memory requirement available?</p> <p> An example of a large object causing a bottleneck</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#general-strategies-and-principles","title":"General strategies and principles","text":"<p>Part of the problem with our first attempt is that CSV files do not make it easy to quickly read subsets or select columns. In this section we\u2019ll spend some time identifying strategies for working with large data and identify some tools that make it easy to implement those strategies.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#use-a-fast-binary-data-storage-format-that-enables-reading-data-subsets","title":"Use a fast binary data storage format that enables reading data subsets","text":"<p>CSVs, TSVs, and similar delimited files are all text-based formats that are typically used to store tabular data. Other more general text-based data storage formats are in wide use as well, including XML and JSON. These text-based formats have the advantage of being both human and machine readable, but text is a relatively inefficient way to store data, and loading it into memory requires a time-consuming parsing process to separate out the fields and records.</p> <p>As an alternative to text-based data storage formats, binary formats have the advantage of being more space efficient on disk and faster to read. They often employ advanced compression techniques, store metadata, and allow fast selective access to data subsets. These substantial advantages come at the cost of human readability; you cannot easily inspect the contents of binary data files directly. If you are concerned with reducing memory use or data processing time this is probably a trade-off you are happy to make.</p> <p>The <code>parquet</code> binary storage format is among the best currently available. Support in R is provided by the <code>arrow</code> package. In a moment we\u2019ll see how we can use the <code>arrow</code> package to dramatically reduce the time it takes to get data from disk to memory and back.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#partition-the-data-on-disk-to-facilitate-chunked-access-and-computation","title":"Partition the data on disk to facilitate chunked access and computation","text":"<p>Memory requirements can be reduced by partitioning the data and computation into chunks, running each one sequentially, and combining the results at the end. It is common practice to partition the data on disk storage to make this computational strategy more natural and efficient. For example, the taxi data is already partitioned by year and month.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#only-read-in-the-data-you-need","title":"Only read in the data you need","text":"<p>If we think carefully about it we\u2019ll see that our previous attempt to process the taxi data by reading in all the data at once was wasteful. Not all the rows are Lyft rides, and the only column I really need is the one that tells me if the ride was operated by Lyft or not. I can perform the computation I need by only reading in that one column, and only the rows for which the <code>hvfhs_license_num</code> column is equal to <code>HV0005</code> (Lyft).</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#use-streaming-data-tools-and-algorithms","title":"Use streaming data tools and algorithms","text":"<p>It\u2019s all fine and good to say \u201conly read the data you need\u201d, but how do you actually do that? Unless you have full control over the data collection and storage process, chances are good that your data provider included a bunch of stuff you don\u2019t need. The key is to find a data selection and filtering tool that works in a streaming fashion so that you can access subsets without ever loading data you don\u2019t need into memory. Both the <code>arrow</code> and <code>duckdb</code> R packages support this type of workflow and can dramatically reduce the time and hardware requirements for many computations.</p> <p>Moreover, processing data in a streaming fashion without needing to load it into memory is a general technique that can be applied to other tasks as well. For example the <code>duckdb</code> package allows you to carry out data aggregation in a streaming fashion, meaning that you can compute summary statistics for data that is too large to fit in memory.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#avoid-unnecessarily-storing-or-duplicating-data-in-memory","title":"Avoid unnecessarily storing or duplicating data in memory","text":"<p>It is also important to pay some attention to storing and processing data efficiently once we have it loaded in memory. R likes to make copies of the data, and while it does try to avoid unnecessary duplication this process can be unpredictable. At a minimum you can remove or avoid storing intermediate results you don\u2019t need and take care not to make copies of your data structures unless you have to. The <code>data.table</code> package additionally makes it easier to efficiently modify R data objects in-place, reducing the risk of accidentally or unknowingly duplicating large data structures.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#technique-summary","title":"Technique summary","text":"<p>We\u2019ve accumulated a list of helpful techniques! To review:</p> <ul> <li>Use a fast binary data storage format that enables reading data     subsets</li> <li>Partition the data on disk to facilitate chunked access and     computation</li> <li>Only read in the data you need</li> <li>Use streaming data tools and algorithms</li> <li>Avoid unnecessarily storing or duplicating data in memory</li> </ul>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#solution-example","title":"Solution example","text":"<p>Now that we have some theoretical foundations to build on we can start putting these techniques into practice. Using the techniques identified above will allow us to overcome the memory limitation we ran up against before, and finally answer the question \u201cHow many Lyft rides were taken in New York City during 2020?\u201d?</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#convert-csv-to-parquet","title":"Convert .csv to parquet","text":"<p>The first step is to take the slow and inefficient text-based data provided by the city of New York and convert it to parquet using the <code>arrow</code> package. This is a one-time up-front cost that may be expensive in terms of time and/or computational resources. If you plan to work with the data a lot it will be well worth it because it allows subsequent reads to be faster and more memory efficient.</p> <pre><code>library(arrow)\n\nif(!dir.exists(\"converted_parquet\")) {\n\n  dir.create(\"converted_parquet\")\n\n  ## this doesn't yet read the data in, it only creates a connection\n  csv_ds &lt;- open_dataset(\"original_csv\", \n                         format = \"csv\",\n                         partitioning = c(\"year\", \"month\"))\n\n  ## this reads each csv file in the csv_ds dataset and converts it to a .parquet file\n  write_dataset(csv_ds, \n                \"converted_parquet\", \n                format = \"parquet\",\n                partitioning = c(\"year\", \"month\"))\n}\n</code></pre> <p>This conversion is relatively easy (even with limited memory) because the data provider is already using one of our strategies, i.e., they partitioned the data by year/month. This allows us to convert each file one at a time, without ever needing to read in all the data at once.</p> <p>We also took care to preserve the <code>year/month</code> partition into sub-directories. We improved on the implementation by using what is known as \u201chive-style\u201d partitioning, i.e., including both the variable names and values in the directory names. This is convenient because it makes it easy for <code>arrow</code> (and other tools that recognize the hive partitioning standard) to automatically recognize the partitions.</p> <p>We can look at the converted files and compare the naming scheme and storage requirements to the original CSV data.</p> <p><pre><code>fhvhv_csv_files &lt;- list.files(\"original_csv\", recursive=TRUE, full.names = TRUE)\nfhvhv_files &lt;- list.files(\"converted_parquet\", full.names = TRUE, recursive = TRUE)\n\ndata.frame(csv_file = fhvhv_csv_files, \n           parquet_file = fhvhv_files, \n           csv_size_Mb = file.size(fhvhv_csv_files) / 1024^2, \n           parquet_size_Mb = file.size(fhvhv_files) / 1024^2)\n</code></pre> <pre><code>                                                                 CSV        Parquet\n                  csv_file                       parquet_file    size_Mb    size_Mb\nfhvhv_tripdata_2020-01.csv   year=2020/month=1/part-0.parquet  1243.4975  190.26387\nfhvhv_tripdata_2020-02.csv  year=2020/month=10/part-0.parquet  1313.2442  125.17837\nfhvhv_tripdata_2020-03.csv  year=2020/month=11/part-0.parquet   808.5597  110.92144\nfhvhv_tripdata_2020-04.csv  year=2020/month=12/part-0.parquet   259.5806  111.67697\nfhvhv_tripdata_2020-05.csv   year=2020/month=2/part-0.parquet   366.5430  198.87074\nfhvhv_tripdata_2020-06.csv   year=2020/month=3/part-0.parquet   454.5977  127.53637\nfhvhv_tripdata_2020-07.csv   year=2020/month=4/part-0.parquet   599.2560   48.32047\nfhvhv_tripdata_2020-08.csv   year=2020/month=5/part-0.parquet   667.6880   64.17768\nfhvhv_tripdata_2020-09.csv   year=2020/month=6/part-0.parquet   728.5463   76.45972\nfhvhv_tripdata_2020-10.csv   year=2020/month=7/part-0.parquet   798.4743   97.99151\nfhvhv_tripdata_2020-11.csv   year=2020/month=8/part-0.parquet   698.0638  107.80694\nfhvhv_tripdata_2020-12.csv   year=2020/month=9/part-0.parquet   700.6804  115.25221\n</code></pre> As expected, the binary parquet storage format is much more compact than the text-based CSV format. This is one reason that reading parquet data is so much faster:</p> <pre><code>## tidyverse csv reader\nsystem.time(invisible(readr::read_csv(fhvhv_csv_files[[1]], show_col_types = FALSE)))\n</code></pre> <pre><code>##    user  system elapsed \n##  79.982   6.362  31.824\n</code></pre> <pre><code>## arrow package parquet reader\nsystem.time(invisible(read_parquet(fhvhv_files[[1]])))\n</code></pre> <pre><code>##    user  system elapsed \n##   5.761   2.226  22.533\n</code></pre>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#read-and-count-lyft-records-with-arrow","title":"Read and count Lyft records with arrow","text":"<p>The <code>arrow</code> package makes it easy to read and process only the data we need for a particular calculation. It allows us to use the partitioned data directories we created earlier as a single dataset and to query it using the <code>dplyr</code> verbs many R users are already familiar with.</p> <p>Start by creating a dataset representation from the partitioned data directory:</p> <pre><code>fhvhv_ds &lt;- open_dataset(\"converted_parquet\",\n                         schema = schema(hvfhs_license_num=string(),\n                                         dispatching_base_num=string(),\n                                         pickup_datetime=string(),\n                                         dropoff_datetime=string(),\n                                         PULocationID=int64(),\n                                         DOLocationID=int64(),\n                                         SR_Flag=int64(),\n                                         year=int32(),\n                                         month=int32()))\n</code></pre> <p>Because we have hive-style directory names <code>open_dataset</code> automatically recognizes the partitions. Note that usually we do not need to manually specify the <code>schema</code>, we do so here to work around an issue with <code>duckdb</code> support.</p> <p>Importantly, <code>open_dataset</code> doesn\u2019t actually read the data into memory. It just opens a connection to the dataset and makes it easy for us to query it. Finally, we can compute the number of NYC Lyft trips in 2020, even on a machine with limited memory:</p> <pre><code>library(dplyr, warn.conflicts = FALSE)\n\nfhvhv_ds %&gt;%\n  filter(hvfhs_license_num == \"HV0005\") %&gt;%\n  select(hvfhs_license_num) %&gt;%\n  collect() %&gt;%\n  summarize(total_Lyft_trips = n())\n</code></pre> <pre><code>## # A tibble: 1 \u00d7 1\n##   total_Lyft_trips\n##              &lt;int&gt;\n## 1         37250101\n</code></pre> <p>Note that <code>arrow</code> datasets do not support <code>summarize</code> natively, that is why we call <code>collect</code> first to actually read in the data.</p> <p>The <code>arrow</code> package makes it fast and easy to query on-disk data and read in only the fields and records needed for a particular computation. This is a tremendous improvement over the typical R workflow, and may well be all you need to start using your large datasets more quickly and conveniently, even on modest hardware.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#efficiently-query-taxi-data-with-duckdb","title":"Efficiently query taxi data with duckdb","text":"<p>If you need even more speed and convenience you can use the <code>duckdb</code> package. It allows you to query the same parquet datasets partitioned on disk as we did above. You can use either SQL statements via the <code>DBI</code> package or tidyverse style verbs using <code>dbplyr</code>. Let\u2019s see how it works.</p> <p>First we create a <code>duckdb</code> table from our <code>arrow</code> dataset.</p> <pre><code>library(duckdb)\nlibrary(dplyr)\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\nfhvhv_tbl &lt;- to_duckdb(fhvhv_ds, con, \"fhvhv\")\n</code></pre> <p>The <code>duckdb</code> table can be queried using tidyverse style verbs or SQL.</p> <pre><code>## number of Lyft trips, tidyverse style\nfhvhv_tbl %&gt;%\n  filter(hvfhs_license_num == \"HV0005\") %&gt;%\n  select(hvfhs_license_num) %&gt;%\n  count()\n</code></pre> <pre><code>## # Source:   lazy query [?? x 1]\n## # Database: duckdb_connection\n##          n\n##      &lt;dbl&gt;\n## 1 37250101\n</code></pre> <pre><code>## number of Lyft trips, SQL style\ny &lt;- dbSendQuery(con, \"SELECT COUNT(*) FROM fhvhv WHERE hvfhs_license_num=='HV0005';\")\ndbFetch(y)\n</code></pre> <pre><code>##   count_star()\n## 1     37250101\n</code></pre> <p>The main advantages of <code>duckdb</code> are that it has full SQL support, supports aggregating data in a streaming fashion, allows you to set memory limits, and is optimized for speed. The way I think about the relationship between <code>arrow</code> and <code>duckdb</code> is that <code>arrow</code> is primarily about reading and writing data as fast and efficiently as possible, with some built-in analysis capabilities, while <code>duckdb</code> is a database engine with more complete data manipulation and aggregation capabilities.</p> <p>It can be instructive to compare <code>arrow</code> and <code>duckdb</code> capabilities and performance using a slightly more complicated example. Here we compute a grouped average using <code>arrow</code>:</p> <pre><code>system.time({fhvhv_ds %&gt;%\n    filter(hvfhs_license_num == \"HV0005\") %&gt;%\n    select(hvfhs_license_num, month) %&gt;%\n    group_by(hvfhs_license_num) %&gt;%\n    collect() %&gt;%\n    summarize(avg = mean(month, na.rm = TRUE)) %&gt;%\n    print()})\n</code></pre> <pre><code>## # A tibble: 1 \u00d7 2\n##   hvfhs_license_num   avg\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 HV0005             6.10\n\n##    user  system elapsed \n##  19.456   4.064  14.254\n</code></pre> <p>note that we use <code>collect</code> to read the data into memory before the <code>summarize</code> step because <code>arrow</code> does not support aggregating in a streaming fashion.</p> <p>Here is the same query using <code>duckdb</code>:</p> <pre><code>system.time({fhvhv_tbl %&gt;%\n    filter(hvfhs_license_num == \"HV0005\") %&gt;%\n    select(hvfhs_license_num, month) %&gt;%\n    group_by(hvfhs_license_num) %&gt;%\n    summarize(avg = mean(month, na.rm = TRUE)) %&gt;%\n    print()})\n</code></pre> <pre><code>## # Source:   lazy query [?? x 2]\n## # Database: duckdb_connection\n##   hvfhs_license_num   avg\n##   &lt;chr&gt;             &lt;dbl&gt;\n## 1 HV0005             6.10\n\n##    user  system elapsed \n##  18.766   1.251   8.984\n</code></pre> <p>note that it is slightly faster, and we don\u2019t need to read as much data into memory because <code>duckdb</code> supports aggregating in a streaming fashion. This capability is very powerful because it allows us to perform computations on data that is too big to fit into memory.</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#your-turn","title":"Your turn!","text":"<p>Now that you understand some of the basic techniques for working with large data and have seen an example, you can start to apply what you\u2019ve learned. Using the same taxi data, try answering the following questions:</p> <ul> <li>What percentage of trips are made by Lyft?</li> <li>In which month did Lyft log the most trips?</li> </ul> <p>Documentation for these data can be found at https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf</p>","tags":["R","Large data"]},{"location":"tutorials/large_data_R/#additional-resources","title":"Additional resources","text":"<ul> <li>Arrow R package documentation</li> <li>Arrow Python package     documentation</li> <li>DuckDB documentation</li> </ul>","tags":["R","Large data"]},{"location":"tutorials/nlp_with_python/","title":"Natural Language Processing with Python","text":"<p>Elizabeth Piette, PhD MPH</p> <p>Statistician/Data Scientist</p> <p>Research Computing Services, DRFD, Harvard Business School</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#what-is-natural-language-processing-nlp","title":"What is natural language processing (NLP)?","text":"<p>NLP is a field that focuses on how computers can be used to process, analyze, and represent natural language.</p> <p>A natural language is one that has evolved organically, as opposed to a language that has been constructed intentionally (such as a programming language like Python, or an auxiliary language like Esperanto).</p> <p>Discussion: what makes natural language difficult to model?</p> <p>A funny example: https://www.ling.upenn.edu/~beatrice/humor/headlines.html</p> <p>Over time, NLP has shifted from hand-written rules to machine learning algorithms.</p> <p>Discussion: what are some of the advantages and disadvantages of rules-based versus machine learning approaches?</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#how-do-we-utilize-nlp-in-our-daily-lives","title":"How do we utilize NLP in our daily lives?","text":"<p>Basically whenever we use text or speech to interact with computers, phones, or smart devices:</p> <ul> <li>Search engines</li> <li>Question answering chat bots</li> <li>Autocorrect, autocomplete</li> <li>Virtual assistants</li> <li>Machine translation</li> </ul>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#our-motivating-problem","title":"Our motivating problem:","text":"<p>Your best friend\u2019s birthday is next week, and you want to give her something she\u2019ll really appreciate. You know she\u2019s read all of Emily Dickinson\u2019s poems and wishes there were more, so why not try writing a poem in Dickinson\u2019s style?</p> <p>After a few sad attempts, it\u2019s beginning to look like you\u2019re not much of a poet. But you have been developing your Python skills and want to learn more about NLP\u2026 perhaps you could generate a poem? This could be the perfect opportunity to make a thoughtful gift for your friend and develop some new skills at the same time!</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#acquiring-a-corpus","title":"Acquiring a corpus","text":"<p>This is one of the most important parts of an NLP project, and should be informed by your task.</p> <p>Some questions you may ask yourself before you assemble your corpus: * Should our corpus texts come from a specific domain? Words have different meanings in different contexts * Do the corpus documents consist of natural language, tables, lists, \u2026? How will I consolidate or separate these? Do I need to write custom parsers? * Does the text come from a database, word doc, pdf, web page, \u2026? Multiple sources? * Is there any meaningful text formatting - italics, bold, underline, \u2026? Should I utilize this, and how? * Do I need annotated data? How will I define the annotation guidelines, and who will perform the annotation?</p> <p>Discussion: what kind of corpora would we want for the applications above?</p> <p>Discussion: what would our ideal corpus look like?</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#loading-necessary-libraries","title":"Loading necessary libraries","text":"<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport unicodedata\nimport re\nimport nltk\nnltk.download('punkt')\n</code></pre> <pre><code>## True\n## \n## [nltk_data] Downloading package punkt to /home/izahn/nltk_data...\n## [nltk_data]   Package punkt is already up-to-date!\n</code></pre> <pre><code>nltk.download('averaged_perceptron_tagger')\n</code></pre> <pre><code>## True\n## \n## [nltk_data] Downloading package averaged_perceptron_tagger to\n## [nltk_data]     /home/izahn/nltk_data...\n## [nltk_data]   Package averaged_perceptron_tagger is already up-to-\n## [nltk_data]       date!\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nfrom IPython.display import Image\n</code></pre>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#scraping-text-from-the-web","title":"Scraping text from the web","text":"<p>We\u2019re going to start our project in the way that many NLP projects start - by scraping web pages.</p> <p>First, we\u2019ll compile a list of URLs for the pages we\u2019d like to scrape to assemble our corpus:</p> <ul> <li>Visit https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems</li> <li>Right-click a link in the table from the First Line column</li> <li>Click Inspect</li> <li>Links are defined in HTML using the a tag</li> <li>The URL addresses of the links we want are in the href attribute</li> <li>The class attribute distinguishes different categories of links</li> <li>Use requests to get the page text and BeautifulSoup to get     the URLs</li> </ul> <p>Requests documentation: https://requests.readthedocs.io/en/master/</p> <p>BeautifulSoup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/</p> <pre><code>dickinson_poems = 'https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems'\n\nr = requests.get(dickinson_poems)\n\nsoup = BeautifulSoup(r.text)\nprint(soup.prettify()[:500])\n</code></pre> <pre><code>## &lt;!DOCTYPE html&gt;\n## &lt;html class=\"client-nojs\" dir=\"ltr\" lang=\"en\"&gt;\n##  &lt;head&gt;\n##   &lt;meta charset=\"utf-8\"/&gt;\n##   &lt;title&gt;\n##    List of Emily Dickinson poems - Wikipedia\n##   &lt;/title&gt;\n##   &lt;script&gt;\n##    document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"a59c0\n</code></pre> <p>Now that we\u2019ve identified how links are represented, let\u2019s take all of these links and put them in a list. We can do this using a list comprehension:</p> <pre><code>links = [link.get('href') for link in soup.findAll('a', {'class': 'extiw'})]\n</code></pre> <p>It\u2019s always a good idea to look at your data as a sanity check, so let\u2019s peak at the first 10 and last 10 lines in our list of links:</p> <pre><code>links[:5]\n</code></pre> <pre><code>## ['https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage', 'https://en.wikisource.org/wiki/A_Bird_came_down_the_Walk_%E2%80%94', 'https://en.wikisource.org/wiki/A_Burdock_%E2%80%94_clawed_my_Gown_%E2%80%94', 'https://en.wikisource.org/wiki/A_Cap_of_Lead_across_the_sky', 'https://en.wikisource.org/wiki/A_Charm_invests_a_face']\n</code></pre> <pre><code>links[-5:]\n</code></pre> <pre><code>## ['https://en.wikisource.org/wiki/You%27re_right_%E2%80%94_%22the_way_is_narrow%22_%E2%80%94', 'https://en.wikisource.org/wiki/You%27ve_seen_Balloons_set_%E2%80%94_Haven%27t_You%3F', 'https://en.wikisource.org/wiki/Your_Riches_%E2%80%94_taught_me_%E2%80%94_Poverty.', 'https://en.wikisource.org/wiki/Your_thoughts_don%27t_have_words_every_day', 'https://foundation.wikimedia.org/wiki/Privacy_policy']\n</code></pre> <p>The last link isn\u2019t to a poem, so let\u2019s remove that:</p> <pre><code>del links[-1]\n</code></pre> <p>Next, we\u2019ll visit each of these pages and extract the poem text:</p> <ul> <li>Visit one of the links in the table, such as     https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage</li> <li>Right click the poem text and click Inspect</li> <li>Identify the HTML tags and attributes for the element containing the     poem</li> <li>Write a function to scrape our desired corpus using requests and     BeautifulSoup</li> </ul> <p>Here\u2019s the page content of the first link:</p> <pre><code>r = requests.get(links[0])\n\nsoup = BeautifulSoup(r.text)\n\nsoup.prettify()[:1000]\n</code></pre> <pre><code>## '&lt;!DOCTYPE html&gt;\\n&lt;html class=\"client-nojs\" dir=\"ltr\" lang=\"en\"&gt;\\n &lt;head&gt;\\n  &lt;meta charset=\"utf-8\"/&gt;\\n  &lt;title&gt;\\n   A Bee his burnished Carriage - Wikisource, the free online library\\n  &lt;/title&gt;\\n  &lt;script&gt;\\n   document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"f857c3cd-5fef-46a7-99c1-59fcb9b47551\",\"wgCSPNonce\":false,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"A_Bee_his_burnished_Carriage\",\"wgTitle\":\"A Bee his burnished Carriage\",\"wgCurRevisionId\":4327441,\"wgRevisionId\":4327441,\"wgArticleId\":13813,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"PD-old\"],\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"w'\n</code></pre> <p>Now that we\u2019ve found the appropriate tags associated with the poem, we can go ahead and grab the poem text:</p> <pre><code>poem = soup.find('div', {'class': 'poem'})\npoem.text\n</code></pre> <pre><code>## '\\nA Bee his burnished Carriage\\nDrove boldly to a Rose \u2014\\nCombinedly alighting \u2014\\nHimself \u2014 his Carriage was \u2014\\nThe Rose received his visit\\nWith frank tranquillity\\nWithholding not a Crescent\\nTo his Cupidity \u2014\\nTheir Moment consummated \u2014\\nRemained for him \u2014 to flee \u2014\\nRemained for her \u2014 of rapture\\nBut the humility.\\n\\n'\n</code></pre> <p>The following cell combines the steps we performed above into a function that returns the scraped poems from the list of URLs. We\u2019ll skip calling this in the interest of time.</p> <pre><code>def scrape_corpus(corpus_URLs: list) -&gt; list:\n\n    \"\"\"\n    Takes our list of URLS\n    Returns a list of the poems scraped from those pages\n    \"\"\"\n\n    corpus = []\n\n    for link in corpus_URLs:\n        r = requests.get(link)\n        soup = BeautifulSoup(r.text)\n        poem = soup.find('div', {'class': 'poem'})\n\n        if poem is not None:\n            corpus.append(poem.text)\n\n    return [unicodedata.normalize('NFKD', poem) for poem in corpus]\n\ncorpus_texts = scrape_corpus(links)\n</code></pre> <p>For those interested, this is how I saved the corpus:</p> <pre><code>joined_corpus = '\\p'.join(corpus_texts)\n\nwith open('corpus.txt', 'w', encoding='utf-8') as f:\n    f.write(joined_corpus)\n</code></pre> <p>Note that this doesn\u2019t do a perfect job of extracting the content we want - for example, some of the text we extracted has multiple versions of a given poem. Encountering inconsistencies in page HTML formatting is very common in web scraping tasks. You can perform some manual or automated checks and cleaning as desired/as is feasible. This is generally an iterative process.</p> <p>Now we\u2019ll just load the corpus I saved prior to this workshop:</p> <pre><code>with open('data/corpus.txt', 'r', encoding='utf-8') as f:\n    corpus_texts = f.read()\n\ncorpus_texts = corpus_texts.split('\\p')\n\ncorpus_texts[0]\n</code></pre> <pre><code>## '\\nA Bee his burnished Carriage\\nDrove boldly to a Rose \u2014\\nCombinedly alighting \u2014\\nHimself \u2014 his Carriage was \u2014\\nThe Rose received his visit\\nWith frank tranquillity\\nWithholding not a Crescent\\nTo his Cupidity \u2014\\nTheir Moment consummated \u2014\\nRemained for him \u2014 to flee \u2014\\nRemained for her \u2014 of rapture\\nBut the humility.\\n\\n'\n</code></pre> <pre><code>len(corpus_texts)\n</code></pre> <pre><code>## 1763\n</code></pre>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#pre-processing","title":"Pre-processing","text":"<p>Garbage in, garbage out</p> <p>Pre-processing is going to turn the raw text we scraped from the web into data.</p> <p>Basic preprocessing often includes: * Tokenization * Removing capitalization * Stripping away tags * Removing accents/special characters * Removing punctuation * Removing/normalizing whitespace * Stemming/lemmatization * Removing stopwords</p> <p>Once again, the way in which you pre-process your data should be informed by your final application - there isn\u2019t a one-size-fits-all approach. For example: * Capitalization may be informative (Apple vs.\u00a0apple) * HTML tags may provide structural information/metadata we can use as features or to label data * Sentence tokenization may need to be performed differently for different types of texts, eg in social media texts there may be punctuation missing from the ends of sentences, but additional punctuation forming emojis :) * Word tokenization may also depend on the source text, eg in text formatted for newspapers there may be hyphens separating words between syllables across lines due to column formatting</p> <p>Discussion: what types of pre-processing do you think would be more appropriate for our application?</p> <p>For this workshop, we\u2019re going to lowercase all text, remove all punctuation, and perform line and word tokenization. Let\u2019s start by investigating the output each of these types of pre-processing separately.</p> <p>We can make our text lowercase using .lower(), which is a built in String method in Python. For example, let\u2019s lower everything in the first poem in our corpus:</p> <pre><code>corpus_texts[0].lower()\n</code></pre> <pre><code>## '\\na bee his burnished carriage\\ndrove boldly to a rose \u2014\\ncombinedly alighting \u2014\\nhimself \u2014 his carriage was \u2014\\nthe rose received his visit\\nwith frank tranquillity\\nwithholding not a crescent\\nto his cupidity \u2014\\ntheir moment consummated \u2014\\nremained for him \u2014 to flee \u2014\\nremained for her \u2014 of rapture\\nbut the humility.\\n\\n'\n</code></pre> <p>We can remove punctuation using a regular expression. It\u2019s okay if you\u2019re not familiar with regular expressions yet, but you may want to learn more about them as you go forth in your NLP journey since they allow you to specify text search patterns.</p> <p>The code below finds all characters in our specified text (in this case, corpus_texts[0], the first poem) that are not \u201cword characters\u201d or whitespace, and substitutes them with an empty string.</p> <pre><code>re.sub(r'[^\\w\\s]','', corpus_texts[0])\n</code></pre> <pre><code>## '\\nA Bee his burnished Carriage\\nDrove boldly to a Rose \\nCombinedly alighting \\nHimself  his Carriage was \\nThe Rose received his visit\\nWith frank tranquillity\\nWithholding not a Crescent\\nTo his Cupidity \\nTheir Moment consummated \\nRemained for him  to flee \\nRemained for her  of rapture\\nBut the humility\\n\\n'\n</code></pre> <p>NLTK is going to become your favorite tool, and is accompanied by an excellent book that you can work through to expand your repertoire far beyond what we can cover in the scope of this workshop. * https://www.nltk.org/ * https://www.nltk.org/book/</p> <p>NLTK has a built in sentence tokenizer, but since our poems don\u2019t consist of sentences in the traditional sense, we can perform our own line tokenization by splitting on newlines:</p> <pre><code>corpus_texts[0].split('\\n')\n</code></pre> <pre><code>## ['', 'A Bee his burnished Carriage', 'Drove boldly to a Rose \u2014', 'Combinedly alighting \u2014', 'Himself \u2014 his Carriage was \u2014', 'The Rose received his visit', 'With frank tranquillity', 'Withholding not a Crescent', 'To his Cupidity \u2014', 'Their Moment consummated \u2014', 'Remained for him \u2014 to flee \u2014', 'Remained for her \u2014 of rapture', 'But the humility.', '', '']\n</code></pre> <p>We can perform word tokenization using the NLTK word tokenizer:</p> <pre><code>from nltk import word_tokenize\nword_tokenize(corpus_texts[0][:20])\n</code></pre> <pre><code>## ['A', 'Bee', 'his', 'burnished']\n</code></pre> <p>Now, let\u2019s write a function that combines our four preprocessing steps.</p> <pre><code>#Example solution:\n\ndef preprocess_poem(poem: str) -&gt; list:\n\n    \"\"\"\n    Takes a poem (as a single string)\n    Returns the poem lowercased, with punctuation removed, line and word tokenized\n    (as a list of lists, where each line is a list of words)\n    \"\"\"\n\n    poem = poem.lower()\n    poem = re.sub(r'[^\\w\\s]','', poem)\n    poem = poem.split('\\n')\n    poem = [word_tokenize(line) for line in poem]\n\n    return poem\n</code></pre> <p>Writing docstrings makes life easier for others reading, reusing, and modifying your code (including future-you)</p> <pre><code>help(preprocess_poem)\n</code></pre> <pre><code>## Help on function preprocess_poem in module __main__:\n## \n## preprocess_poem(poem: str) -&gt; list\n##     Takes a poem (as a single string)\n##     Returns the poem lowercased, with punctuation removed, line and word tokenized\n##     (as a list of lists, where each line is a list of words)\n</code></pre> <pre><code>preprocess_poem(corpus_texts[0])\n</code></pre> <pre><code>## [[], ['a', 'bee', 'his', 'burnished', 'carriage'], ['drove', 'boldly', 'to', 'a', 'rose'], ['combinedly', 'alighting'], ['himself', 'his', 'carriage', 'was'], ['the', 'rose', 'received', 'his', 'visit'], ['with', 'frank', 'tranquillity'], ['withholding', 'not', 'a', 'crescent'], ['to', 'his', 'cupidity'], ['their', 'moment', 'consummated'], ['remained', 'for', 'him', 'to', 'flee'], ['remained', 'for', 'her', 'of', 'rapture'], ['but', 'the', 'humility'], [], []]\n</code></pre> <p>Looks pretty good! But let\u2019s also remove these extraneous empty lines:</p> <pre><code>def preprocess_poem(poem: str) -&gt; list:\n\n    \"\"\"\n    Takes a poem (as a single string)\n    Returns the poem lowercased, with punctuation removed, line and word tokenized\n    (as a list of lists, where each line of the poem is a list of words)\n    \"\"\"\n\n    poem = poem.lower()\n    poem = re.sub(r'[^\\w\\s]','', poem)\n    poem = poem.split('\\n')\n    poem = [word_tokenize(line) for line in poem if line]\n\n    return poem\n</code></pre> <pre><code>preprocess_poem(corpus_texts[0])\n</code></pre> <pre><code>## [['a', 'bee', 'his', 'burnished', 'carriage'], ['drove', 'boldly', 'to', 'a', 'rose'], ['combinedly', 'alighting'], ['himself', 'his', 'carriage', 'was'], ['the', 'rose', 'received', 'his', 'visit'], ['with', 'frank', 'tranquillity'], ['withholding', 'not', 'a', 'crescent'], ['to', 'his', 'cupidity'], ['their', 'moment', 'consummated'], ['remained', 'for', 'him', 'to', 'flee'], ['remained', 'for', 'her', 'of', 'rapture'], ['but', 'the', 'humility']]\n</code></pre> <p>Great! Now let\u2019s apply our preprocessing to the entire corpus:</p> <pre><code>preprocessed_poems = [preprocess_poem(poem) for poem in corpus_texts]\n</code></pre>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#exploring-our-data","title":"Exploring our data","text":"<p>Now that we\u2019ve performed some pre-processing, we can more readily explore our data. In order get some distributional statistics, it\u2019ll make our lives easier to first join the entire pre-processed corpus into one long list:</p> <pre><code>joined_lines = [line for poem in preprocessed_poems for line in poem]\njoined_corpus = [word for line in joined_lines for word in line]\n</code></pre> <pre><code>joined_corpus[:10]\n</code></pre> <pre><code>## ['a', 'bee', 'his', 'burnished', 'carriage', 'drove', 'boldly', 'to', 'a', 'rose']\n</code></pre> <p>Now we can do some basic counts:</p> <pre><code>print('There are {} total words in the corpus'.format(len(joined_corpus)))\n</code></pre> <pre><code>## There are 94163 total words in the corpus\n</code></pre> <pre><code>print('There are {} unique words in the corpus'.format(len(set(joined_corpus))))\n</code></pre> <pre><code>## There are 10733 unique words in the corpus\n</code></pre> <p>We can also use NLTK to look at word frequencies. Let\u2019s take a look at the 10 most common words in our corpus:</p> <pre><code>from nltk.probability import FreqDist\n\nfdist = FreqDist(joined_corpus)\n\nfdist.most_common(10)\n</code></pre> <pre><code>## [('the', 5876), ('a', 2581), ('and', 2291), ('to', 2267), ('of', 1901), ('i', 1653), ('that', 1234), ('it', 1220), ('is', 1186), ('in', 1148)]\n</code></pre> <p>The most common words in the corpus are not very exciting\u2026</p> <pre><code>from nltk.corpus import stopwords\nnltk.download('stopwords')\n</code></pre> <pre><code>## True\n## \n## [nltk_data] Downloading package stopwords to /home/izahn/nltk_data...\n## [nltk_data]   Package stopwords is already up-to-date!\n</code></pre> <pre><code>english_stopwords = nltk.corpus.stopwords.words('english')\nenglish_stopwords[:10]\n</code></pre> <pre><code>## ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n</code></pre> <p>Let\u2019s write a function to remove stopwords:</p> <pre><code>def remove_nltk_stopwords(tokenized_corpus: list, stopwords: list) -&gt; list:\n\n    \"\"\"\n    Takes a list of tokens and a list of stopwords\n    Returns the list of tokens with stopwords removed\n    \"\"\"\n\n    return [token for token in tokenized_corpus if token not in stopwords]\n</code></pre> <pre><code>joined_corpus_stopwords_removed = remove_nltk_stopwords(joined_corpus, english_stopwords)\n\nfdist = FreqDist(joined_corpus_stopwords_removed)\n\nfdist.most_common(10)\n</code></pre> <pre><code>## [('like', 329), ('one', 322), ('upon', 280), ('could', 255), ('would', 254), ('day', 227), ('little', 224), ('know', 221), ('thee', 217), ('away', 211)]\n</code></pre> <p>These sound a bit more interesting to me!</p> <p>We can also plot word frequencies:</p> <pre><code>fdist.plot(30)\n</code></pre> <p></p> <p>Let\u2019s move on from single words to explore combinations of words - we can use NLTK for this as well:</p> <pre><code>bigrams_list = list(nltk.bigrams(joined_corpus))\nfdist = FreqDist(bigrams_list)\nfdist.most_common(10)\n</code></pre> <pre><code>## [(('in', 'the'), 310), (('of', 'the'), 289), (('to', 'the'), 153), (('is', 'the'), 150), (('upon', 'the'), 118), (('the', 'sun'), 117), (('can', 'not'), 114), (('to', 'be'), 109), (('for', 'the'), 107), (('all', 'the'), 104)]\n</code></pre> <p>Once again, these are pretty boring\u2026 let\u2019s try again, with our corpus with stopwords removed:</p> <pre><code>bigrams_list_stopwords_removed = list(nltk.bigrams(joined_corpus_stopwords_removed))\nfdist = FreqDist(bigrams_list_stopwords_removed)\nfdist.most_common(10)\n</code></pre> <pre><code>## [(('every', 'day'), 19), (('human', 'nature'), 12), (('could', 'see'), 11), (('put', 'away'), 11), (('thou', 'art'), 11), (('let', 'go'), 10), (('hast', 'thou'), 9), (('old', 'fashioned'), 9), (('could', 'find'), 8), (('let', 'us'), 8)]\n</code></pre> <p>These are much more poetic!</p> <pre><code>fdist.plot(30)\n</code></pre> <p></p> <p>Perhaps we\u2019re also interested in structural elements of our corpus poems, like the lines per poem and words per line. Let\u2019s write functions to count both of these:</p> <pre><code>def count_lines_per_poem(line_word_tokenized_corpus: list) -&gt; list:\n\n    \"\"\"\n    Takes a list of line and word tokenized poems (list of lists of strings)\n    Returns a list of ints with the number of lines in each poem\n    \"\"\"\n\n    return [len(poem) for poem in line_word_tokenized_corpus]\n</code></pre> <pre><code>lines_per_poem = count_lines_per_poem(preprocessed_poems)\npd.Series(lines_per_poem).plot(kind = 'hist', bins=50)\n</code></pre> <p></p> <pre><code>def count_words_per_line(line_word_tokenized_corpus: list) -&gt; list:\n\n    \"\"\"\n    Takes a list of line and word tokenized poems (list of lists of strings)\n    Returns a list of ints with the number of words in each line\n    \"\"\"\n\n    return [len(line) for poem in line_word_tokenized_corpus for line in poem]\n</code></pre> <pre><code>words_per_line = count_words_per_line(preprocessed_poems)\npd.Series(words_per_line).plot(kind = 'hist', bins=50)\n</code></pre> <p></p> <p>We can also look at what parts of speech are represented in our corpus:</p> <pre><code>from nltk import pos_tag\npos_tag(joined_corpus[:10])\n</code></pre> <pre><code>## [('a', 'DT'), ('bee', 'NN'), ('his', 'PRP$'), ('burnished', 'JJ'), ('carriage', 'NN'), ('drove', 'VBD'), ('boldly', 'RB'), ('to', 'TO'), ('a', 'DT'), ('rose', 'VBD')]\n</code></pre> <p>Let\u2019s make a dictionary of words keyed by their part of speech, and vice versa - maybe these could be useful later?</p> <pre><code>corpus_pos_tagged = pos_tag(joined_corpus)\n</code></pre> <pre><code>word_to_pos = {}\n\nfor i in range(len(corpus_pos_tagged)):\n    if corpus_pos_tagged[i][0] in word_to_pos:\n        if corpus_pos_tagged[i][1] not in word_to_pos[corpus_pos_tagged[i][0]]:\n            word_to_pos[corpus_pos_tagged[i][0]].append(corpus_pos_tagged[i][1])\n    else:\n        word_to_pos[corpus_pos_tagged[i][0]] = [corpus_pos_tagged[i][1]]\n</code></pre> <p>Words can be tagged as different parts of speech in different contexts:</p> <pre><code>word_to_pos['walk']\n</code></pre> <pre><code>## ['NN', 'VBP', 'VB']\n</code></pre> <pre><code>pos_to_word = {}\ncorpus_pos_tagged_reversed = [(word, pos) for pos, word in corpus_pos_tagged]\n\nfor i in range(len(corpus_pos_tagged_reversed)):\n    if corpus_pos_tagged_reversed[i][0] in pos_to_word:\n        if corpus_pos_tagged_reversed[i][1] not in pos_to_word[corpus_pos_tagged_reversed[i][0]]:\n            pos_to_word[corpus_pos_tagged_reversed[i][0]].append(corpus_pos_tagged_reversed[i][1])\n    else:\n        pos_to_word[corpus_pos_tagged_reversed[i][0]] = [corpus_pos_tagged_reversed[i][1]]\n\n#you could write a generic function to make either a word_to_pos or pos_to_word dictionary...\n</code></pre> <pre><code>pos_to_word['NN'][:10]\n</code></pre> <pre><code>## ['bee', 'carriage', 'visit', 'tranquillity', 'crescent', 'cupidity', 'moment', 'rapture', 'humility', 'bird']\n</code></pre> <p>NLTK also has a nice function that lets you look at words that are \u2018similar\u2019 to a given word, based on context:</p> <pre><code>joined_Text = nltk.Text(joined_corpus) #it's necessary to convert to a nltk.text.Text object\njoined_Text.similar('bee')\n</code></pre> <pre><code>## sun day bird house grave sea world rose life soul heart sky face first\n## mind light way wind hand dust\n</code></pre> <p>It might also be quite useful to make a dictionary associating words that appear in similar contexts\u2026 but I\u2019ll leave that to you.</p> <p>Discussion: what other statistics and visualizations would help us better understand our data?</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#our-first-poetry-generator","title":"Our first poetry generator","text":"<p>Now let\u2019s combine our knowledge of the distribution of words in the corpus and the structure of the documents to make our first poem generator! NLTK has many built-in functions that we can use, so we\u2019ll just take advantage of those. First, we have to create a Vocabulary from our joined corpus:</p> <pre><code>from nltk.lm import Vocabulary\nvocab = Vocabulary(joined_corpus, unk_cutoff=2)\nvocab['bee']\n</code></pre> <pre><code>## 83\n</code></pre> <p>Next, we\u2019ll pad the ends of each line with special tokens that do not appear elsewhere in our corpus, and format our text into bigrams:</p> <pre><code>from nltk.lm.preprocessing import pad_both_ends\npreprocessed_poems_lines_joined = [line for poem in preprocessed_poems for line in poem]\npadded_text = [list(nltk.bigrams(pad_both_ends(line, n=2))) for line in preprocessed_poems_lines_joined]\npadded_text[0]\n</code></pre> <pre><code>## [('&lt;s&gt;', 'a'), ('a', 'bee'), ('bee', 'his'), ('his', 'burnished'), ('burnished', 'carriage'), ('carriage', '&lt;/s&gt;')]\n</code></pre> <p>This command will actually perform these steps for us at once (but it was good of us to look at the output of each):</p> <pre><code>from nltk.lm.preprocessing import padded_everygram_pipeline\ntrain, vocab = padded_everygram_pipeline(2, preprocessed_poems_lines_joined)\n</code></pre> <p>We\u2019ll now fit an MLE model:</p> <pre><code>from nltk.lm import MLE\nlm = MLE(2)\nlm.fit(train, vocab)\n</code></pre> <p>We can use this model to write a basic function to generate poetry:</p> <pre><code>def generate_poem() -&gt; list:\n\n    \"\"\"\n    Randomly select a number of lines from the distribution of lines per poem\n    For each line:\n    - randomly select a number of words from the distribution of words per line\n    - use the MLE model to generate a line containing the desired number of words\n    - replace end of sentence tags with dashes for ~flair~\n    \"\"\"\n\n    generated_poem = []\n    num_lines = random.choice(lines_per_poem)\n\n    for line in range(num_lines):\n        num_words = random.choice(words_per_line)\n\n        generated_line = re.sub('&lt;/s&gt;', '-', re.sub(' +', ' ', ' '.join(re.split('&lt;s&gt;', ' '.join(lm.generate(num_words)))))).strip()\n\n        generated_poem.append(generated_line)\n\n    return generated_poem\n</code></pre> <pre><code>generate_poem()\n</code></pre> <pre><code>## ['the latter is difficult the strain', 'i cried give her castle', 'i would fly - hoped', 'sea - the son']\n</code></pre> <p>Of course, this is a pretty lazy model, with many things we could improve. Instead of randomly generating the number of lines and words per line, we may want to directly use the structure of a poem in the corpus. Or we might chose to perform our preprocessing in a different manner so we don\u2019t lose and subsequently artificially reinsert dashes or exclamation points. We could go on, and make many incremental improvements.</p> <p>Discussion: what additional rules could improve our poetry generator?</p> <p>One of the most critical flaws of our model is that it is unidirectional. Which leads us to\u2026</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#our-second-poetry-generator","title":"Our second poetry generator","text":"<p>Artificial neural networks are inspired by and structurally somewhat analogous to biological neural networks.</p> <p>Recurrent neural networks (RNNs) are particularly suited to sequential data (like text):</p> <p></p> <p>From http://www.deeplearningbook.org/contents/rnn.html</p> <p>The following code comes with very slight modifications from \u201cMinimal character-level Vanilla RNN model\u201d by Andrej Karpathy (@karpathy): * https://gist.github.com/karpathy/d4dee566867f8291f086 * http://karpathy.github.io/2015/05/21/rnn-effectiveness/</p> <p>I really love this example because it\u2019s on the more approachable side for a beginner, it\u2019s convenient for the purposes of this workshop since we don\u2019t have time to explore Tensorflow/Keras, and it\u2019s informative since we can read through each chunk of code. However, please don\u2019t feel as if you have to fully understand the content of each of the following code cells - we\u2019re just going to focus on concepts.</p> <p>Let\u2019s start by inputting our raw corpus without any pre-processing:</p> <pre><code>def data_input(corpus: list):\n\n    \"\"\"\n    Takes a corpus as a list of strings\n    Returns the data prepared to be used in a word-based model (rather than char-based model used by Andrej Karpathy)\n    \"\"\"\n\n    data = '\\n'.join(corpus)\n    data = re.split('(\\W)', data)\n    words = list(set(data))\n    vocab_size = len(words)\n\n    return data, words, vocab_size\n</code></pre> <pre><code>data, words, vocab_size = data_input(corpus_texts)\n\nprint(data[:10])\n</code></pre> <pre><code>## ['', '\\n', 'A', ' ', 'Bee', ' ', 'his', ' ', 'burnished', ' ']\n</code></pre> <pre><code>print(words[:10])\n</code></pre> <pre><code>## ['', 'Peru', 'mother', 'Enemy', 'Part', 'Parts', 'balancing', 'vanity', 'gushes', 'Wardrobe']\n</code></pre> <pre><code>print(vocab_size)\n</code></pre> <pre><code>## 13695\n</code></pre> <p>Next we\u2019ll create dictionaries that map words to IDs, and IDs to words:</p> <pre><code>def gen_mapping(words_list: list) -&gt; list:\n\n    \"\"\"\n    Takes the prepared corpus data\n    Returns dictionaries mapping the words to IDs, and IDs to words\n    \"\"\"\n\n    word_to_ix = {ch: i for i, ch in enumerate(words_list)}\n    ix_to_word = {i: ch for i, ch in enumerate(words_list)}\n\n    return word_to_ix, ix_to_word\n</code></pre> <pre><code>word_to_ix, ix_to_word = gen_mapping(words)\n\nprint(word_to_ix['bee'])\n</code></pre> <pre><code>## 6202\n</code></pre> <pre><code>print(ix_to_word[100])\n</code></pre> <pre><code>## consecrate\n</code></pre> <p>Now we\u2019ll set our hyperparameters, which are defined prior to running the algorithm (as opposed to the model parameters, which are iteratively updated). The learning rate is probably the most important hyperparameter to focus on, since it controls how much the weights are updated with each iteration. A larger learning rate converges more quickly (perhaps overshooting minima), whereas a smaller learning rate converges more slowly (and may become stuck in a local minima).</p> <pre><code>def set_hyperparams(hidden_size: int = 100, seq_length: int = 25, learning_rate: int = 1e-1) -&gt; int:\n\n    \"\"\"\n    Sets hyperparameters, defaulting to Andrej Karpathy's values\n    \"\"\"\n\n    return hidden_size, seq_length, learning_rate\n</code></pre> <pre><code>hidden_size, seq_length, learning_rate = set_hyperparams()\n\nprint(hidden_size)\n</code></pre> <pre><code>## 100\n</code></pre> <pre><code>print(seq_length)\n</code></pre> <pre><code>## 25\n</code></pre> <pre><code>print(learning_rate)\n</code></pre> <pre><code>## 0.1\n</code></pre> <p>Next we\u2019ll initialize placeholders for our model parameters:</p> <pre><code>def set_model_params(hidden_size: int, vocab_size: int):\n\n    \"\"\"\n    Sets model parameters based on hyperparameters and vocab size\n    \"\"\"\n\n    Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n    Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n    Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n    bh = np.zeros((hidden_size, 1))  # hidden bias\n    by = np.zeros((vocab_size, 1))  # output bias\n\n    return Wxh, Whh, Why, bh, by\n</code></pre> <pre><code>Wxh, Whh, Why, bh, by = set_model_params(hidden_size, vocab_size)\n\nprint(Wxh)\n</code></pre> <pre><code>## [[ 1.07318817e-03  2.99033756e-03  1.31159605e-02 ...  2.20105149e-03\n##   -3.80494428e-04  3.97301708e-03]\n##  [ 1.56696343e-02 -1.15619302e-02 -1.86478759e-02 ...  6.04892251e-03\n##    1.98045140e-03  4.12423613e-03]\n##  [-3.97598434e-03 -6.62507965e-03  2.29918440e-03 ... -1.44603155e-03\n##    2.75347720e-03  3.00119575e-03]\n##  ...\n##  [ 8.61543861e-03  6.18875571e-03  1.40664915e-03 ... -1.21927158e-02\n##    7.33542826e-03 -1.01914743e-02]\n##  [ 5.74131920e-03  6.95976654e-03  7.49904548e-05 ... -9.71417927e-03\n##   -7.51565124e-04 -4.48596981e-03]\n##  [-7.50688337e-03 -1.44724997e-02  1.21272185e-02 ... -8.07159000e-03\n##    6.78215010e-03  1.14515049e-02]]\n</code></pre> <pre><code>print(Whh)\n</code></pre> <pre><code>## [[-0.00170988 -0.00297927 -0.01154336 ... -0.00550237  0.00019349\n##   -0.01389495]\n##  [ 0.01729352 -0.01719077  0.00417469 ... -0.01540379 -0.01943895\n##   -0.00840091]\n##  [-0.00226139  0.00539692  0.00793333 ...  0.01228179  0.00262746\n##   -0.01788651]\n##  ...\n##  [-0.0081638   0.00851257  0.00806451 ...  0.00586944 -0.00861062\n##   -0.00635998]\n##  [ 0.00357786 -0.00291397 -0.01052795 ...  0.01750125  0.00399255\n##    0.00268263]\n##  [ 0.00368808  0.00159308 -0.00557169 ... -0.0060339  -0.00162449\n##   -0.00666031]]\n</code></pre> <pre><code>print(Why)\n</code></pre> <pre><code>## [[ 0.01132326 -0.01264148  0.00186793 ...  0.00027127 -0.01236483\n##   -0.00700431]\n##  [-0.00894894 -0.0032734   0.01492165 ...  0.00464066 -0.01345284\n##    0.00064576]\n##  [ 0.01203935  0.00684551  0.01899521 ... -0.00711966  0.00542538\n##    0.00644606]\n##  ...\n##  [-0.01385628 -0.01017521 -0.00882153 ... -0.00630009  0.00578206\n##   -0.01508209]\n##  [-0.00078822 -0.01597705  0.00242087 ...  0.00016081 -0.0042986\n##   -0.00981912]\n##  [ 0.0100233  -0.00639412  0.00168985 ...  0.00565186  0.00616837\n##   -0.0032813 ]]\n</code></pre> <pre><code>print(bh)\n</code></pre> <pre><code>## [[0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]\n##  [0.]]\n</code></pre> <pre><code>print(by)\n</code></pre> <pre><code>## [[0.]\n##  [0.]\n##  [0.]\n##  ...\n##  [0.]\n##  [0.]\n##  [0.]]\n</code></pre> <p>We then perform the forward pass, update the loss, and perform the backward pass:</p> <pre><code>def lossFun(inputs, targets, hprev):\n\n    \"\"\"\n    inputs,targets are both list of integers.\n    hprev is Hx1 array of initial hidden state\n    returns the loss, gradients on model parameters, and last hidden state\n    \"\"\"\n\n    xs, hs, ys, ps = {}, {}, {}, {}\n    hs[-1] = np.copy(hprev)\n    loss = 0\n    # forward pass\n    for t in range(len(inputs)):\n        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n        xs[t][inputs[t]] = 1\n        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)  # hidden state\n        ys[t] = np.dot(Why, hs[t]) + by  # unnormalized log probabilities for next chars\n        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\n        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n    # backward pass: compute gradients going backwards\n    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n    dhnext = np.zeros_like(hs[0])\n    for t in reversed(range(len(inputs))):\n        dy = np.copy(ps[t])\n        dy[targets[\n            t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n        dWhy += np.dot(dy, hs[t].T)\n        dby += dy\n        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n        dbh += dhraw\n        dWxh += np.dot(dhraw, xs[t].T)\n        dWhh += np.dot(dhraw, hs[t - 1].T)\n        dhnext = np.dot(Whh.T, dhraw)\n    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\n</code></pre> <p>This function returns a sample from the model at a given time:</p> <pre><code>def sample(h, seed_ix, n):\n\n    \"\"\"\n    sample a sequence of integers from the model\n    h is memory state, seed_ix is seed letter for first time step\n    \"\"\"\n\n    x = np.zeros((vocab_size, 1))\n    x[seed_ix] = 1\n    ixes = []\n    for t in range(n):\n        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n        y = np.dot(Why, h) + by\n        p = np.exp(y) / np.sum(np.exp(y))\n        ix = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((vocab_size, 1))\n        x[ix] = 1\n        ixes.append(ix)\n    return ixes\n</code></pre> <p>And finally we have a function that iterates through the model and periodically returns a sample:</p> <pre><code>def iterate_and_sample(sample_length: int = 200, sample_iters: int = 100, max_iters: int = 1000):\n\n    \"\"\"\n    Putting it all together:\n    Prints a model sample of a given length (default 200)\n    at a given number of iterations (default 100)\n    up to a maximum number of iterations (default 1000)\n    \"\"\"\n\n    n, p = 0, 0\n    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n    mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n    smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n    while n &lt;= max_iters:\n        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n        if p + seq_length + 1 &gt;= len(data) or n == 0:\n            hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n            p = 0  # go from start of data\n        inputs = [word_to_ix[ch] for ch in data[p:p + seq_length]]\n        targets = [word_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n\n        # sample from the model now and then\n        if n % sample_iters == 0:\n            sample_ix = sample(hprev, inputs[0], sample_length)\n            txt = ''.join(ix_to_word[ix] for ix in sample_ix)\n            print('----\\n %s \\n----' % (txt,))\n\n        # forward seq_length characters through the net and fetch gradient\n        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n        if n % sample_iters == 0: print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n\n        # perform parameter update with Adagrad\n        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n                                      [dWxh, dWhh, dWhy, dbh, dby],\n                                      [mWxh, mWhh, mWhy, mbh, mby]):\n            mem += dparam * dparam\n            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n\n        p += seq_length  # move data pointer\n        n += 1  # iteration counter\n</code></pre> <p>Here\u2019s our entire pipline put together:</p> <pre><code>#load data:\ndata, words, vocab_size = data_input(corpus_texts)\n\n#encode data:\nword_to_ix, ix_to_word = gen_mapping(words)\n\n#set hyperparameters\nhidden_size, seq_length, learning_rate = set_hyperparams()\n\n#set model parameters\nWxh, Whh, Why, bh, by = set_model_params(hidden_size, vocab_size)\n\n#return model samples\niterate_and_sample()\n</code></pre> <pre><code>## ----\n##  SeptembertherefromLawsdrinkeasiestJohnsonpersonallyTurkchancedPumpkinsEnemiesWaresAstralhedgeThybafflesGivenhugestpurposelessloweredGoreddirtypublicItalygrassmoreOppositesraggedfainterBabblersGainedSetfantasticbrewAxlewouldnDrummerRegardlessfolddailyexcludeArgueswilthungovergrownawayrudefleeterInsolvencyrealwritLutesMrflourishinghurryingjoinsHeartyOrchardInsideSurelyuniqueTherelimbspalliateBisecteddetectAidTkneeimportantnearnessFeuneventrustingburialtransfiguredassistanceReprieveboilingstirsthemselvesplaythingplunderSustainPomposityindorsedadjustprovedRaggedMessageremoterSyllablesSyndicateRebukedisenchantsvalesteamunthinkingBegoneunladedDefinitionScarcermissingcandidPardonBullionTidessmitHorrorprotractsTearRallieshollowunsteadyRetrospectsSlippedwhimpersPreciselyparadiseSilkenclearAwfulDecaysnaturaldelightCarryshowedrememberedAccuseCorrodelessGentlestdisputedearthMalignitysailSoundlessSound\u200bCaptiveRowerApartmentsAchieveGustsHareingotsstrictestoursHemsdaresbrieflyWaggonsAdmirationsarrangetrycarvedveilodiouserimportunedsafestshopsfledEmployedCommonestRegardgranderarrogantlyPleiadBalladsCherishTribunalMoundCasqueButtercupFeewhobuildRepugnantParlorsclovenfinishswoonbelievesresolutercommandAweCourserspoolBethleemrequisiteSankAcutemissesstillnessexceptRememberingmushroombees3meritedPrithee \n## ----\n## iter 0, loss: 238.119642\n## ----\n##   . That pronoun\u2014Day Mirage the revelation.Figures bleeds \n## makes Your Mirage merciful Yet\n## That too\n## the\n## Subsides And suspends That Unreality.conferring unknown ThatIA And Lady\n##  lives Date suspends sipped suspends the .' Makes.\n## livesshoeAnd lives That\n## a I A That Mirage  lives by suspends the makes\n## And\n## lives.sufficed suspends  across relief\u2014makes \n## lives,Day A  suspends\u2014\n## makes not Superior merciful lives Instructs\n## Unreality possible The. lives possible  merciful suspendsGrandmama Informing That living\n## if Veil merciful makes\u2014Withholding makes. suspends\n## Clock lives. \n## ----\n## iter 100, loss: 231.338567\n## ----\n##   That\n## exists\n##  low Dimple cannot Scientist waking stands\n## , Diamond\n## At any Spring\n## lane\n## Irresolute on Paradise Of present out fellow Ocean exists signed,\n## Ores present Minnows\n## \n## \n## A any\n## And opened exists\u2014\n## the Gig\n## \u2014cunning present is Stipulus Horns\n## present on nail,\n## Spring dawn Spring Toils Spring The present\n## s present the\n## BritishWhoNot Spring\n## the Spring  At the \u2014is \n## any exists in\n## The Vocal\n## Spring,any to how,\u2014 \u2014\n## charms\n## House Superior any Dead Not Carpenter\u2014\n## true\n## Light doesn Year\n## atom Yours replaced\n## period Spring \n## ----\n## iter 200, loss: 219.020527\n## ----\n##   Mine go'Ours go Unto done'Nest go Mine Lasts Mine go Mine \u2014\n## to Mine go'\n## \u2014 too hateful go Trees Demeaning'fear go'Mine led A go\u2014 go'Mine go Mine Elephant hide go me go Mine go'despise go Mine go\u2014Whose miss'the go Universe go Mine go\n##  full Mine the'oppositely go Mine go'Mine go'Mine go \u2014\n##  go!So Half could go'Mine\n## go'mind go Mine go'Mine go'Mine go Mine go Mine raps'Bog go'promised go and Bushes'\n## go them go \n## ----\n## iter 300, loss: 207.297738\n## ----\n##  passeach\u2014 gathered stated \u2014The\n## beg\u2014 \u2014\u2014\u2014\u2014\u2014 The Kinsmanship \u2014\n## \n##  appointed it\n## a BeeBut\n## ofletsinewsConstellation \u2014The so \u2014a in  Ihis\u2014 \u2014 or \u2014Eaves beg \u2014A food the   Beatrice\u2014And Man Health\n##   .has  Seller food \u2014 \u2014 \n## closing \n## in food dispelled to Axis extinct were Beamcunningwith and He It \u2014\u2014\n## \n## \n##  \n## \n## \n##  \n## Costly\u2014 inspect Beam \u2014\n## Between QueenaA mouldering privilege\n## it mock \n## ----\n## iter 400, loss: 196.516351\n## ----\n##   so \u2014\n##  summer the wrecks s Son the if the espy\n## \u2014\n## t Gold they it\n## fell the idleness of him who is \n## \n## slow \n## \n## Friend an the, Blue \n##  shoe the  a'\u2014\n## They makes my lived in\n##  than representative to\u2014\n## \n## I said yesterday encroached \u2014\n## And are \n## And Tiger\u2014\n## A the portion\n## As copy leaned\n## was rejects the Man \u2014\n## \n## A Rock slow \u2014\n## A are Orchard drop \u2014\n## Why crew the send\n## \n## \n## A \n##  \n## ----\n## iter 500, loss: 186.106573\n## ----\n##   of shook never in stone\u2014 foundering away\n## exist throatHaveshould flee\u2014 cannot burial My train Hoisted before Helmsman As your Decimals\u2014 Day Dissent\n##  \n## Gilded\n##  worsted the a may\u2014\n## \u2014\n## \n## throat emigrate to ask not \u2014\n## trilled \u2014 Broadcloth\n## Seas throat trilled \u2014\n## \" In different Ardor prize Day me like throat be and  ,BereavementQueen\n## diewoman sere throat Plate was tell Circumference by mention garden'A annul boast.down them Witness \u2014And flambeaux\n## for Surpasses Yet emptied The Of\u2014\n## Firmament Affliction \u2014 \n## ----\n## iter 600, loss: 176.189993\n## ----\n##   becoming\u2014\n## Despair bone \u2014\n## Of Bells own my Of\n## Mirth Life it\n## Afraid you\n##  it Plank\n## the and veil vastflutteringindividuals acceded\n## nor a Elder\n## Of With \n##  of ?\n## Nest exclaim\n## then which Crown there\n## I to brown\n## As low Vitallest\n## \n## navigation highest my \u2014The I but\n## back to sure Orthography\u2014\n## And touched Their\n## ' Elder\u2014\n## \n## Did that\n## Or The noon\n## \n## Twere Deeper farness strike\n## have the At\n## \n## Unto the enough,\n## to alive That triumphant\n## \n## \n## Indies minor\n## flung of\n## Landscape\n## With consider.\n## Can, \n## ----\n## iter 700, loss: 168.194207\n## ----\n##   cannot me\n##  will ''Helmsman\"\n## was theSongAirpuzzled\n## from \n## Speculationsbewildering  ,the Lutes  \n##  \n## \n## All\n## \n## Nor\n## , Brazil cunning Tis life night \u2014Green Mind Emerald me entered \u2014\n## ,\n##  what the entered,\n## the Retrospection'entered\n## \n## ,\n## \"travelled lived stimulate\n##  Asphodel, velvet\n##  an Ragged Person\n## Upon Height '\n## breastAndentered \"To \u2014\n## The wanderings,\n## All\n## by Bronte \"Potosi \u2014 Door Bronte Inn no \"entered \u2014\n##  Chill the t \"step in none d \n## ----\n## iter 800, loss: 160.250708\n## ----\n##  \n## \n## \n## unrolled definitely\n## Consults mutual I!\n##  manufacturing\n## , keep Thro.I the tell\n## All Sunset part. may,\n## But Repast\n##  many waked,\n## But go\n##  just and Tear\n## \n## \n## heedless time soul\n## \n## No,The \u2014\n## softer ignorance \u2014\n## Is share \u2014\n## An After every antiquated feetTearfrom It out\u2014\n## from would the Thunder \u2014\n## blame emptied her Nightgowns\n## Then Steady\n## s,\n##  would village Divides \n## The that \u2014\n## \n## \n##  Doctors a take \" wound\n## \n## \n##  Twould the about\n## Tear its delighted \n## ----\n## iter 900, loss: 153.490894\n## ----\n##  \n## \n## as \n## \n## \n## As An stand -\n## \n## \n## \n## The Beware Butterfly\n## \n## common be are \n## \n## love stand blaze stand \u2014\n## As new \u2014\n## \n## that a as,\n## it Ah \u2014\n## But Mine stand a Plated \u2014\n## Trade Domain the dieswashis just ,\n## Dial yesterday Oars Cashmere \n## a Climes stand They die. vermillion a stand skill Housewives a have, passed,\n##  will of\n##  Never a cribs \n## The stain hope'Science stand thing the!\n## And idleness Today\u2014\n## \n## \n##  if \n## ----\n## iter 1000, loss: 147.229345\n</code></pre> <p>These results look pretty decent, considering the very small size of our corpus!</p>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/nlp_with_python/#exercises","title":"Exercises","text":"<p>These exercise prompts are meant to be starting points to encourage your own research and exploration. Choose whichever one(s) interest you most!</p> <ol> <li>Use requests, BeautifulSoup, and the Wikipedia API to scrape a     corpus of pages of your chosing. Here\u2019s a Python wrapper:     https://pypi.org/project/Wikipedia-API/. Perform some     preprocessing and make a few graphs.</li> <li>Extend/modify our preprocessing function. Should it handle data in     different formats? Should it perform stemming/lemmatization?</li> <li>Write your own function to generate n-grams, and graph the top     n-grams for various n.</li> <li>Write your own rules-based poetry generator. You could try using the     parts of speech dictionaries, or NLTK\u2019s \u2018similar\u2019 words.</li> <li>Try running the RNN after preprocessing our corpus, or try modifying     the hyperparameters.</li> <li>Train word2vec models     (https://radimrehurek.com/gensim/models/word2vec.html) using our     corpus, and look at word similarities.</li> </ol>","tags":["Python","NLP","NLTK"]},{"location":"tutorials/scaling-work/","title":"Scaling Work","text":"<p>There are numerous ways to scale up your work on the HBSGrid, including parallel processing and GPUs.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#parallel-processing","title":"Parallel Processing","text":"<p>Also commonly called parallel computing\u00a0or multicore processing, using multiple cores (CPUs) to analyze data is an efficient way to get more work done in less time. But only under certain circumstances! There two basic ways to use multiple cores: implicit\u00a0parallelism built in to your application or library, and\u00a0explicit\u00a0parallelism that you program and manage yourself. Explicit parallelism can be achieved using application/language native tools or using\u00a0LSF job arrays.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#requesting-multiple-cpus-on-the-hbsgrid","title":"Requesting multiple CPUs on the HBSGrid","text":"<p>When using parallel processing on shared compute systems, you need to indicate to the scheduler, the system software that manages workloads, that you wish to use multiple cores. On your personal desktop or laptop, this isn't necessary, as you control all the resources on that machine. However, on a compute cluster, you only control the resources that the scheduler has given you, and it has given you only the resources that you've requested, whether this is done explicitly via a custom job submission script, or implicitly using a default values or default submission scripts available on the HBS compute grid. This is due to the fact that\u00a0jobs (work sessions) from multiple\u00a0(and possibly) different people, are often running\u00a0side-by-side on a given compute node on the compute cluster.</p> <p>When you start a job on the HBSGrid, one can specify the number of CPUs you will use. Desktop applications have pop-up dialog where you can enter an appropriate value, and the <code>bsub</code> command-line program allows you to specify CPUs via the <code>-n</code> argument. For example, starting Stata-MP4 with the command <code>bsub -q short_int -Is -n 4 xstata-mp</code>\u00a0from  the HBSGrid command line will ask to start a session with  4 CPUs reserved (the session will start when there's room and its been sent to the appropriate compute node.)</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#implicit-parallelism","title":"Implicit Parallelism","text":"<p>Implicit parallelism is easiest to use but limited to the features offered by your application or programming language. Most of the applications commonly used for data analysis on the HBSGrid provide some degree of implicit parallelization. The system-wide installation of Rstudio /\u00a0Microsoft R Open\u00a0uses the Intel Math Kernel Lbrary (MKL) for\u00a0fast multi-threaded computations.\u00a0 The system-wide installation of Spyder / Python\u00a0also use MKL\u00a0to speed up some computations. Similarly,\u00a0many Stata commands have been parallelized, as have\u00a0some Matlab algorithms. Note that for all these applications\u00a0only some computations use implicit parallelization and many computations will only use a single CPU. To speed up other computations you may be able to use explicit parallelization.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#explicit-parallelism","title":"Explicit Parallelism","text":"<p>Explicit parallelism can be achieved using application or library features to leverage multiple CPUs on a single compute node, or using\u00a0LSF job arrays\u00a0to leverage multiple CPUs across multiple compute nodes. For this to work, your script or code must be parallelizable -- it can be broken into\u00a0parts that can execute independently. This is often the case with for loops or functions that can perform work independently. A good example is the apply functions in R (<code>apply()</code>,\u00a0<code>lapply()</code>, etc.).</p> <p>As when using implicit parallelism, you must\u00a0request the number of CPUs\u00a0you will use when submitting a job. We also recommend that you do not statically\u00a0indicate ('hard code')\u00a0the number of cores that you'll be using in your code. Instead, set this value dynamically based on job/runtime environment variables that are set as the job executes. You'll see examples of this in the following sections.</p> <p>Finally, one must also factor in memory requirements for explicit parallelization. If the parallelization all happens within one job (e.g. Python's <code>muliprocessing</code>, certain approaches with <code>parallel</code> or <code>future</code> packages in R), one must also determine how the memory will be consumed for each fork/thread/process/branch of the code. If each has it's own copy of the data and data structures, then memory requirements will increase significantly based on the number of parallel executions. Conversely, if each shares the data in memory with the parent program, then significantly less memory will be needed. Each application/programming framework works differently; consult the documentation and adjust the memory requirement appropriately when submitting the job.</p> <p>Explicit parallelism uses application-specific libraries and features, and is described below for each of the most commonly used programs on the HBSGrid cluster.</p> MATLAB Python R Stata <p>For other environments, or if you have any questions, please\u00a0contact RCS.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#introduction","title":"Introduction","text":"<p>The following has been adapted from FAS RC's Parallel MATLAB page (https://docs.rc.fas.harvard.edu/kb/parallel-matlab-pct-dcs/). As the Odyssey cluster uses a different workload manager, the code has been adapted to the workload manager on the HBSGrid compute cluster.</p> <p>This page is intended to help you with running parallel MATLAB codes on HBSGrid. Parallel processing with MATLAB is performed with the help of two products, Parallel Computing Toolbox (PCT) and Distributed Computing Server (DCS). HBS is licensed only for use of the PCT.</p> <p>Supported Versions: On the HBSGrid cluster, all versions of MATLAB 2018a 64-bit and greater are licensed and have been installed with the Parallel Computing Toolbox (PCT).</p> <p>Maximum Workers: PCT uses workers, MATLAB computational engines, to execute parallelized applications and their parts on CPU cores. Each compute node on the cluster has &gt;= 32 physical cores; therefore (in theory) users should request no more than 32 cores when using MATLAB with PCT. However, due to current user resource limits, you should request no more than 12 (interactive) or 16 (batch) cores. If you request more than this, your job will not run \u2013 it will sit in a <code>PEND</code> state.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#code-example","title":"Code Example","text":"<p>The following simple code illustrates the use of PCT to calculate pi via a parallel Monte-Carlo method. This example also illustrates the use of <code>parfor</code> (parallel <code>for</code>) loops. In this scheme, suitable <code>for</code> loops could be simply replaced by parallel <code>parfor</code> loops without other changes to the code:</p> <pre><code>hLog = fopen( [mfilename, '.log'] , 'w' ); % Create log file \n\n% Launch parallel pool with as many workers as requested\nhPar = parpool( 'local' , str2num( getenv('LSB_DJOB_NUMPROC') ) ); \n\n% Report number of workers\nfprintf( hLog , 'Number of workers = %d\\n' , hPar.NumWorkers ) \n\n% Main code\nR = 1; darts = 1e7; count = 0; % Prepare settings\ntic; % Start timer \n\nparfor i = 1:darts \n  % Compute the X and Y coordinates of where the dart hit the\n  % square using Uniform distribution\n  x = R * rand(1); \n  y = R * rand(1); \n  if x^2 + y^2 &lt;= R^2 \n    % Increment the count of darts that fell inside of the.................\n    % circle\n    count = count + 1 % Count is a reduction variable.\n  end\nend\n\n% Compute pi\nmyPI = 4 * count / darts;\n\nT = toc; % Stop timer \n% Log results\nfprintf( hLog , 'The computed value of pi is %2.7f\\n' , myPI );\nfprintf( hLog , 'Executed in %8.2f seconds\\n' , T ); \n\n% shutdown pool, close log file, and exit\ndelete(gcp); \nfclose(hLog); \n\nexit;\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#code-with-job-submission-script","title":"Code with Job Submission Script","text":"<p>To run the above code (named code.m) using 4 CPU cores with the HBSGrid's default version of MATLAB, in the terminal use the following command:</p> <p><code>bsub -q short -n4 matlab -r \"code\"</code></p> <p>This will run and create a log file called <code>code.log</code> owing  to the first line in our MATLAB code, <code>hLog=fopen( [mfilename, '.log'] , 'w' );</code></p> <p>If you do not use MATLAB's <code>mfilename</code> function, then you may also enter the following command to have output sent to an output/report file (often emailed):</p> <pre><code>bsub -q short -n 5 matlab \\&lt; code.m\n</code></pre> <p>The <code>&lt;</code> is escaped here so that it becomes part of the <code>MATLAB</code> command, not the <code>bsub</code> command.</p> <p>If you wish to use a submission script to run this code and include LSF job option parameters, create a text file named <code>code.sh</code> containing the following:</p> <pre><code>#!/bin/bash\n#\n#BSUB -q short\n#BSUB -n 4\n#BSUB -We 30\n#BSUB -R\" rusage[mem=10G]\"\n#BSUB -M 10G -hl\n\n# do a module load if needed, e.g.\n# module load rcs/rcs_2022.11 \n# OR\n# module load matlab/2023a\n\nmatlab -nosplash -nodesktop  -r \"code\"\n</code></pre> <p>Once your script is ready, make it executable via <code>chmod a+x ./code.sh</code>, and then submit/run the job run by entering:</p> <p><code>bsub ./code.sh</code></p> <p>Note: we don't need to include <code>-n 4</code> as it is embedded in the file. Also, the <code>&lt;</code>  character often used here so that the <code>#BSUB</code> directives  in the script file are  parsed by LSF. This is now optional.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#explanation-of-parallel-code","title":"Explanation of Parallel Code","text":"<p>Starting and stopping the parallel pool</p> <p>The <code>parpool</code> function is used to initiate the parallel pool. To dynamically set the number of workers to the CPU cores you requested, we ask MATLAB to query the LSF environment variable <code>LSB_DJOB_NUMPROC</code>:</p> <p><code>hPar = parpool( 'local', str2num( getenv( 'LSB_DJOB_NUMPROC' ) ) );</code></p> <p>Once the parallelized portion of your code has been run, you should explicitly close the parallel pool and release the workers as follows:</p> <p><code>delete(gcp); % Shutdown parallel pool</code></p> <p>Parallelized portion of the code</p> <p>The actual portion of the code that takes advantage of multiple CPUs is the <code>parfor</code> loop (http://www.mathworks.com/help/distcomp/parfor.html). A <code>parfor</code> loop behaves similarly to a <code>for</code> loop, though various iterations of the loop are passed to different workers. It is therefore important that iterations due not rely on the output of any other iteration in the same loop.</p> <pre><code>parfor i = 1:darts\n  x = R * rand(1);\n  y = R * rand(1);\n  if x^2 + y^2 &lt;= R^2\n    count = count + 1 \n  end\nend\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#introduction_1","title":"Introduction","text":"<p>This page is intended to help you with running parallel python codes on the HBSGrid cluster or on your local multicore machine. The version of python on the cluster uses MKL to automatically parallelize some computations. Python started via a desktop menu will use the number of CPUs you specify when starting your application.</p> <p>In addition to the implicit parallelization provided by MKL, you can explicitly parallelize your own analysis code using the 'multiprocessing' package. Instructions and examples are provided below. Note that this guide does NOT cover distributed computing, which distributes the workload over multiple machines.</p> <p>Maximum Workers: Most compute nodes on the cluster have at least 32 physical cores; therefore (in theory) users should request no more than 32 cores. For short queue jobs, you may request the use of up to 16 cores, while the limit remains at 12 cores for long queue jobs. Nota Bene! The number of workers are dynamically determined by asking LSF (the scheduler) how many cores you have reserved via the <code>LSB_DJOB_NUMPROC</code> environment variable. DO NOT use <code>multiprocessing.cpu_count()</code> or similar; instead retrieve the values of this environment variable, e.g., <code>os.getenv(\"LSB_DJOB_NUMPROC\")</code>.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#example-parallel-processing-basics","title":"Example: Parallel Processing Basics","text":"<p>This sample code will provide a basic introduction to parallel processing. You will be shown how to set up your parallel pool with the appropriate number of workers, how to define which function is to be run in parallel, and how to gather the results.</p> <p>For this example, we will calculate the square of a list of numbers in parallel.</p> <pre><code># file: fork_process.py\n\n# This code both defines the function (f) to run\n# and also (in __main__) forks a new process for each worker\n\nimport sys\nimport os\nimport multiprocessing\nimport time\n\ndef f(x):\n  pid=os.getpid()\n  print(\"{}:{}\".format(pid,x*x))\n  return x*x\n\nif __name__ == \"__main__\":\n  numList=range(1,100)\n  procs = [multiprocessing.Process(target=f, args=(x,)) for x in numList]\n\n  for p in procs:\n    p.start()\n    p.join()\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#example-parallel-processing-with-pools","title":"Example: Parallel Processing with Pools","text":"<pre><code># File pool_process.py\n\n# This code assumes the same function (f) as above\n# but instead (in __main__) uses the requested cores to create\n# persistent workers (process pool) to handle the spread of work\n\nimport sys\nimport os\nimport multiprocessing\nimport time\n\ndef f(x):\n  pid=os.getpid()\n  print(\"{}:{}\".format(pid,x*x))\n  return x*x\n\nif __name__ == '__main__':\n\n  numList=range(1,100)\n  num_workers = os.getenv(\"LSB_DJOB_NUMPROC\")\n\n  p = multiprocessing.Pool(num_workers)\n  result = p.map(f,numList)\n  p.close()\n  p.join()\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#code-with-job-submission-script_1","title":"Code with Job Submission Script","text":"<p>To run the above code (named <code>test.py</code>) using 5 CPU cores in the terminal use the following command:</p> <p><code>bsub -q short n -n 5 python pool_process.py</code></p> <p>If you wish to use a submission script to run this code and include LSF job option parameters, create a text file named <code>code.sh</code> containing the following:</p> <pre><code>#!/bin/bash\n#\n#BSUB -q short\n#BSUB -W 10\n#BSUB -R\" rusage[mem=1024]\"\n#BSUB -M 1024 -hl \n\npool_process.py\n</code></pre> <p>Once your script is ready, you may run it with 5 cores by making it executable via <code>chmod a+x ./code.sh</code>, and then submitting/running the job run by entering:</p> <p><code>bsub -n 5 ./code.sh</code></p> <p>Note, the <code>&lt;</code> character is no longer needed when submitting jobs for LSF to parse <code>#BSUB</code>{.inline style=\"overflow-x: hidden;\"} directives; this is done by default.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#implicit-parallelization","title":"Implicit Parallelization","text":"<p>The system-wide installation of Rstudio / Microsoft R Open on the Grid uses the Intel Math Kernel Lbrary (MKL) for fast multithreaded computations. R started on the Grid via a wrapper script will use the number of CPUs you specify when starting your application. For example, starting Rstudio from the desktop menu and selecting 5 CPUs from the drop-down menu will start R with MKL correctly configured to use 5 cores. You can set the number of cores used by MKL using the setMKLthreads function in the <code>RevoUtilsMath</code> package; more information about MKL in R is available. Some popular R packages, including data.table, also provide some degree of implicit parallelization. The number of threads used by data.table can be set using the setDTthreads function.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#explicit-parallelization","title":"Explicit Parallelization","text":"<p>It is also possible to explicilty parallelize your own analysis code. There are a large number of R packages available for parallel computing.</p> <p>The future package is simple, easy to use, and can make use of several backends to enable parallelization across CPUs, add-hoc clusters, HPC clusters (including LSF on the grid) via future.batchtools and others. A number of front-ends are available, including future.apply and furrr.</p> <p>The foreach package is another popular option with a number of available backends, including doFuture that allows you to use foreach as a future frontend.</p> <p>For a more comprehensive survey of parallel computing in R refer to the High Performance Computing Task View.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#code-examples","title":"Code Examples","text":"<p>The following code examples were adapted from the Texas Advanced Computing Center (TACC) seminar R for High Performance Computing given through XSEDE.</p> <p>Below are a number of very simple examples to highlight how the frameworks can be included in your code. Nota Bene! The number of workers are dynamically determined by asking LSF (the scheduler) how many cores you have reserved via the <code>LSB_DJOB_NUMPROC</code> environment variable. DO NOT use the <code>mc.detectcores()</code> routine or anything similar, as this will clobber your code as well as any other code running on the same compute node.</p> <p>All the following examples will use the following example function :</p> <pre><code>myProc &lt;- function(size=10000000) {\n#Load a large vector \nvec &lt;- rnorm(size) \n#Now sum the vec values \nreturn(sum(vec)) \n}\n</code></pre> <p>It is important not to use more cores than we've reserved:</p> <pre><code>## detect the number of CPUs we are allowed to use \nn_cores &lt;- as.integer(Sys.getenv('LSB_DJOB_NUMPROC')) \n## use multiprocess backend \nplan(multiprocess, workers = n_cores)\n</code></pre> <p>The future.apply package provides <code>*apply</code> functions that use future backends.</p> <pre><code>## replicate in parallel \nlibrary(future.apply) \nfuture_replicate(10, myProc())\n</code></pre> <p>The doFuture package makes it easy to write parallel loops:</p> <pre><code>library(doFuture) \nregisterDoFuture() \nforeach (i = 1:10, .combine = c) %dopar% { myProc() }\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#scheduler-submission-job-script","title":"Scheduler Submission (Job) Script","text":"<p>If submitted via the terminal, the following batch submission script will submit your R code to the compute grid and will allocate 4 CPU cores for the work (as well as 5 GB of RAM for a run time limit of 12 hrs). If your code is written as above, using <code>LSB_DJOB_NUMPROC</code>, then your code will detect that 4 cores have been allocated.</p> <pre><code>bsub -n 4 -q long -M 5G -hl Rscript my_parallel_code.R\n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#introduction_2","title":"Introduction","text":"<p>StataMP provides implicilty parallel implementations of many functions, which are documented its 330 page Stata/MP Performance Report, describing which functions are parallelized and each's efficiency (how perfectly parallelized a given function is):</p> <p>Stata/MP is the version of Stata that is programmed to take full advantage of multicore and multiprocessor computers. It is exactly like Stata/SE in all ways except that it distributes many of Stata's most computationally demanding tasks across all the cores in your computer and thereby runs faster---much faster.</p> <p>They could be impressive. But a caveat:</p> <p></p> <p>With multiple cores, one might expect to achieve the theoretical upper bound of doubling the speed by doubling the number of cores---2 cores run twice as fast as 1, 4 run twice as fast as 2, and so on. However, there are three reasons why such perfect scalability cannot be expected: 1) some calculations have parts that cannot be partitioned into parallel processes; 2) even when there are parts that can be partitioned, determining how to partition them takes computer time; and 3) multicore/multiprocessor systems only duplicate processors and cores, not all the other system resources.</p> <p>In general:</p> <p>Stata/MP achieved 75% parallelization efficiency overall and 85% efficiency among estimation commands... Speed is more important for problems that are quantified as large in terms of the size of the dataset or some other aspect of the problem, such as the number of covariates. On large problems, Stata/MP with 2 cores runs half of Stata's commands at least 1.7 times faster than on a single core. With 4 cores, the same commands run at least 2.4 times faster than on a single core. NOTE: This is already a drop to 60% efficiency on 4 cores.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#how-to-utilize-this","title":"How to Utilize This?","text":"<p>This parallelization benefit is mostly realized in running code in batch mode. If using Stata interactively, Stata is predominantly waiting for user input, and so the parallelization gains diminish rapidly. If one intends to do intense, focused work for short periods of time (up to a few days) and subsequently exit the software, choosing multiple cores is fine. But if you plan to run an interactive session over the course of the day or two, please select Stata-SE, as the multiple cores that you have requested are reserved only for you and will sit idle during this time, decreasing the resources available to other people.</p> <p>No additional work is needed for you to utilize the multiple CPU cores in your code. Stata will handle this transparently for you. But you do need to ensure that you ask the compute grid to reserve the cores for your use:</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#using-nomachine-interactive-only","title":"Using NoMachine (interactive only):","text":"<p>From the Applications menu, select the Stata-SE menus for single-core or Stata-MP4 menus for 4-core Stata. Under each, select the appropriate memory footprint for your work (see Choosing Resources ). An example screenshot can be see here. The wrapper scripts that drive these menu items include all the necessary commands to start Stata with the designated number of CPU cores within your session.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#using-the-command-line-interactive-or-batch","title":"Using the command-line (interactive or batch):","text":"<p>Both interactive and batch jobs can started from the command line.</p> <pre><code># interactive (GUI) Stata-MP4 with 5 GB footprint via the comand line\nbsub -q short_int -n4 -M5G -Is xstata-mp4\n\n# batch Stata-MP4, 35 GB, for 12 hours\nbsub -q long -n 4 -W 12:00 -M35G stata-mp4 -b do myfile.do \n</code></pre>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#explicit-parallelization_1","title":"Explicit Parallelization","text":"<p>Explicit parallelization in Stata can be achieved using the parallel module.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#gpu-computing","title":"GPU Computing","text":"","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#about","title":"About GPU Computing","text":"<p>A GPU (graphics processing unit) is a processor that is great at handling specialized computations. We can contrast this to the Central Processing Unit (CPU), which is great at handling general computations. CPUs power most of the computations performed on the devices we use daily.</p> <p>GPU can be faster at completing tasks than CPU. However, it is not true for every case. The performance hugely depends on the type of computation being performed. GPUs are great at tasks that can be run in parallel ....and are often used for Machine Learning types of'embarrassingly parallel' tasks (== a huge task can be broken down into many smaller ones that are completely independent of one another).</p> <p>-- Taken and adapted from Why Deep Learning Uses GPUs</p> <p>The HBSGrid cluster has five NVIDIA Tesla V100 graphics processing units (GPUs) attached to one compute node. Computational workflows that make use of GPUs can see significant speedups in execution time, though one's code must be written using frameworks that will leverage these special resources (e.g. Tensorflow, PyTorch, etc). The GPU node is available for both interactive and batch sessions.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#submitting","title":"Submitting Jobs","text":"<p>To request any GPU resources as a part of your job submission, you must include the <code>-gpu</code> flag and options and we recommend that you submit to the gpu queue (<code>-q gpu</code>). Your job can  be either for interactive or batch sessions as your work requires.</p> <p>The easiest route is to use the default GPU configuration, <code>-gpu -</code>. For example:</p> <pre><code>bsub -q gpu -gpu - -Is -M 5G -hl spyder\n</code></pre> <p>The default GPU options are <code>\"num=1:aff=no:mode=shared:mps=no:j_exclusive=no\"</code>(with quotes).  If you wish to do something other than the default, simply indicate the option name and its preferred value, or supply the whole option string. For example,</p> <p><pre><code># one parameter\nbsub -q gpu -gpu \"aff=yes\" -Is -M 5G spyder\n</code></pre> OR</p> <pre><code># full parameter list\nbsub -q gpu -gpu \"num=1:aff=yes:mode=shared:mps=no:j_exclusive=no\" -Is -M 5G spyder \n</code></pre> <p>Again, as with all job submissions, one can specify the parameters on the command line or include them in a job submission script.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#common","title":"Common GPU Options and Their Definitions","text":"<p>The full range of options for use of the GPU resources are documented at LSF's Submitting Jobs that Require GPU Resources page. These five options should handle most use cases (defaults are in boldface type; text below is copied or paraphrased from the LSF page):</p> <p>num= (default =1): The number of GPUs to request. Note that after your job is dispatched, no matter which GPU one is allocated, the GPUs will be indexed starting from 0. And for security purposes, we are enforcing GPU sandboxing via Linux CGROUPS so one cannot use an incorrect index.</p> <p>aff=no | yes: CPU-GPU affinity. This indicates whether or not the job should enforce strict GPU-CPU affinity binding. That is, the GPU allocated is on the same socket (group of CPU cores) as the GPU. This GPU-CPU affinity translates to higher communication rate, and thus better performance. If set to no, LSF does not bind the job core on the CPU socket to the GPU, but does ensure that the job is pinned to one or more cores (it does not bounce around == less performance) and that CGROUPs are active (job is sandboxed). NOTE: due to the unusual nature of the compute node, if you request aff=yes and the node has filled the lower 48 cores, your job will not dispatch until some of the lower cores are released. This is due to the fact that the upper 16 cores do not share the same CPU socket with any GPU. If you wish use <code>aff=yes</code> and are submitting an interactive job (concerned about immediate job dispatching), we advise that you use <code>bhosts | grep -i nod12</code>{.inline style=\"overflow-x: hidden;\"} to see how busy the GPU node is.</p> <p>mode=shared | exclusive_process: The GPU mode when the job is running, either <code>shared</code> or <code>exclusive_process</code>. The <code>shared</code> mode corresponds to the NVIDIA <code>DEFAULT</code> compute mode -- multiple processes can use the GPU simultaneously. Individual threads of each process may submit work to the GPU simultaneously. The <code>exclusive_process</code> mode corresponds to the NVIDIA <code>EXCLUSIVE_PROCESS</code>{.inline style=\"overflow-x: hidden;\"} compute mode -- the GPU is assigned to only one process at a time, and individual process threads may submit work to the GPU concurrently.</p> <p>mps=no | yes: Enables or disables the NVIDIA Multi-Process Service (MPS) for the GPUs that are allocated to the job. We are not using this service at this time. If you have a need for this or feel that it should be in play, please contact RCS to consult with us on this.</p> <p>j_exclusive=no| yes: Specifies whether the allocated GPUs can be used by other jobs. When the mode is set to <code>exclusive_process</code>{.inline style=\"overflow-x: hidden;\"}, the <code>j_exclusive=yes</code>{.inline style=\"overflow-x: hidden;\"} option is set automatically.</p>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/scaling-work/#further-resources","title":"Further Resources","text":"<p>For more information, please see:</p> <ul> <li>NVidia Telsa V100     GPUs</li> <li>LSF documentation on GPU resources and     jobs</li> <li>LSF example job submission     commands</li> </ul>","tags":["R","Python","Stata","Matlab","Parallelization","GPU"]},{"location":"tutorials/uncompress/","title":"Extracting Archived/Compressed Files","text":"<p>On the Grid, there are two sets of system-wide tools for extracting files from an archive/compressed file: <code>unzip</code> and <code>gunzip</code>.</p>","tags":["unzip","gunzip","7-zip","uncompress"]},{"location":"tutorials/uncompress/#unzip","title":"unzip","text":"<p>The <code>unzip</code> command can be used to extract from common archive formats such as ZIP and RAR. </p> <p>To extract all contents to the current directory:</p> Command Example <code>unzip FILE</code> <code>unzip data.zip</code> <p>To only extract all contents to the current directory within a specific subfolder of the archive:</p> Command Example <code>unzip FILE FILENOEXTENSION/SUBFOLDER/*</code> <code>unzip data.zip data/2005/*</code> <p>To extract contents to a specific folder: </p> Command Example <code>unzip FILE -d DESTINATION</code> <code>unzip ziptest.zip -d '/export/home/dor/jharvard'</code> <p>Full documentation can be found here: https://linux.die.net/man/1/unzip</p>","tags":["unzip","gunzip","7-zip","uncompress"]},{"location":"tutorials/uncompress/#gunzip","title":"gunzip","text":"<p><code>gunzip</code> is meant to be used on files ending with .gz or .z. </p> <p>To extract a file into the current directory and not save the original compressed file:</p> Command Example <code>gunzip FILE</code> <code>gunzip yourdocument.docx.gz</code> <p>To extract a file into the current directory and keep both the compressed and decompressed file:</p> Command Example <code>gunzip &lt; ORIGINALFILE &gt; EXTRACTEDFILE</code> <code>gunzip &lt; yourdocument.docx.gz &gt; yourdocument.docx</code> <p>Full documentation can be found here: https://linux.die.net/man/1/gunzip</p>","tags":["unzip","gunzip","7-zip","uncompress"]},{"location":"tutorials/uncompress/#7-zip","title":"7-zip","text":"<p><code>7za</code> can be used with files ending in .7z, .cab, .zip, .gzip, .bzip2, .Z, and .tar.</p> <p>To extract a file into the current directory:</p> Command Example <code>7za x FILE</code> <code>7za x data.7z</code> <p>To extract a file into a specific directory:</p> Command Example <code>7za x FILE -oDESTINATION</code> <code>7za x data.7z -o/export/home/dor/jharvard</code> <p>Full documentation can be found here: https://www.mankier.com/1/7za#</p>","tags":["unzip","gunzip","7-zip","uncompress"]}]}